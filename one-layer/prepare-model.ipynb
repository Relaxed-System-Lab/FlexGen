{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingfangyu/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import OPTForCausalLM, AutoModelForCausalLM, MixtralForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:08<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "moe = AutoModelForCausalLM.from_pretrained('mistralai/Mixtral-8x7B-Instruct-v0.1', torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.0.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.0.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.1.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.1.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.1.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.2.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.2.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.2.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.3.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.3.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.3.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.4.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.4.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.4.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.5.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.5.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.5.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.6.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.6.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.6.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.7.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.7.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.7.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.8.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.8.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.8.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.9.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.9.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.9.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.10.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.10.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.10.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.11.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.11.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.11.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.12.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.12.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.12.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.13.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.13.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.13.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.14.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.14.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.14.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.15.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.15.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.15.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.16.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.16.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.16.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.17.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.17.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.17.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.18.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.18.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.18.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.19.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.19.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.19.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.20.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.20.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.20.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.21.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.21.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.21.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.22.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.22.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.22.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.23.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.23.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.23.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.24.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.24.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.24.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.25.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.25.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.25.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.26.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.26.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.26.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.27.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.27.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.27.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.28.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.28.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.28.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.29.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.29.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.29.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.30.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.30.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.30.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n",
      "model.layers.31.self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "model.layers.31.self_attn.rotary_emb.cos_cached torch.Size([32768, 128])\n",
      "model.layers.31.self_attn.rotary_emb.sin_cached torch.Size([32768, 128])\n"
     ]
    }
   ],
   "source": [
    "for name, param in moe.named_buffers():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "model = OPTForCausalLM.from_pretrained('facebook/opt-13b', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = model.model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_layer = dec.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTDecoderLayer(\n",
       "  (self_attn): OPTAttention(\n",
       "    (k_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (v_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (q_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "    (out_proj): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "  )\n",
       "  (activation_fn): ReLU()\n",
       "  (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=5120, out_features=20480, bias=True)\n",
       "  (fc2): Linear(in_features=20480, out_features=5120, bias=True)\n",
       "  (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_layers = dec.layers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to np.memmap files\n",
    "# choices: 1) dir + files, 2) one file => choose 1\n",
    "model_dir = './_model_dir'\n",
    "import os \n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem: parameters, buffers, state_dict\n",
    "sub_model = five_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sub_model.state_dict().keys())  == set(list(n for n, p in sub_model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sub_model.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions: what are the buffers in llm models?\n",
    "# answer: 1) norm layer mean/std, 2) embedding, 3) dropout mask\n",
    "#       ~ 0.5GB, very small\n",
    "# it is ok to regard buffers as a part of the model weights\n",
    "for name, param in model.named_buffers():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./_model_dir/0.self_attn.k_proj.weight\n",
      "./_model_dir/0.self_attn.k_proj.bias\n",
      "./_model_dir/0.self_attn.v_proj.weight\n",
      "./_model_dir/0.self_attn.v_proj.bias\n",
      "./_model_dir/0.self_attn.q_proj.weight\n",
      "./_model_dir/0.self_attn.q_proj.bias\n",
      "./_model_dir/0.self_attn.out_proj.weight\n",
      "./_model_dir/0.self_attn.out_proj.bias\n",
      "./_model_dir/0.self_attn_layer_norm.weight\n",
      "./_model_dir/0.self_attn_layer_norm.bias\n",
      "./_model_dir/0.fc1.weight\n",
      "./_model_dir/0.fc1.bias\n",
      "./_model_dir/0.fc2.weight\n",
      "./_model_dir/0.fc2.bias\n",
      "./_model_dir/0.final_layer_norm.weight\n",
      "./_model_dir/0.final_layer_norm.bias\n",
      "./_model_dir/1.self_attn.k_proj.weight\n",
      "./_model_dir/1.self_attn.k_proj.bias\n",
      "./_model_dir/1.self_attn.v_proj.weight\n",
      "./_model_dir/1.self_attn.v_proj.bias\n",
      "./_model_dir/1.self_attn.q_proj.weight\n",
      "./_model_dir/1.self_attn.q_proj.bias\n",
      "./_model_dir/1.self_attn.out_proj.weight\n",
      "./_model_dir/1.self_attn.out_proj.bias\n",
      "./_model_dir/1.self_attn_layer_norm.weight\n",
      "./_model_dir/1.self_attn_layer_norm.bias\n",
      "./_model_dir/1.fc1.weight\n",
      "./_model_dir/1.fc1.bias\n",
      "./_model_dir/1.fc2.weight\n",
      "./_model_dir/1.fc2.bias\n",
      "./_model_dir/1.final_layer_norm.weight\n",
      "./_model_dir/1.final_layer_norm.bias\n",
      "./_model_dir/2.self_attn.k_proj.weight\n",
      "./_model_dir/2.self_attn.k_proj.bias\n",
      "./_model_dir/2.self_attn.v_proj.weight\n",
      "./_model_dir/2.self_attn.v_proj.bias\n",
      "./_model_dir/2.self_attn.q_proj.weight\n",
      "./_model_dir/2.self_attn.q_proj.bias\n",
      "./_model_dir/2.self_attn.out_proj.weight\n",
      "./_model_dir/2.self_attn.out_proj.bias\n",
      "./_model_dir/2.self_attn_layer_norm.weight\n",
      "./_model_dir/2.self_attn_layer_norm.bias\n",
      "./_model_dir/2.fc1.weight\n",
      "./_model_dir/2.fc1.bias\n",
      "./_model_dir/2.fc2.weight\n",
      "./_model_dir/2.fc2.bias\n",
      "./_model_dir/2.final_layer_norm.weight\n",
      "./_model_dir/2.final_layer_norm.bias\n",
      "./_model_dir/3.self_attn.k_proj.weight\n",
      "./_model_dir/3.self_attn.k_proj.bias\n",
      "./_model_dir/3.self_attn.v_proj.weight\n",
      "./_model_dir/3.self_attn.v_proj.bias\n",
      "./_model_dir/3.self_attn.q_proj.weight\n",
      "./_model_dir/3.self_attn.q_proj.bias\n",
      "./_model_dir/3.self_attn.out_proj.weight\n",
      "./_model_dir/3.self_attn.out_proj.bias\n",
      "./_model_dir/3.self_attn_layer_norm.weight\n",
      "./_model_dir/3.self_attn_layer_norm.bias\n",
      "./_model_dir/3.fc1.weight\n",
      "./_model_dir/3.fc1.bias\n",
      "./_model_dir/3.fc2.weight\n",
      "./_model_dir/3.fc2.bias\n",
      "./_model_dir/3.final_layer_norm.weight\n",
      "./_model_dir/3.final_layer_norm.bias\n",
      "./_model_dir/4.self_attn.k_proj.weight\n",
      "./_model_dir/4.self_attn.k_proj.bias\n",
      "./_model_dir/4.self_attn.v_proj.weight\n",
      "./_model_dir/4.self_attn.v_proj.bias\n",
      "./_model_dir/4.self_attn.q_proj.weight\n",
      "./_model_dir/4.self_attn.q_proj.bias\n",
      "./_model_dir/4.self_attn.out_proj.weight\n",
      "./_model_dir/4.self_attn.out_proj.bias\n",
      "./_model_dir/4.self_attn_layer_norm.weight\n",
      "./_model_dir/4.self_attn_layer_norm.bias\n",
      "./_model_dir/4.fc1.weight\n",
      "./_model_dir/4.fc1.bias\n",
      "./_model_dir/4.fc2.weight\n",
      "./_model_dir/4.fc2.bias\n",
      "./_model_dir/4.final_layer_norm.weight\n",
      "./_model_dir/4.final_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "from accelerate.utils import named_module_tensors\n",
    "nmt = named_module_tensors(sub_model, include_buffers=True, recurse=True)\n",
    "\n",
    "# nmt to np.memmap \n",
    "for n, t in nmt:\n",
    "    path = os.path.join(model_dir, n)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import named_module_tensors\n",
    "nmt = named_module_tensors(sub_model, include_buffers=True, recurse=True)\n",
    "\n",
    "import numpy as np \n",
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "# nmt to np.memmap \n",
    "for n, t in nmt:\n",
    "    path = os.path.join(model_dir, n)\n",
    "    # save \n",
    "    np_t = t.detach().numpy()\n",
    "    open_memmap(path, mode=\"w+\", shape=np_t.shape, dtype=np_t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "#   init:           state0 (empty) -> state1\n",
    "#   layer by layer: state1 (partially loaded) -> state2 (fully loaded) -> state3 (partially loaded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
