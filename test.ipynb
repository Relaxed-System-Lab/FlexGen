{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, OPTForCausalLM, BloomForCausalLM, CodeGenForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.33.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n",
    "# b_model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
    "# c_model = CodeGenForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(26, <function __main__.pre_hook(module, args, kwargs)>)])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_hook(module, args, kwargs):\n",
    "    # print(args, kwargs)\n",
    "    display = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            display[k] = v.shape \n",
    "        elif k == 'past_key_values' and v is not None:\n",
    "            display[k] = f'{len(v)} * ' + str((v[0][0].shape, v[0][1].shape))\n",
    "        else:\n",
    "            display[k] = v\n",
    "    print(display)\n",
    "\n",
    "    return args, kwargs\n",
    "\n",
    "model._forward_pre_hooks.clear()\n",
    "model.register_forward_pre_hook(pre_hook, with_kwargs=True)\n",
    "model._forward_pre_hooks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([24, 20]), 'past_key_values': None, 'use_cache': True, 'attention_mask': torch.Size([24, 20]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 20, 64]), torch.Size([24, 12, 20, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 21]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 21, 64]), torch.Size([24, 12, 21, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 22]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 22, 64]), torch.Size([24, 12, 22, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 23]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 23, 64]), torch.Size([24, 12, 23, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 24]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 24, 64]), torch.Size([24, 12, 24, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 25]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 25, 64]), torch.Size([24, 12, 25, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 26]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 26, 64]), torch.Size([24, 12, 26, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 27]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 27, 64]), torch.Size([24, 12, 27, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 28]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 28, 64]), torch.Size([24, 12, 28, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 29]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 29, 64]), torch.Size([24, 12, 29, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 30]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 30, 64]), torch.Size([24, 12, 30, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 31]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 31, 64]), torch.Size([24, 12, 31, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 32]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 32, 64]), torch.Size([24, 12, 32, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 33]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 33, 64]), torch.Size([24, 12, 33, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 34]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 34, 64]), torch.Size([24, 12, 34, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 35]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 35, 64]), torch.Size([24, 12, 35, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 36]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 36, 64]), torch.Size([24, 12, 36, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 37]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 37, 64]), torch.Size([24, 12, 37, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 38]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 38, 64]), torch.Size([24, 12, 38, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 39]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 39, 64]), torch.Size([24, 12, 39, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 40]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 40, 64]), torch.Size([24, 12, 40, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 41]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 41, 64]), torch.Size([24, 12, 41, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 42]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 42, 64]), torch.Size([24, 12, 42, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 43]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 43, 64]), torch.Size([24, 12, 43, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 44]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 44, 64]), torch.Size([24, 12, 44, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 45]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 45, 64]), torch.Size([24, 12, 45, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 46]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 46, 64]), torch.Size([24, 12, 46, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 47]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 47, 64]), torch.Size([24, 12, 47, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 48]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "{'input_ids': torch.Size([24, 1]), 'past_key_values': '12 * (torch.Size([24, 12, 48, 64]), torch.Size([24, 12, 48, 64]))', 'use_cache': True, 'attention_mask': torch.Size([24, 49]), 'return_dict': True, 'output_attentions': False, 'output_hidden_states': False}\n",
      "Who are you? Are you conscious?\n",
      "I'm not conscious, but I'm aware of the fact that I'm not conscious. I'm aware of the fact that I'm not conscious\n",
      "----------\n",
      "Where is Deutschland?\n",
      "It's in Germany.\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?\n",
      "Huawei Mate 60 Pro is the latest smartphone from the company. The smartphone is powered by a Qualcomm Snapdragon 855 processor and comes with a 5\n",
      "----------\n",
      "Who are you? Are you conscious?\n",
      "I'm not conscious, but I'm aware of the fact that I'm not conscious. I'm aware of the fact that I'm not conscious\n",
      "----------\n",
      "Where is Deutschland?\n",
      "It's in Germany.\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?\n",
      "Huawei Mate 60 Pro is the latest smartphone from the company. The smartphone is powered by a Qualcomm Snapdragon 855 processor and comes with a 5\n",
      "----------\n",
      "Who are you? Are you conscious?\n",
      "I'm not conscious, but I'm aware of the fact that I'm not conscious. I'm aware of the fact that I'm not conscious\n",
      "----------\n",
      "Where is Deutschland?\n",
      "It's in Germany.\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?\n",
      "Huawei Mate 60 Pro is the latest smartphone from the company. The smartphone is powered by a Qualcomm Snapdragon 855 processor and comes with a 5\n",
      "----------\n",
      "Who are you? Are you conscious?\n",
      "I'm not conscious, but I'm aware of the fact that I'm not conscious. I'm aware of the fact that I'm not conscious\n",
      "----------\n",
      "Where is Deutschland?\n",
      "It's in Germany.\n",
      "----------\n",
      "How is Huawei Mate 60 Pro?\n",
      "Huawei Mate 60 Pro is the latest smartphone from the company. The smartphone is powered by a Qualcomm Snapdragon 855 processor and comes with a 5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    'Who are you? Are you conscious?',\n",
    "    'Where is Deutschland?',\n",
    "    'How is Huawei Mate 60 Pro?'\n",
    "] * 4\n",
    "\n",
    "prompt_len = 20\n",
    "\n",
    "inputs = tokenizer(prompts, padding=\"max_length\", max_length=prompt_len, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=30 + prompt_len,\n",
    "    num_beams=2,\n",
    "    # num_beam_groups=2,\n",
    "    # diversity_penalty=0.1,\n",
    "    # do_sample=True,\n",
    ")\n",
    "\n",
    "output_texts = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "for output_text in output_texts:\n",
    "    print(output_text)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
