2023-10-07 11:14:54,852 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpsx7n5y16
2023-10-07 11:14:54,853 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpsx7n5y16/_remote_module_non_scriptable.py
2023-10-07 11:14:55,348 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-07 11:14:55,403 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-07 11:14:56,955 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-07 11:14:57,256 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-07 11:14:57,256 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-07 11:14:57,256 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-07 11:14:57,257 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-07 11:14:58,075 [flexgen_init.py:40 in policy_init] DEBUG - Got empty CausalLM: 'facebook/opt-125m' on meta device.
2023-10-07 11:14:58,086 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-07 11:14:58,087 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-07 11:14:58,087 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-07 11:14:58,088 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-07 11:14:58,089 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-07 11:14:58,090 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-07 11:14:58,091 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-07 11:14:58,093 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-07 11:14:58,094 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-07 11:14:58,096 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-07 11:14:58,097 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-07 11:14:58,098 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-07 11:14:58,099 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-07 11:14:58,100 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-07 11:14:58,101 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-07 11:14:58,101 [flexgen_init.py:215 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-07 11:14:58,102 [flexgen_init.py:219 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-07 11:14:58,105 [flexgen_init.py:225 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-07 11:14:58,150 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-07 11:14:58,277 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-07 11:14:58,363 [flexgen_init.py:72 in policy_init] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-07 11:14:58,414 [flexgen_init.py:84 in policy_init] INFO - model has been loaded by policy.
2023-10-07 11:14:58,415 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-07 11:14:58,415 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-07 11:14:58,415 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-07 11:14:58,415 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-07 11:14:58,415 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-07 11:14:58,416 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-07 11:14:58,416 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-07 11:14:58,416 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-07 11:14:58,416 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-07 11:14:58,416 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-07 11:14:58,417 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-07 11:14:58,417 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-07 11:14:58,417 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-07 11:14:58,417 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-07 11:14:58,417 [flexgen_forward.py:38 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-07 11:14:58,418 [flexgen_forward.py:38 in to_test_forward] DEBUG - lm_head to test forward
2023-10-07 11:14:58,658 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-07 11:14:58,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:14:58,832 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.embed_tokens forward pass:
2023-10-07 11:14:58,833 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:14:58,834 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:14:58,835 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.embed_positions forward pass:
2023-10-07 11:14:58,835 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:14:58,835 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:14:58,842 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.0 forward pass:
2023-10-07 11:14:58,858 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:14:58,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:14:58,867 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.1 forward pass:
2023-10-07 11:14:58,871 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:14:58,873 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:14:58,880 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.2 forward pass:
2023-10-07 11:14:58,884 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:14:58,886 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:14:58,893 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.3 forward pass:
2023-10-07 11:14:58,899 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:14:58,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:14:58,908 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.4 forward pass:
2023-10-07 11:14:58,914 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:14:58,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:14:58,922 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.5 forward pass:
2023-10-07 11:14:58,926 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:14:58,927 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:14:58,934 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.6 forward pass:
2023-10-07 11:14:58,937 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:14:58,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:14:58,944 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.7 forward pass:
2023-10-07 11:14:58,947 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:14:58,949 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:14:58,955 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.8 forward pass:
2023-10-07 11:14:58,978 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:14:58,980 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:14:58,986 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.9 forward pass:
2023-10-07 11:14:58,990 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:14:58,991 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:14:58,997 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.10 forward pass:
2023-10-07 11:14:59,001 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:14:59,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:14:59,009 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.layers.11 forward pass:
2023-10-07 11:14:59,012 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:14:59,014 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:14:59,015 [flexgen_forward.py:47 in new_forward] DEBUG - model.decoder.final_layer_norm forward pass:
2023-10-07 11:14:59,016 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:14:59,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:14:59,016 [flexgen_forward.py:47 in new_forward] DEBUG - lm_head forward pass:
2023-10-07 11:14:59,028 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:14:59,037 [flexgen_test.py:30 in test_hf_gen] INFO - 0.
2023-10-07 11:14:59,038 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:14:59,047 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-07 11:14:59,047 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-07 11:14:59,047 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-07 11:14:59,048 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-07 11:14:59,048 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-07 11:14:59,048 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-07 11:14:59,048 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-07 11:14:59,048 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-07 11:14:59,048 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-07 11:14:59,048 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-07 11:14:59,048 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-07 11:14:59,049 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-07 11:14:59,049 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-07 11:14:59,049 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-07 11:14:59,049 [flexgen_forward.py:27 in to_old_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-07 11:14:59,049 [flexgen_forward.py:27 in to_old_forward] DEBUG - lm_head from test to old.
2023-10-07 11:14:59,049 [flexgen_init.py:104 in policy_init] INFO - layer order: ['model.decoder.embed_tokens', 'model.decoder.embed_positions', 'model.decoder.layers.0', 'model.decoder.layers.1', 'model.decoder.layers.2', 'model.decoder.layers.3', 'model.decoder.layers.4', 'model.decoder.layers.5', 'model.decoder.layers.6', 'model.decoder.layers.7', 'model.decoder.layers.8', 'model.decoder.layers.9', 'model.decoder.layers.10', 'model.decoder.layers.11', 'model.decoder.final_layer_norm', 'lm_head']
2023-10-07 11:14:59,049 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-07 11:14:59,050 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-07 11:14:59,050 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-07 11:14:59,050 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-07 11:14:59,050 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-07 11:14:59,050 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-07 11:14:59,050 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-07 11:14:59,050 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-07 11:14:59,050 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-07 11:14:59,051 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-07 11:14:59,051 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-07 11:14:59,051 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-07 11:14:59,051 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-07 11:14:59,051 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-07 11:14:59,051 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-07 11:14:59,051 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-07 11:14:59,091 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-07 11:14:59,265 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:14:59,266 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:14:59,267 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10]),)
2023-10-07 11:14:59,267 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:14:59,267 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10]),)
2023-10-07 11:14:59,267 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:14:59,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:14:59,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:14:59,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:14:59,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:14:59,268 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 768])
2023-10-07 11:14:59,268 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 768])
2023-10-07 11:14:59,268 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:14:59,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:14:59,270 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:14:59,276 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10]), 0)
2023-10-07 11:14:59,276 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:14:59,277 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10]), 0)
2023-10-07 11:14:59,277 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:14:59,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:14:59,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:14:59,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:14:59,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:14:59,278 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 768])
2023-10-07 11:14:59,278 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 768])
2023-10-07 11:14:59,278 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:14:59,279 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:14:59,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:14:59,293 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,293 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,293 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,293 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:14:59,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:14:59,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:14:59,312 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:14:59,315 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,316 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,316 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:14:59,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:14:59,325 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:14:59,333 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,333 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,333 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,333 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:14:59,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:14:59,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:14:59,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:14:59,355 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,355 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,356 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:14:59,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:14:59,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:14:59,380 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,380 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,380 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,381 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:14:59,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:14:59,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:14:59,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:14:59,397 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,398 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,398 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:14:59,401 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:14:59,411 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:14:59,421 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,422 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,422 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,422 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,422 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:14:59,427 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:14:59,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:14:59,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:14:59,444 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,445 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,445 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:14:59,448 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:14:59,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:14:59,469 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,469 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,470 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,470 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:14:59,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:14:59,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:14:59,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:14:59,488 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,489 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,489 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:14:59,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:14:59,503 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:14:59,514 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,514 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,514 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,514 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:14:59,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:14:59,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:14:59,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:14:59,531 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,532 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,532 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:14:59,533 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:14:59,540 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:14:59,547 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,547 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,547 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,547 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:14:59,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:14:59,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:14:59,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:14:59,560 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,560 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,560 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:14:59,562 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:14:59,569 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:14:59,576 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,576 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,576 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,576 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:14:59,580 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:14:59,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:14:59,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:14:59,588 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,588 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,589 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:14:59,590 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:14:59,597 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:14:59,604 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,604 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,604 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,604 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:14:59,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:14:59,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:14:59,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:14:59,617 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,617 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,617 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:14:59,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:14:59,626 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:14:59,633 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,633 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,633 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,634 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:14:59,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:14:59,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:14:59,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:14:59,649 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,649 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,649 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:14:59,651 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:14:59,657 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:14:59,664 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,665 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,665 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,665 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:14:59,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:14:59,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:14:59,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:14:59,680 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,680 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,680 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:14:59,682 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:14:59,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:14:59,690 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,690 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,690 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,690 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 10, 10]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:14:59,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:14:59,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:14:59,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:14:59,701 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 10, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-07 11:14:59,702 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 10, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-07 11:14:59,702 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:14:59,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:14:59,704 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:14:59,705 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,705 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:14:59,705 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,705 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:14:59,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:14:59,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:14:59,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:14:59,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:14:59,706 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 768])
2023-10-07 11:14:59,706 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 768])
2023-10-07 11:14:59,706 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:14:59,707 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:14:59,708 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:14:59,708 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 10, 768]),)
2023-10-07 11:14:59,709 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:14:59,709 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 10, 768]),)
2023-10-07 11:14:59,709 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:14:59,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:14:59,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:14:59,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:14:59,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:14:59,758 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 10, 50272])
2023-10-07 11:14:59,761 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 10, 50272])
2023-10-07 11:14:59,762 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:14:59,768 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:14:59,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:14:59,770 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:14:59,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:14:59,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:14:59,770 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:14:59,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:14:59,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:14:59,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:14:59,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:14:59,771 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:14:59,771 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:14:59,771 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:14:59,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:14:59,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:14:59,779 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 11]), 10)
2023-10-07 11:14:59,779 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:14:59,780 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 11]), 10)
2023-10-07 11:14:59,780 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:14:59,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:14:59,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:14:59,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:14:59,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:14:59,781 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:14:59,781 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:14:59,781 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:14:59,781 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:14:59,788 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:14:59,795 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,795 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,795 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,795 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:14:59,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:14:59,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:14:59,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:14:59,805 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:14:59,805 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:14:59,806 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:14:59,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:14:59,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:14:59,821 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,821 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,821 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,822 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:14:59,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:14:59,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:14:59,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:14:59,830 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:14:59,831 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:14:59,831 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:14:59,832 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:14:59,839 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:14:59,846 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,846 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,846 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,846 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:14:59,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:14:59,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:14:59,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:14:59,855 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:14:59,855 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:14:59,855 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:14:59,857 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:14:59,863 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:14:59,870 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,870 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,870 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,871 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:14:59,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:14:59,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:14:59,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:14:59,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:14:59,880 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:14:59,880 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:14:59,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:14:59,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:14:59,895 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,896 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,896 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,896 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:14:59,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:14:59,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:14:59,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:14:59,906 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:14:59,907 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:14:59,907 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:14:59,908 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:14:59,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:14:59,921 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,921 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,921 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,921 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:14:59,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:14:59,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:14:59,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:14:59,930 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:14:59,931 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:14:59,931 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:14:59,932 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:14:59,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:14:59,945 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,945 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,946 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,946 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:14:59,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:14:59,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:14:59,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:14:59,955 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:14:59,956 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:14:59,956 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:14:59,958 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:14:59,964 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:14:59,971 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,971 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,971 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,971 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:14:59,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:14:59,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:14:59,978 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:14:59,980 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:14:59,980 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:14:59,980 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:14:59,982 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:14:59,988 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:14:59,995 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:14:59,995 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,995 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:14:59,995 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:14:59,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:14:59,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:00,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:00,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:00,004 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:15:00,005 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:15:00,005 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:00,006 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:00,013 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:00,020 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,020 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,020 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,021 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,021 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:00,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:00,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:00,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:00,031 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:15:00,031 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:15:00,031 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:00,033 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:00,039 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:00,047 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,047 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,047 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,047 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:00,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:00,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:00,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:00,056 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:15:00,057 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:15:00,057 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:00,058 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:00,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:00,066 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,066 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,067 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,067 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:00,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:00,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:00,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:00,076 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-07 11:15:00,076 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-07 11:15:00,076 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:00,078 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:00,079 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:00,079 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,080 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,080 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,080 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:00,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:00,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:00,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:00,081 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,081 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,081 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:00,081 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:00,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:00,082 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,083 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,083 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,083 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:00,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:00,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:00,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:00,118 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:00,119 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:00,119 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:00,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:00,128 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:00,129 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:00,129 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,130 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:00,130 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:00,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:00,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:00,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:00,131 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,131 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,131 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:00,131 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:00,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:00,139 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 12]), 11)
2023-10-07 11:15:00,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,139 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 12]), 11)
2023-10-07 11:15:00,139 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:00,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:00,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:00,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:00,140 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,140 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,140 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:00,141 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:00,147 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:00,154 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,154 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,155 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,155 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:00,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:00,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:00,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:00,164 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,165 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,165 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:00,167 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:00,173 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:00,179 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,180 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,180 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,180 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:00,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:00,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:00,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:00,190 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,190 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,190 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:00,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:00,198 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:00,205 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,205 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,206 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,206 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:00,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:00,211 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:00,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:00,215 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,215 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:00,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:00,224 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:00,230 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,231 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,231 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,231 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:00,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:00,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:00,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:00,239 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,240 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,240 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:00,241 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:00,248 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:00,255 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,255 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,255 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,255 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:00,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:00,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:00,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:00,263 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,264 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,264 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:00,265 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:00,272 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:00,278 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,279 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,279 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,279 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:00,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:00,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:00,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:00,288 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,288 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,288 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:00,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:00,296 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:00,303 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,303 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,303 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,303 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:00,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:00,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:00,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:00,313 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,313 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,313 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:00,315 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:00,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:00,328 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,328 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,329 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,329 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:00,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:00,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:00,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:00,339 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,339 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,339 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:00,340 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:00,347 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:00,354 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,354 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,355 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,355 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:00,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:00,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:00,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:00,363 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,364 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,364 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:00,365 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:00,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:00,379 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,379 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,379 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,379 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:00,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:00,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:00,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:00,388 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,388 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,389 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:00,390 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:00,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:00,403 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,404 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,404 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,404 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:00,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:00,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:00,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:00,413 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,413 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,413 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:00,415 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:00,421 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:00,422 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,423 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,423 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,423 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:00,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:00,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:00,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:00,440 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-07 11:15:00,440 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-07 11:15:00,440 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:00,442 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:00,443 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:00,444 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,444 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,444 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,444 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:00,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:00,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:00,453 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:00,453 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,453 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,454 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:00,454 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:00,455 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:00,455 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,456 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,456 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,456 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:00,464 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:00,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:00,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:00,488 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:00,500 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:00,500 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:00,507 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:00,508 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:00,509 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:00,510 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,510 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:00,511 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:00,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:00,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:00,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:00,513 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,513 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,513 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:00,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:00,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:00,521 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 13]), 12)
2023-10-07 11:15:00,521 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,521 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 13]), 12)
2023-10-07 11:15:00,522 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:00,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:00,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:00,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:00,523 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,523 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,523 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:00,523 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:00,530 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:00,537 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,537 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,537 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,537 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:00,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:00,542 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:00,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:00,546 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,546 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,546 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:00,548 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:00,554 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:00,561 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,561 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,561 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,562 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:00,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:00,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:00,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:00,570 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,570 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,570 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:00,572 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:00,579 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:00,585 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,585 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,586 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,586 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:00,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:00,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:00,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:00,594 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,595 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,595 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:00,596 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:00,603 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:00,610 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,610 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,610 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,610 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,610 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:00,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:00,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:00,617 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:00,619 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,619 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,619 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:00,621 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:00,627 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:00,634 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,634 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,635 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,635 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:00,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:00,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:00,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:00,643 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,644 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,644 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:00,645 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:00,652 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:00,658 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,658 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,659 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,659 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:00,667 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:00,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:00,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:00,673 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,673 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,673 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:00,675 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:00,681 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:00,688 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,688 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,689 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,689 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:00,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:00,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:00,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:00,699 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,699 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,699 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:00,701 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:00,707 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:00,714 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,714 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,714 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,714 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:00,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:00,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:00,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:00,722 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,723 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,723 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:00,724 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:00,731 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:00,738 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,738 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,738 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,738 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:00,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:00,743 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:00,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:00,747 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,747 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,747 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:00,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:00,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:00,762 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,762 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,763 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,763 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,763 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:00,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:00,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:00,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:00,771 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,772 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,772 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:00,773 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:00,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:00,787 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,787 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,787 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,787 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:00,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:00,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:00,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:00,796 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,797 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,797 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:00,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:00,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:00,806 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,806 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,806 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,806 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,806 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:00,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:00,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:00,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:00,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-07 11:15:00,817 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-07 11:15:00,817 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:00,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:00,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:00,820 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,821 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,821 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,821 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:00,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:00,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:00,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:00,825 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,825 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,826 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:00,826 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:00,827 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:00,827 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,827 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,828 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,828 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:00,840 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:00,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:00,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:00,862 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:00,863 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:00,863 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:00,870 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:00,871 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:00,872 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:00,872 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,872 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:00,872 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:00,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:00,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:00,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:00,874 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,874 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,874 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:00,875 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:00,875 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:00,883 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 14]), 13)
2023-10-07 11:15:00,883 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:00,883 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 14]), 13)
2023-10-07 11:15:00,883 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:00,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:00,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:00,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:00,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:00,884 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:00,885 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:00,885 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:00,885 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:00,892 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:00,898 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,899 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,899 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,899 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:00,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:00,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:00,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:00,907 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:00,908 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:00,908 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:00,910 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:00,917 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:00,923 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,923 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,923 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,924 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:00,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:00,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:00,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:00,932 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:00,932 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:00,932 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:00,934 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:00,941 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:00,948 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,948 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,948 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,948 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:00,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:00,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:00,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:00,959 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:00,960 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:00,960 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:00,962 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:00,968 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:00,975 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:00,976 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,976 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:00,976 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:00,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:00,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:00,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:00,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:00,985 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:00,985 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:00,985 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:00,987 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:00,994 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:01,001 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,001 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,002 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,002 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:01,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:01,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:01,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:01,010 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:01,011 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:01,011 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:01,013 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:01,019 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:01,026 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,026 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,026 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,026 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:01,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:01,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:01,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:01,034 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:01,035 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:01,035 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:01,036 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:01,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:01,051 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,051 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,051 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,051 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,051 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:01,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:01,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:01,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:01,060 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:01,060 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:01,061 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:01,062 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:01,069 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:01,076 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,076 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,076 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,076 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:01,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:01,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:01,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:01,085 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:01,085 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:01,085 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:01,087 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:01,094 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:01,100 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,101 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,101 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,101 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:01,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:01,105 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:01,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:01,109 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:01,110 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:01,110 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:01,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:01,118 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:01,125 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,125 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,125 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,125 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:01,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:01,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:01,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:01,135 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:01,135 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:01,135 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:01,137 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:01,143 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:01,150 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,150 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,150 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,150 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:01,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:01,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:01,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:01,159 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:01,159 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:01,159 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:01,161 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:01,168 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:01,169 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,169 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,169 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,169 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:01,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:01,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:01,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:01,190 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-07 11:15:01,190 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-07 11:15:01,191 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:01,193 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:01,194 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:01,194 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,195 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,195 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,195 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:01,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:01,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:01,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:01,199 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:01,200 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:01,200 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:01,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:01,202 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:01,203 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,203 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,203 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,203 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:01,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:01,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:01,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:01,243 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:01,244 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:01,244 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:01,253 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:01,254 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:01,255 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:01,255 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,255 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:01,255 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:01,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:01,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:01,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:01,257 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:01,257 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:01,257 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:01,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:01,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:01,266 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 15]), 14)
2023-10-07 11:15:01,266 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,267 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 15]), 14)
2023-10-07 11:15:01,267 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:01,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:01,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:01,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:01,268 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:01,269 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:01,269 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:01,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:01,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:01,284 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,284 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,284 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,285 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,285 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:01,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:01,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:01,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:01,297 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,297 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,298 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:01,300 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:01,308 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:01,315 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,315 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,315 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,315 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:01,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:01,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:01,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:01,326 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,327 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,327 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:01,330 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:01,337 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:01,344 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,344 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,345 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,345 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:01,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:01,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:01,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:01,356 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,357 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:01,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:01,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:01,374 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,374 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,374 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,374 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:01,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:01,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:01,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:01,396 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,397 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,397 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:01,399 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:01,406 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:01,413 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,413 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,413 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,414 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:01,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:01,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:01,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:01,423 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,424 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,424 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:01,425 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:01,432 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:01,439 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,439 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,439 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,439 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:01,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:01,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:01,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:01,449 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,449 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,449 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:01,451 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:01,457 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:01,464 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,464 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,464 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,465 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:01,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:01,469 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:01,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:01,474 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,474 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,474 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:01,476 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:01,482 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:01,489 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,490 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,490 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,490 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:01,493 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:01,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:01,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:01,499 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,499 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,499 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:01,501 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:01,507 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:01,514 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,515 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,515 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,515 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:01,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:01,520 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:01,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:01,530 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,530 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,530 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:01,532 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:01,539 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:01,546 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,546 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,546 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,546 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:01,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:01,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:01,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:01,555 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,555 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,555 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:01,557 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:01,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:01,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,571 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,571 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,571 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:01,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:01,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:01,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:01,580 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,581 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,581 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:01,582 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:01,589 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:01,590 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,590 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,590 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,590 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:01,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:01,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:01,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:01,599 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-07 11:15:01,599 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-07 11:15:01,600 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:01,601 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:01,602 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:01,602 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,603 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,603 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,603 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:01,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:01,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:01,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:01,604 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:01,604 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:01,604 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:01,604 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:01,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:01,605 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,606 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,606 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,606 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:01,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:01,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:01,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:01,634 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:01,635 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:01,635 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:01,642 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:01,643 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:01,643 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:01,643 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,644 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:01,644 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:01,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:01,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:01,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:01,645 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:01,645 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:01,645 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:01,645 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:01,646 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:01,653 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 16]), 15)
2023-10-07 11:15:01,653 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,653 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 16]), 15)
2023-10-07 11:15:01,653 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:01,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:01,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:01,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:01,654 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:01,654 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:01,655 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:01,655 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:01,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:01,668 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,668 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,669 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,669 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:01,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:01,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:01,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:01,678 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,679 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,679 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:01,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:01,687 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:01,694 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,694 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,694 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,694 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:01,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:01,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:01,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:01,708 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,708 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,709 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:01,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:01,717 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:01,724 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,724 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,724 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,724 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,724 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:01,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:01,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:01,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:01,733 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,734 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,734 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:01,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:01,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:01,749 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,749 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,750 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,750 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:01,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:01,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:01,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:01,758 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,759 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,759 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:01,761 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:01,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:01,775 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,775 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,775 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,775 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:01,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:01,779 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:01,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:01,783 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,784 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,784 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:01,785 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:01,792 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:01,799 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,799 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,800 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,800 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,800 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:01,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:01,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:01,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:01,809 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,809 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,810 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:01,811 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:01,817 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:01,824 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,825 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,825 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,825 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:01,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:01,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:01,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:01,835 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,835 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,835 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:01,837 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:01,844 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:01,850 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,850 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,851 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,851 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:01,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:01,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:01,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:01,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,860 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,861 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:01,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:01,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:01,876 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,876 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,876 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,876 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:01,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:01,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:01,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:01,886 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,887 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,887 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:01,888 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:01,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:01,902 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,902 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,902 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,902 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:01,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:01,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:01,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:01,912 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,912 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,912 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:01,914 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:01,920 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:01,927 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,927 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,928 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,928 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:01,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:01,934 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:01,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:01,940 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,940 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,940 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:01,942 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:01,949 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:01,950 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,950 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,950 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,950 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:01,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:01,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:01,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:01,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:01,961 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-07 11:15:01,962 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-07 11:15:01,962 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:01,963 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:01,964 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:01,965 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,965 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,965 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,965 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:01,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:01,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:01,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:01,966 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:01,967 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:01,967 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:01,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:01,968 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:01,968 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:01,968 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:01,969 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:01,969 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:01,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:01,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:01,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:01,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:02,003 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:02,004 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:02,004 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:02,012 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:02,013 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:02,014 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:02,014 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,014 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:02,014 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:02,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:02,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:02,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:02,015 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:02,015 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:02,016 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:02,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:02,017 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:02,024 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 17]), 16)
2023-10-07 11:15:02,024 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,024 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 17]), 16)
2023-10-07 11:15:02,024 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:02,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:02,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:02,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:02,026 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:02,026 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:02,026 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:02,026 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:02,033 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:02,040 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,040 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,040 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,040 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,041 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:02,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:02,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:02,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:02,051 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,051 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,052 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:02,053 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:02,060 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:02,067 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,067 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,067 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,068 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:02,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:02,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:02,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:02,076 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,076 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,076 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:02,078 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:02,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:02,092 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,092 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,092 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,092 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:02,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:02,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:02,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:02,102 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,103 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,103 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:02,104 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:02,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:02,118 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,119 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,119 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,119 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:02,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:02,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:02,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:02,128 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,129 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,129 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:02,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:02,137 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:02,144 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,144 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,145 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,145 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:02,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:02,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:02,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:02,153 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,153 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:02,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:02,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:02,169 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,169 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,169 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,169 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:02,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:02,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:02,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:02,177 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,178 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,178 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:02,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:02,186 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:02,193 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,193 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,194 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,194 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:02,197 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:02,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:02,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:02,204 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,205 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,205 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:02,206 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:02,213 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:02,220 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,220 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,220 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,221 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:02,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:02,226 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:02,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:02,230 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,230 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,231 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:02,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:02,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:02,246 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,246 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,246 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,246 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:02,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:02,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:02,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:02,254 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,255 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,255 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:02,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:02,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:02,270 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,270 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,271 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,271 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:02,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:02,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:02,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:02,280 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,280 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,280 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:02,282 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:02,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:02,296 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,296 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,296 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,296 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:02,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:02,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:02,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:02,305 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,305 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,305 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:02,307 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:02,314 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:02,315 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,315 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,315 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,315 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:02,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:02,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:02,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:02,324 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-07 11:15:02,325 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-07 11:15:02,325 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:02,326 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:02,327 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:02,328 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,328 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,328 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,328 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:02,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:02,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:02,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:02,329 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:02,329 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:02,329 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:02,330 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:02,330 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:02,331 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,331 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:02,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:02,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:02,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:02,361 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:02,362 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:02,362 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:02,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:02,371 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:02,371 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:02,371 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,371 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:02,372 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:02,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:02,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:02,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:02,373 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:02,373 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:02,373 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:02,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:02,374 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:02,381 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 18]), 17)
2023-10-07 11:15:02,381 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,381 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 18]), 17)
2023-10-07 11:15:02,381 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:02,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:02,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:02,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:02,382 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:02,382 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:02,382 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:02,383 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:02,389 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:02,396 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,397 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,397 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,397 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,397 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:02,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:02,402 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:02,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:02,406 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,407 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,407 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:02,409 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:02,416 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:02,423 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,423 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,423 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,423 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:02,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:02,429 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:02,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:02,432 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,433 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,433 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:02,434 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:02,441 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:02,448 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,448 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,448 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,448 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:02,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:02,453 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:02,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:02,456 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,457 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,457 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:02,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:02,465 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:02,472 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,472 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,473 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,473 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:02,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:02,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:02,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:02,481 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,482 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,482 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:02,483 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:02,490 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:02,497 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,498 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,498 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,498 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:02,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:02,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:02,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:02,507 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,508 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,508 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:02,510 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:02,516 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:02,523 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,523 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,524 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,524 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:02,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:02,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:02,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:02,533 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,533 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,533 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:02,535 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:02,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:02,548 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,549 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,549 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,549 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:02,552 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:02,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:02,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:02,557 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,557 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,558 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:02,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:02,566 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:02,572 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,573 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,573 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,573 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:02,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:02,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:02,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:02,581 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,581 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,581 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:02,583 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:02,590 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:02,597 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,597 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,597 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,597 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:02,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:02,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:02,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:02,606 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,607 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,607 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:02,608 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:02,615 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:02,622 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,622 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,622 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,622 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:02,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:02,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:02,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:02,632 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,633 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,633 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:02,634 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:02,641 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:02,648 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,648 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,649 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,649 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:02,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:02,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:02,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:02,659 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,659 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,659 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:02,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:02,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:02,669 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,669 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,669 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,669 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:02,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:02,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:02,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:02,678 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-07 11:15:02,678 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-07 11:15:02,679 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:02,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:02,681 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:02,681 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,682 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,682 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,682 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:02,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:02,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:02,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:02,683 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:02,683 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:02,683 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:02,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:02,684 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:02,685 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,685 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,685 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,685 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:02,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:02,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:02,712 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:02,720 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:02,724 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:02,725 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:02,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:02,743 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:02,744 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:02,744 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,744 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:02,744 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:02,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:02,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:02,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:02,745 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:02,745 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:02,745 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:02,746 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:02,746 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:02,754 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 19]), 18)
2023-10-07 11:15:02,754 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:02,754 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 19]), 18)
2023-10-07 11:15:02,754 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:02,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:02,755 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:02,755 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:02,755 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:02,755 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:02,756 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:02,756 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:02,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:02,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:02,770 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,770 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:02,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:02,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:02,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:02,778 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,779 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,779 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:02,781 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:02,788 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:02,795 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,795 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,795 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,796 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:02,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:02,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:02,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:02,806 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,807 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,807 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:02,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:02,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:02,822 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,822 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,823 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,823 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:02,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:02,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:02,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:02,832 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,833 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,833 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:02,834 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:02,842 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:02,849 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,849 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,849 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,849 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:02,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:02,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:02,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:02,859 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,859 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,859 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:02,861 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:02,868 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:02,875 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,875 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,876 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,876 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:02,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:02,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:02,882 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:02,884 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,885 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,885 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:02,886 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:02,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:02,901 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,901 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,901 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,901 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:02,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:02,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:02,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:02,912 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,912 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,913 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:02,914 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:02,920 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:02,928 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,928 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,928 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,928 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:02,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:02,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:02,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:02,939 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,940 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,940 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:02,942 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:02,949 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:02,955 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,955 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,956 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,956 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:02,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:02,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:02,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:02,966 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,966 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,967 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:02,968 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:02,975 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:02,982 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:02,982 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,983 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:02,983 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:02,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:02,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:02,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:02,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:02,992 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:02,993 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:02,993 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:02,994 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:03,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:03,008 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,009 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,009 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,009 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:03,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:03,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:03,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:03,021 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:03,021 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:03,021 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:03,023 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:03,029 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:03,036 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,037 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,037 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,037 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:03,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:03,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:03,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:03,047 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:03,048 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:03,048 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:03,049 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:03,057 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:03,058 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,058 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,059 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,059 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:03,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:03,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:03,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:03,068 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-07 11:15:03,068 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-07 11:15:03,068 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:03,070 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:03,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:03,072 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,072 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,072 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,072 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:03,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:03,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:03,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:03,073 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,073 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,073 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:03,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:03,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:03,075 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,075 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,075 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,075 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:03,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:03,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:03,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:03,107 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:03,108 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:03,109 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:03,116 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:03,117 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:03,118 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:03,118 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,118 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:03,119 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:03,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:03,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:03,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:03,120 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,120 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,120 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:03,120 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:03,121 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:03,127 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 20]), 19)
2023-10-07 11:15:03,128 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,128 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 20]), 19)
2023-10-07 11:15:03,128 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:03,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:03,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:03,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:03,129 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,129 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,129 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:03,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:03,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:03,142 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,142 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,142 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,143 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:03,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:03,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:03,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:03,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,152 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,152 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:03,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:03,161 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:03,168 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,168 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,169 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,169 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:03,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:03,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:03,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:03,179 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,179 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,179 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:03,181 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:03,188 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:03,195 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,195 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,195 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,195 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:03,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:03,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:03,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:03,206 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,206 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,206 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:03,208 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:03,215 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:03,222 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,222 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,222 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,222 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,222 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:03,226 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:03,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:03,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:03,233 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,233 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,233 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:03,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:03,242 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:03,249 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,249 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,249 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,249 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:03,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:03,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:03,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:03,259 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,259 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,259 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:03,261 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:03,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:03,274 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,274 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,275 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,275 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:03,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:03,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:03,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:03,285 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,285 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,285 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:03,287 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:03,293 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:03,300 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,301 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,301 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,301 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:03,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:03,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:03,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:03,311 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,311 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:03,313 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:03,319 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:03,326 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,327 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,327 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,327 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,327 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:03,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:03,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:03,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:03,336 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,336 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,336 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:03,338 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:03,344 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:03,351 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,351 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,351 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,351 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:03,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:03,357 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:03,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:03,361 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,361 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,361 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:03,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:03,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:03,376 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,376 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,377 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,377 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,377 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:03,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:03,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:03,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:03,386 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,386 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,387 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:03,388 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:03,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:03,401 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,402 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,402 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,402 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,402 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:03,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:03,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:03,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:03,411 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,412 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,412 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:03,414 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:03,420 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:03,421 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,421 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,422 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,422 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,422 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:03,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:03,427 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:03,429 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:03,431 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-07 11:15:03,431 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-07 11:15:03,432 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:03,433 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:03,434 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:03,435 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,435 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,435 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,435 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:03,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:03,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:03,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:03,436 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,436 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,436 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:03,437 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:03,437 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:03,438 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,438 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,438 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,438 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:03,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:03,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:03,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:03,469 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:03,469 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:03,470 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:03,478 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:03,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:03,480 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:03,480 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,480 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:03,480 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:03,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:03,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:03,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:03,482 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,482 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,482 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:03,482 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:03,483 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:03,490 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 21]), 20)
2023-10-07 11:15:03,490 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,490 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 21]), 20)
2023-10-07 11:15:03,490 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:03,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:03,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:03,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:03,491 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,492 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,492 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:03,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:03,500 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:03,507 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,507 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,507 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,508 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:03,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:03,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:03,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:03,517 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,518 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,518 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:03,519 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:03,527 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:03,536 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,536 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,537 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,537 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:03,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:03,542 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:03,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:03,547 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,547 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,548 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:03,549 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:03,556 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:03,562 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,563 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,563 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,563 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:03,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:03,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:03,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:03,572 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,573 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,573 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:03,574 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:03,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:03,587 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,587 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,587 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,587 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:03,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:03,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:03,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:03,597 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,598 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,598 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:03,600 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:03,611 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:03,621 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,621 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,621 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,621 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,621 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:03,624 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:03,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:03,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:03,632 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,633 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,633 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:03,634 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:03,641 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:03,648 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,648 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,649 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,649 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:03,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:03,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:03,656 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:03,658 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,658 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,658 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:03,659 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:03,667 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:03,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,674 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,674 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,674 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:03,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:03,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:03,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:03,684 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,684 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,684 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:03,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:03,693 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:03,703 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,703 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,704 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,704 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:03,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:03,711 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:03,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:03,717 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,718 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,718 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:03,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:03,729 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:03,736 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,737 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,737 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,737 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:03,740 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:03,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:03,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:03,749 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,749 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,750 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:03,751 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:03,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:03,765 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,765 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,765 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,765 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:03,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:03,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:03,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:03,776 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,776 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,777 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:03,778 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:03,785 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:03,792 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,792 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,793 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,793 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:03,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:03,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:03,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:03,808 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,808 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,808 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:03,811 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:03,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:03,821 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,821 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,821 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,821 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:03,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:03,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:03,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:03,834 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-07 11:15:03,835 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-07 11:15:03,835 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:03,836 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:03,837 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:03,838 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,838 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,838 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,838 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:03,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:03,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:03,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:03,839 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,839 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,839 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:03,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:03,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:03,841 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,841 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,841 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,841 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:03,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:03,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:03,872 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:03,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:03,881 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:03,881 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:03,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:03,890 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:03,891 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:03,891 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,891 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:03,891 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:03,891 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:03,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:03,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:03,892 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,893 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:03,893 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:03,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:03,901 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 22]), 21)
2023-10-07 11:15:03,901 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:03,901 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 22]), 21)
2023-10-07 11:15:03,901 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:03,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:03,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:03,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:03,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:03,902 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:03,903 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:03,903 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:03,903 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:03,910 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:03,917 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,917 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,918 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,918 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:03,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:03,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:03,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:03,934 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:03,935 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:03,935 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:03,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:03,948 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:03,955 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,955 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,955 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,955 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:03,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:03,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:03,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:03,967 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:03,967 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:03,967 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:03,969 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:03,976 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:03,982 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:03,983 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,983 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:03,983 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:03,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:03,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:03,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:03,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:03,992 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:03,993 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:03,993 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:03,995 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:04,004 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:04,011 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,011 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,012 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,012 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:04,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:04,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:04,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:04,023 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,024 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,024 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:04,026 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:04,033 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:04,040 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,041 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,041 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,041 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,041 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:04,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:04,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:04,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:04,050 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,050 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,050 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:04,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:04,059 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:04,066 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,066 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,067 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,067 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:04,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:04,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:04,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:04,076 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,076 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,076 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:04,077 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:04,084 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:04,091 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,091 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,091 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,091 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:04,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:04,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:04,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:04,102 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,103 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,103 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:04,105 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:04,113 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:04,122 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,123 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,123 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,123 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:04,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:04,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:04,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:04,133 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,134 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,134 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:04,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:04,142 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:04,149 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,149 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,149 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,149 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:04,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:04,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:04,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:04,158 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,158 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,158 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:04,160 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:04,167 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:04,174 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,174 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,174 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,174 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,174 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:04,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:04,179 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:04,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:04,183 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,183 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,184 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:04,185 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:04,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:04,199 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,199 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,199 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,199 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:04,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:04,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:04,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:04,210 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,210 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,211 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:04,212 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:04,219 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:04,220 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,220 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,221 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,221 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:04,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:04,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:04,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:04,233 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-07 11:15:04,234 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-07 11:15:04,234 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:04,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:04,236 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:04,237 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,237 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:04,237 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,237 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:04,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:04,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:04,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:04,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:04,238 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:04,238 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:04,238 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:04,239 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:04,239 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:04,240 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,240 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:04,240 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,240 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:04,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:04,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:04,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:04,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:04,268 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:04,269 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:04,269 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:04,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:04,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:04,278 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:04,278 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:04,278 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:04,279 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:04,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:04,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:04,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:04,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:04,279 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:04,280 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:04,280 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:04,280 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:04,281 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:04,288 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 23]), 22)
2023-10-07 11:15:04,288 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:04,288 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 23]), 22)
2023-10-07 11:15:04,288 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:04,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:04,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:04,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:04,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:04,289 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:04,290 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:04,290 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:04,290 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:04,298 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:04,308 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,309 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,309 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,309 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:04,313 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:04,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:04,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:04,321 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,321 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,322 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:04,324 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:04,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:04,340 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,340 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,340 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,340 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:04,344 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:04,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:04,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:04,350 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,351 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,351 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:04,352 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:04,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:04,367 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,367 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,367 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,367 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:04,370 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:04,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:04,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:04,376 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,376 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,376 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:04,378 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:04,385 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:04,391 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,392 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,392 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,392 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:04,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:04,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:04,402 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:04,404 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,405 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,406 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:04,408 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:04,419 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:04,430 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,430 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,430 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,431 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:04,434 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:04,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:04,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:04,444 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,444 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,444 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:04,446 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:04,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:04,461 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,461 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,461 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,461 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:04,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:04,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:04,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:04,480 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,482 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,483 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:04,485 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:04,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:04,503 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,504 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,504 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,504 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:04,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:04,511 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:04,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:04,518 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,519 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,519 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:04,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:04,533 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:04,543 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,543 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,544 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,544 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:04,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:04,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:04,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:04,561 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,562 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,562 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:04,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:04,569 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:04,576 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,576 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,576 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,576 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:04,580 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:04,581 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:04,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:04,585 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,585 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,586 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:04,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:04,594 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:04,600 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,600 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,601 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,601 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:04,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:04,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:04,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:04,610 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,611 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,611 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:04,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:04,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:04,626 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,626 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,626 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,626 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:04,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:04,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:04,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:04,647 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,648 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,648 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:04,650 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:04,656 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:04,657 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,657 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,658 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,658 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:04,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:04,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:04,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:04,667 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-07 11:15:04,667 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-07 11:15:04,667 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:04,669 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:04,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:04,670 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,670 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:04,671 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,671 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:04,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:04,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:04,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:04,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:04,673 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:04,673 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:04,673 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:04,674 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:04,674 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:04,675 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,675 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:04,675 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,675 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:04,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:04,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:04,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:04,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:04,703 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:04,704 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:04,704 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:04,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:04,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:04,712 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:04,712 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:04,712 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:04,712 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:04,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:04,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:04,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:04,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:04,713 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:04,714 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:04,714 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:04,714 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:04,715 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:04,722 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 24]), 23)
2023-10-07 11:15:04,722 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:04,722 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 24]), 23)
2023-10-07 11:15:04,722 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:04,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:04,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:04,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:04,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:04,723 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:04,723 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:04,724 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:04,724 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:04,730 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:04,737 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,737 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,738 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,738 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:04,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:04,743 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:04,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:04,747 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,747 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,747 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:04,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:04,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:04,762 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,763 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,763 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,763 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,763 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:04,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:04,768 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:04,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:04,771 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,772 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,772 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:04,774 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:04,780 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:04,786 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,787 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,787 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,787 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:04,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:04,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:04,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:04,796 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,796 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,796 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:04,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:04,804 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:04,811 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,811 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,812 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,812 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:04,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:04,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:04,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:04,822 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,822 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,822 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:04,824 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:04,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:04,837 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,838 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,838 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,838 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:04,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:04,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:04,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:04,848 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,848 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,848 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:04,850 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:04,857 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:04,863 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,864 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,864 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,864 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:04,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:04,870 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:04,872 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:04,874 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,874 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,874 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:04,876 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:04,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:04,889 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,889 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,889 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,890 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:04,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:04,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:04,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:04,900 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,901 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,901 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:04,902 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:04,909 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:04,916 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,916 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,916 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,916 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:04,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:04,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:04,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:04,927 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,927 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,927 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:04,928 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:04,935 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:04,941 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,941 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,942 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,942 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:04,945 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:04,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:04,949 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:04,951 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,951 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,951 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:04,953 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:04,960 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:04,966 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,966 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,967 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,967 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:04,972 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:04,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:04,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:04,978 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:04,978 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:04,978 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:04,980 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:04,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:04,993 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:04,993 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,993 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:04,994 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:04,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:04,999 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:05,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:05,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:05,005 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:05,006 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:05,006 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:05,008 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:05,015 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:05,016 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,016 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,016 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,016 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:05,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:05,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:05,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:05,027 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-07 11:15:05,028 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-07 11:15:05,028 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:05,029 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:05,030 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:05,031 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,031 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,031 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,031 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:05,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:05,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:05,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:05,032 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,032 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,033 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:05,033 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:05,034 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:05,034 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,034 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,034 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,035 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:05,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:05,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:05,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:05,078 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:05,079 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:05,079 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:05,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:05,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:05,089 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:05,089 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,089 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:05,089 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:05,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:05,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:05,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:05,090 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,091 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,091 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:05,091 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:05,092 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:05,099 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 25]), 24)
2023-10-07 11:15:05,099 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,099 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 25]), 24)
2023-10-07 11:15:05,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:05,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:05,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:05,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:05,101 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,101 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,101 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:05,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:05,108 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:05,115 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,115 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,115 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,115 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:05,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:05,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:05,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:05,125 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,126 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,126 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:05,128 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:05,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:05,142 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,142 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,143 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,143 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:05,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:05,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:05,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:05,153 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,154 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,154 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:05,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:05,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:05,169 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,169 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,169 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,169 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:05,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:05,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:05,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:05,180 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,180 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,180 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:05,182 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:05,189 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:05,195 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,195 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,196 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,196 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:05,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:05,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:05,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:05,206 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,207 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,207 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:05,209 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:05,215 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:05,222 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,223 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,223 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,223 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:05,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:05,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:05,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:05,238 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,239 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,239 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:05,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:05,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:05,254 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,254 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,254 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,254 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:05,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:05,260 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:05,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:05,265 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,265 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,265 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:05,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:05,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:05,280 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,281 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,281 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,281 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:05,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:05,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:05,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:05,291 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,292 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,292 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:05,294 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:05,300 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:05,307 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,307 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,308 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,308 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:05,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:05,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:05,316 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:05,319 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,320 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,320 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:05,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:05,328 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:05,335 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,335 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,335 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,335 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:05,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:05,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:05,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:05,345 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,346 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,346 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:05,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:05,354 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:05,361 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,361 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,362 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,362 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:05,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:05,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:05,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:05,373 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,373 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,373 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:05,375 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:05,381 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:05,388 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,388 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,388 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,388 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:05,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:05,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:05,397 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:05,399 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,400 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,400 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:05,402 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:05,409 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:05,410 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,410 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,410 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,410 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,410 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:05,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:05,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:05,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:05,421 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-07 11:15:05,422 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-07 11:15:05,422 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:05,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:05,424 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:05,425 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,425 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,425 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,425 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:05,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:05,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:05,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:05,426 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,426 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,426 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:05,427 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:05,427 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:05,428 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,428 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,428 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,428 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:05,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:05,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:05,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:05,463 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:05,464 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:05,464 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:05,471 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:05,472 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:05,473 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:05,473 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,473 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:05,474 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:05,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:05,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:05,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:05,475 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,475 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,475 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:05,475 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:05,476 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:05,483 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 26]), 25)
2023-10-07 11:15:05,483 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,483 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 26]), 25)
2023-10-07 11:15:05,483 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:05,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:05,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:05,484 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:05,484 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,485 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,485 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:05,485 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:05,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:05,499 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,499 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,499 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,499 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:05,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:05,506 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:05,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:05,511 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,511 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,512 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:05,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:05,520 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:05,527 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,527 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,527 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,527 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,528 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:05,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:05,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:05,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:05,537 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,538 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,538 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:05,539 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:05,546 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:05,553 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,553 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,553 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,553 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:05,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:05,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:05,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:05,565 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,565 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,565 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:05,567 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:05,574 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:05,581 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,581 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,581 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,581 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,581 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:05,584 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:05,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:05,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:05,591 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,591 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,591 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:05,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:05,600 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:05,607 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,608 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,608 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,608 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:05,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:05,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:05,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:05,623 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,624 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,624 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:05,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:05,632 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:05,639 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,639 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,639 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,639 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:05,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:05,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:05,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:05,650 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,651 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,651 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:05,652 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:05,659 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:05,666 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,666 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,666 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,666 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:05,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:05,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:05,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:05,677 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,678 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,678 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:05,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:05,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:05,693 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,693 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,694 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,694 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:05,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:05,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:05,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:05,709 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,710 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,710 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:05,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:05,718 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:05,725 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,725 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,726 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,726 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:05,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:05,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:05,734 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:05,736 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,737 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,737 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:05,739 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:05,745 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:05,752 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,753 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,753 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,753 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:05,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:05,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:05,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:05,764 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,764 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,764 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:05,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:05,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:05,780 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,780 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,780 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,780 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:05,784 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:05,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:05,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:05,791 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,791 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,791 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:05,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:05,800 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:05,801 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,801 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,801 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,801 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:05,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:05,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:05,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:05,812 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-07 11:15:05,812 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-07 11:15:05,813 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:05,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:05,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:05,815 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,816 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,816 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,816 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:05,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:05,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:05,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:05,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:05,818 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:05,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:05,819 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,819 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,819 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,820 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:05,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:05,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:05,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:05,855 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:05,856 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:05,856 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:05,865 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:05,866 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:05,866 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:05,866 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,866 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:05,867 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:05,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:05,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:05,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:05,867 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,868 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,868 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:05,868 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:05,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:05,876 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 27]), 26)
2023-10-07 11:15:05,876 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:05,876 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 27]), 26)
2023-10-07 11:15:05,876 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:05,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:05,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:05,877 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:05,877 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:05,877 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:05,877 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:05,877 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:05,878 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:05,884 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:05,891 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,891 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,892 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,892 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:05,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:05,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:05,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:05,902 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:05,903 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:05,903 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:05,905 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:05,911 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:05,918 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,918 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,918 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,918 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:05,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:05,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:05,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:05,929 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:05,930 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:05,930 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:05,932 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:05,939 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:05,946 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,946 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,946 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,946 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:05,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:05,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:05,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:05,957 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:05,958 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:05,958 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:05,960 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:05,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:05,973 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:05,973 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,974 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:05,974 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:05,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:05,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:05,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:05,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:05,984 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:05,984 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:05,984 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:05,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:05,993 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:06,000 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,000 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,000 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,000 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,001 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:06,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:06,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:06,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:06,011 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:06,011 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:06,011 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:06,013 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:06,020 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:06,027 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,027 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,027 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,027 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:06,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:06,034 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:06,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:06,038 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:06,039 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:06,039 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:06,040 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:06,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:06,054 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,055 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,055 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,055 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:06,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:06,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:06,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:06,065 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:06,066 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:06,066 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:06,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:06,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:06,081 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,082 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,082 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,082 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:06,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:06,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:06,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:06,092 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:06,092 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:06,092 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:06,094 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:06,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:06,107 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,107 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,108 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,108 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:06,111 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:06,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:06,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:06,118 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:06,118 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:06,118 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:06,120 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:06,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:06,133 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,133 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,134 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,134 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:06,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:06,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:06,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:06,143 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:06,144 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:06,144 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:06,145 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:06,152 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:06,159 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,159 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,159 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,159 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:06,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:06,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:06,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:06,169 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:06,170 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:06,170 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:06,172 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:06,178 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:06,179 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,179 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,180 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,180 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:06,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:06,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:06,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:06,190 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-07 11:15:06,190 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-07 11:15:06,190 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:06,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:06,193 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:06,193 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,194 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:06,194 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,194 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:06,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:06,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:06,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:06,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:06,195 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:06,195 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:06,195 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:06,195 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:06,196 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:06,197 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,197 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:06,197 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,197 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:06,197 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:06,207 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:06,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:06,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:06,232 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:06,233 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:06,233 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:06,241 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:06,242 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:06,242 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:06,243 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:06,243 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:06,243 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:06,244 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:06,244 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:06,244 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:06,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:06,245 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:06,245 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:06,245 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:06,246 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:06,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:06,254 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 28]), 27)
2023-10-07 11:15:06,254 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:06,254 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 28]), 27)
2023-10-07 11:15:06,255 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:06,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:06,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:06,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:06,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:06,256 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:06,256 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:06,256 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:06,256 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:06,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:06,270 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,270 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,270 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,270 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:06,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:06,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:06,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:06,281 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,281 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,281 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:06,283 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:06,290 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:06,297 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,297 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,297 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,297 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:06,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:06,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:06,306 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:06,308 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:06,311 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:06,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:06,324 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,324 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,325 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,325 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:06,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:06,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:06,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:06,335 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,335 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,335 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:06,337 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:06,344 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:06,350 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,351 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,351 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,351 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:06,354 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:06,357 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:06,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:06,361 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,362 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,362 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:06,364 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:06,371 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:06,377 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,378 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,378 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,378 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:06,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:06,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:06,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:06,388 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,389 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,389 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:06,391 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:06,398 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:06,405 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,405 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,405 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,405 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:06,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:06,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:06,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:06,415 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,416 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,416 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:06,417 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:06,424 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:06,431 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,431 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,432 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,432 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:06,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:06,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:06,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:06,442 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,444 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,444 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:06,445 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:06,452 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:06,459 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,459 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,460 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,460 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:06,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:06,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:06,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:06,470 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,471 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,471 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:06,473 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:06,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:06,487 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,487 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,487 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,487 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:06,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:06,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:06,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:06,496 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,497 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,497 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:06,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:06,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:06,512 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,512 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,512 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,512 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:06,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:06,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:06,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:06,521 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,534 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,534 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:06,536 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:06,543 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:06,550 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,550 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,551 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,551 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:06,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:06,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:06,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:06,621 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,622 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,623 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:06,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:06,633 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:06,634 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,634 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,634 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,635 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:06,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:06,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:06,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:06,645 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-07 11:15:06,646 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-07 11:15:06,646 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:06,648 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:06,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:06,650 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,651 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:06,651 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,651 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:06,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:06,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:06,652 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:06,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:06,653 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:06,653 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:06,654 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:06,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:06,655 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:06,656 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,657 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:06,657 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,657 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:06,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:06,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:06,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:06,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:06,688 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:06,689 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:06,690 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:06,698 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:06,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:06,700 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:06,700 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:06,701 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:06,701 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:06,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:06,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:06,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:06,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:06,703 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:06,704 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:06,704 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:06,705 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:06,706 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:06,713 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 29]), 28)
2023-10-07 11:15:06,713 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:06,714 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 29]), 28)
2023-10-07 11:15:06,714 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:06,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:06,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:06,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:06,716 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:06,716 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:06,716 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:06,717 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:06,717 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:06,723 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:06,730 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,730 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,730 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,731 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:06,734 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:06,736 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:06,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:06,740 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,740 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,740 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:06,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:06,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:06,755 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,755 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,756 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,756 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:06,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:06,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:06,767 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:06,771 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,772 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,772 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:06,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:06,784 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:06,794 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,794 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,795 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,795 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:06,798 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:06,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:06,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:06,803 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,804 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,804 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:06,806 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:06,812 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:06,819 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,819 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,819 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,819 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:06,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:06,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:06,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:06,827 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,828 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,828 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:06,829 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:06,836 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:06,843 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,843 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,843 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,843 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:06,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:06,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:06,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:06,852 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,852 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,852 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:06,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:06,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:06,867 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,867 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,867 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,867 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:06,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:06,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:06,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:06,877 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,878 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,878 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:06,879 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:06,886 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:06,893 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,893 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,894 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,894 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:06,897 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:06,899 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:06,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:06,903 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,903 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,903 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:06,905 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:06,911 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:06,918 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,918 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,918 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,919 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,919 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:06,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:06,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:06,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:06,927 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,928 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,928 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:06,929 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:06,936 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:06,943 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,943 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,943 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,943 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:06,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:06,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:06,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:06,952 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,953 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,953 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:06,955 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:06,961 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:06,968 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,968 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,968 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,968 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:06,972 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:06,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:06,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:06,977 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:06,978 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:06,978 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:06,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:06,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:06,993 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:06,993 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,993 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:06,993 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:06,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:06,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:06,998 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:07,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:07,002 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:07,002 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:07,003 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:07,004 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:07,011 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:07,012 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,013 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,013 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,013 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:07,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:07,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:07,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:07,022 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-07 11:15:07,023 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-07 11:15:07,023 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:07,024 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:07,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:07,026 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,026 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,026 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,027 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:07,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:07,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:07,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:07,028 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,028 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,028 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:07,028 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:07,029 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:07,029 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,030 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,030 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,030 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,030 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:07,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:07,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:07,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:07,059 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:07,060 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:07,060 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:07,067 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:07,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:07,068 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:07,069 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,069 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:07,069 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:07,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:07,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:07,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:07,070 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,070 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,070 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:07,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:07,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:07,079 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 30]), 29)
2023-10-07 11:15:07,079 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,079 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 30]), 29)
2023-10-07 11:15:07,079 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:07,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:07,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:07,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:07,081 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,081 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,081 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:07,081 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:07,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:07,095 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,096 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,096 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,096 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:07,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:07,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:07,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:07,106 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,106 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,107 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:07,108 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:07,116 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:07,123 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,123 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,123 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,124 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:07,127 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:07,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:07,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:07,135 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,135 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,136 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:07,137 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:07,145 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:07,151 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,152 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,152 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,152 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:07,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:07,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:07,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:07,161 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,162 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,162 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:07,163 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:07,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:07,178 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,178 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,178 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,178 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:07,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:07,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:07,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:07,188 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,188 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,188 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:07,190 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:07,197 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:07,204 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,205 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,205 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,205 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:07,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:07,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:07,217 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:07,219 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,219 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,219 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:07,221 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:07,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:07,235 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,235 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,235 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,236 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,236 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:07,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:07,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:07,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:07,244 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,244 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,244 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:07,246 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:07,252 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:07,261 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,261 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,262 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,262 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:07,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:07,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:07,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:07,271 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,271 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,271 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:07,273 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:07,280 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:07,287 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,287 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,287 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,287 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:07,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:07,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:07,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:07,295 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,295 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,296 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:07,297 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:07,303 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:07,310 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,311 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,311 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,311 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:07,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:07,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:07,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:07,319 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,320 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,320 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:07,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:07,328 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:07,335 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,335 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,335 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,335 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:07,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:07,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:07,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:07,343 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,344 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,344 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:07,345 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:07,352 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:07,360 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,360 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,360 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,360 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:07,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:07,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:07,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:07,368 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,369 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,369 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:07,371 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:07,378 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:07,379 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,379 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,379 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,379 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:07,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:07,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:07,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:07,388 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-07 11:15:07,389 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-07 11:15:07,389 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:07,390 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:07,391 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:07,392 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,392 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,392 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,392 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:07,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:07,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:07,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:07,393 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,393 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,394 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:07,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:07,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:07,395 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,395 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,395 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,395 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:07,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:07,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:07,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:07,428 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:07,429 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:07,429 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:07,435 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:07,436 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:07,437 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:07,437 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,437 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:07,438 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:07,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:07,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:07,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:07,439 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,439 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,439 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:07,439 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:07,440 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:07,447 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 31]), 30)
2023-10-07 11:15:07,447 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,447 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 31]), 30)
2023-10-07 11:15:07,447 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:07,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:07,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:07,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:07,448 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,449 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,449 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:07,449 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:07,455 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:07,462 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,462 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,463 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,463 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:07,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:07,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:07,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:07,471 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,472 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,472 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:07,474 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:07,480 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:07,487 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,487 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,487 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,487 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,488 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:07,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:07,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:07,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:07,500 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,501 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,501 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:07,503 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:07,510 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:07,517 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,517 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,517 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,517 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:07,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:07,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:07,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:07,529 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,530 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,530 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:07,532 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:07,538 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:07,545 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,545 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,545 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,546 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:07,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:07,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:07,552 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:07,554 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,554 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,555 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:07,556 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:07,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:07,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,570 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,570 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,570 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:07,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:07,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:07,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:07,579 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,579 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,579 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:07,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:07,588 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:07,595 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,595 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,595 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,595 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:07,598 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:07,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:07,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:07,605 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,606 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,606 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:07,607 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:07,617 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:07,628 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,629 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,629 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,629 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:07,651 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:07,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:07,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:07,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,657 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,658 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:07,659 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:07,666 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:07,673 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,673 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,674 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,674 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:07,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:07,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:07,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:07,683 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,683 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,683 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:07,684 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:07,691 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:07,703 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,703 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,703 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,704 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:07,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:07,711 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:07,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:07,717 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,718 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,718 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:07,721 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:07,731 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:07,738 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,739 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,739 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,739 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,739 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:07,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:07,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:07,748 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:07,749 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,750 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,750 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:07,751 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:07,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:07,765 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,766 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,766 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,766 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:07,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:07,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:07,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:07,777 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,778 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,778 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:07,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:07,786 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:07,787 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,788 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,788 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,788 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:07,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:07,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:07,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:07,800 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-07 11:15:07,800 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-07 11:15:07,801 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:07,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:07,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:07,806 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,806 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,806 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,806 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:07,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:07,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:07,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:07,807 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,808 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,808 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:07,808 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:07,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:07,809 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,810 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,810 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,810 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,810 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:07,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:07,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:07,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:07,840 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:07,841 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:07,841 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:07,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:07,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:07,850 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:07,850 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,850 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:07,850 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:07,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:07,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:07,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:07,851 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,852 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,852 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:07,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:07,853 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:07,860 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 32]), 31)
2023-10-07 11:15:07,860 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:07,860 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 32]), 31)
2023-10-07 11:15:07,860 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:07,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:07,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:07,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:07,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:07,861 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:07,862 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:07,862 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:07,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:07,869 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:07,875 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,876 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,876 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,876 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:07,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:07,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:07,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:07,885 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:07,885 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:07,885 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:07,887 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:07,895 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:07,907 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,907 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,907 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,908 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:07,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:07,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:07,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:07,919 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:07,920 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:07,920 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:07,922 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:07,929 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:07,936 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,936 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,936 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,937 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:07,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:07,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:07,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:07,945 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:07,945 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:07,945 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:07,947 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:07,954 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:07,961 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,961 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,961 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,961 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:07,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:07,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:07,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:07,971 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:07,972 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:07,972 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:07,974 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:07,981 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:07,988 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:07,988 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,988 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:07,988 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:07,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:07,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:07,993 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:07,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:08,000 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:08,000 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:08,001 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:08,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:08,015 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:08,024 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,024 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,025 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,025 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:08,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:08,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:08,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:08,035 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:08,035 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:08,035 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:08,037 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:08,044 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:08,051 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,051 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,051 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,051 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,051 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:08,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:08,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:08,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:08,061 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:08,062 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:08,062 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:08,064 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:08,070 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:08,077 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,077 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,078 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,078 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:08,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:08,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:08,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:08,087 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:08,087 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:08,088 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:08,089 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:08,098 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:08,111 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,111 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,111 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,112 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:08,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:08,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:08,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:08,124 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:08,124 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:08,125 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:08,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:08,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:08,142 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,142 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,142 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,142 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,142 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:08,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:08,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:08,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:08,151 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:08,152 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:08,152 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:08,153 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:08,160 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:08,167 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,167 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,167 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:08,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:08,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:08,174 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:08,176 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:08,177 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:08,177 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:08,178 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:08,185 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:08,186 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,186 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,187 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,187 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:08,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:08,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:08,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:08,195 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-07 11:15:08,195 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-07 11:15:08,195 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:08,197 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:08,198 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:08,198 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,199 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,199 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,199 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:08,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:08,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:08,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:08,200 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:08,200 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:08,200 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:08,200 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:08,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:08,202 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,202 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,202 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,202 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:08,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:08,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:08,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:08,236 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:08,237 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:08,237 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:08,244 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:08,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:08,246 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:08,246 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,246 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:08,247 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:08,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:08,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:08,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:08,248 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:08,248 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:08,248 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:08,248 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:08,249 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:08,256 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 33]), 32)
2023-10-07 11:15:08,256 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,256 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 33]), 32)
2023-10-07 11:15:08,256 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:08,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:08,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:08,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:08,257 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:08,257 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:08,258 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:08,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:08,264 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:08,271 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,272 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,272 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,272 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,272 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:08,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:08,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:08,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:08,282 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,282 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,282 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:08,284 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:08,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:08,298 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,298 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,298 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,298 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:08,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:08,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:08,306 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:08,308 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,308 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:08,310 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:08,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:08,324 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,324 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,325 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,325 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:08,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:08,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:08,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:08,334 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,334 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,334 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:08,336 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:08,343 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:08,350 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,350 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,350 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,350 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,350 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:08,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:08,357 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:08,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:08,360 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,361 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,361 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:08,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:08,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:08,377 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,378 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,378 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,378 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:08,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:08,383 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:08,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:08,387 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,387 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,387 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:08,389 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:08,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:08,403 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,403 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,403 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,403 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:08,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:08,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:08,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:08,416 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,416 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,417 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:08,418 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:08,425 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:08,432 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,432 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,432 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,432 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:08,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:08,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:08,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:08,449 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,450 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,450 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:08,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:08,463 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:08,473 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,473 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,474 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,474 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,474 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:08,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:08,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:08,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:08,483 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,483 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,483 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:08,485 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:08,491 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:08,498 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,499 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,499 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,499 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:08,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:08,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:08,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:08,510 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,510 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,510 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:08,512 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:08,519 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:08,526 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,526 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,526 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,526 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:08,530 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:08,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:08,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:08,537 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,538 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,538 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:08,539 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:08,546 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:08,553 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,554 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,554 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,554 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:08,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:08,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:08,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:08,564 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,565 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,565 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:08,567 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:08,574 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:08,575 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,575 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,575 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,575 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:08,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:08,581 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:08,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:08,585 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-07 11:15:08,586 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-07 11:15:08,586 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:08,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:08,588 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:08,589 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,589 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,589 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,589 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:08,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:08,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:08,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:08,590 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:08,590 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:08,590 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:08,591 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:08,591 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:08,592 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,592 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,592 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,592 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:08,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:08,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:08,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:08,629 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:08,629 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:08,629 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:08,637 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:08,638 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:08,639 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:08,639 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,639 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:08,639 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:08,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:08,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:08,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:08,640 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:08,640 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:08,640 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:08,641 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:08,642 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:08,649 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 34]), 33)
2023-10-07 11:15:08,649 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,649 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 34]), 33)
2023-10-07 11:15:08,649 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:08,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:08,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:08,650 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:08,650 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:08,650 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:08,651 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:08,651 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:08,657 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:08,665 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,665 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,665 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,665 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:08,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:08,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:08,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:08,677 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,678 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,678 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:08,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:08,687 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:08,694 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,694 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,694 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,694 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:08,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:08,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:08,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:08,705 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,706 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,706 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:08,708 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:08,715 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:08,722 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,723 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,723 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,723 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:08,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:08,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:08,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:08,734 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,735 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,735 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:08,737 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:08,743 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:08,750 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,751 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,751 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,751 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:08,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:08,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:08,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:08,764 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,764 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,764 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:08,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:08,773 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:08,780 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,780 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,780 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,781 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:08,784 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:08,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:08,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:08,791 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,791 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,791 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:08,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:08,800 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:08,807 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,807 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,807 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,807 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:08,810 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:08,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:08,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:08,818 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:08,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:08,827 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:08,834 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,834 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,834 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,834 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:08,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:08,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:08,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:08,843 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,843 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,843 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:08,845 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:08,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:08,859 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,859 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,859 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,859 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:08,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:08,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:08,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:08,868 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,869 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,869 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:08,870 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:08,877 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:08,883 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,883 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,884 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,884 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:08,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:08,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:08,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:08,892 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,893 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,893 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:08,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:08,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:08,908 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,908 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,908 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,909 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:08,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:08,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:08,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:08,918 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,919 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,919 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:08,920 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:08,927 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:08,934 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,935 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,935 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,935 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:08,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:08,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:08,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:08,949 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,950 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,950 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:08,952 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:08,958 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:08,959 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,960 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,960 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,960 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:08,960 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:08,963 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:08,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:08,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:08,970 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-07 11:15:08,971 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-07 11:15:08,971 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:08,972 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:08,973 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:08,974 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,974 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,974 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,974 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:08,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:08,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:08,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:08,975 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:08,975 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:08,975 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:08,976 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:08,976 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:08,977 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:08,977 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:08,977 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:08,977 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:08,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:08,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:08,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:08,999 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:09,006 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:09,006 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:09,006 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:09,014 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:09,015 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:09,015 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:09,016 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,016 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:09,016 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:09,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:09,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:09,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:09,017 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:09,017 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:09,017 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:09,017 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:09,018 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:09,025 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 35]), 34)
2023-10-07 11:15:09,025 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,025 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 35]), 34)
2023-10-07 11:15:09,025 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:09,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:09,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:09,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:09,027 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:09,027 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:09,027 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:09,027 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:09,034 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:09,041 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,041 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,041 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,042 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:09,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:09,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:09,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:09,051 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,051 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,052 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:09,053 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:09,060 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:09,067 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,067 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,067 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,067 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:09,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:09,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:09,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:09,076 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,077 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,077 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:09,078 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:09,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:09,092 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,092 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,092 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,092 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:09,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:09,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:09,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:09,101 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,102 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,102 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:09,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:09,110 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:09,117 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,117 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,118 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,118 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,118 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:09,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:09,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:09,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:09,127 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,127 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,127 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:09,129 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:09,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:09,143 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,143 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,143 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,143 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:09,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:09,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:09,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:09,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,153 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:09,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:09,161 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:09,168 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,168 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,168 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:09,171 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:09,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:09,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:09,177 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,177 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,178 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:09,179 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:09,186 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:09,193 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,193 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,193 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,194 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:09,197 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:09,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:09,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:09,203 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,203 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,204 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:09,205 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:09,212 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:09,219 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,219 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,219 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,219 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:09,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:09,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:09,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:09,247 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,247 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,247 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:09,249 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:09,255 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:09,263 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,263 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,263 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,263 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:09,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:09,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:09,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:09,272 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,272 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,272 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:09,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:09,281 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:09,288 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,288 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,288 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,288 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:09,291 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:09,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:09,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:09,297 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,298 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,298 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:09,299 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:09,306 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:09,313 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,314 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,314 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,314 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:09,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:09,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:09,321 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:09,323 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,323 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,324 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:09,325 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:09,332 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:09,333 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,333 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,333 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,334 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:09,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:09,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:09,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:09,342 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-07 11:15:09,342 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-07 11:15:09,343 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:09,344 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:09,345 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:09,345 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,346 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,346 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,346 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:09,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:09,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:09,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:09,347 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:09,347 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:09,347 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:09,347 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:09,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:09,349 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,349 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,349 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,349 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:09,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:09,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:09,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:09,378 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:09,379 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:09,380 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:09,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:09,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:09,388 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:09,388 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,388 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:09,388 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:09,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:09,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:09,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:09,389 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:09,390 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:09,390 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:09,390 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:09,391 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:09,398 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 36]), 35)
2023-10-07 11:15:09,398 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,398 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 36]), 35)
2023-10-07 11:15:09,398 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:09,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:09,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:09,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:09,399 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:09,400 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:09,400 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:09,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:09,407 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:09,414 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,414 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,414 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,414 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:09,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:09,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:09,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:09,425 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,425 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,425 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:09,427 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:09,434 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:09,441 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,441 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,441 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,441 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:09,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:09,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:09,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:09,470 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,471 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,472 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:09,474 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:09,481 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:09,488 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,488 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,488 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,489 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:09,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:09,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:09,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:09,499 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,499 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,500 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:09,502 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:09,508 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:09,515 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,515 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,515 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,515 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:09,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:09,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:09,525 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:09,526 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,527 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,527 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:09,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:09,536 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:09,543 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,543 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,543 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,543 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:09,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:09,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:09,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:09,552 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,552 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,552 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:09,554 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:09,561 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:09,567 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,568 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,568 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,568 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:09,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:09,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:09,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:09,576 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,577 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,577 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:09,578 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:09,585 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:09,592 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,592 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,592 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,593 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:09,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:09,598 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:09,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:09,603 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,603 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,603 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:09,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:09,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:09,618 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,618 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,618 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,619 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:09,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:09,623 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:09,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:09,628 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,628 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,628 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:09,630 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:09,636 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:09,643 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,643 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,643 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,643 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:09,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:09,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:09,651 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:09,652 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,653 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,653 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:09,655 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:09,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:09,668 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,668 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,668 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,668 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:09,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:09,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:09,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:09,680 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,681 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,681 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:09,682 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:09,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:09,696 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,696 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,696 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,696 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:09,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:09,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:09,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:09,707 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,707 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,707 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:09,709 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:09,716 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:09,717 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,717 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,717 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,717 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:09,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:09,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:09,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:09,727 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-07 11:15:09,728 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-07 11:15:09,728 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:09,729 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:09,730 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:09,731 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,731 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,731 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,731 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:09,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:09,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:09,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:09,732 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:09,732 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:09,733 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:09,733 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:09,733 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:09,734 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,734 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,734 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,734 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:09,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:09,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:09,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:09,767 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:09,767 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:09,768 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:09,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:09,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:09,776 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:09,776 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,776 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:09,776 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:09,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:09,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:09,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:09,777 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:09,777 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:09,778 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:09,778 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:09,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:09,785 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 37]), 36)
2023-10-07 11:15:09,786 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:09,786 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 37]), 36)
2023-10-07 11:15:09,786 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:09,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:09,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:09,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:09,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:09,787 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:09,787 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:09,787 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:09,788 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:09,794 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:09,801 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,801 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,801 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,801 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:09,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:09,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:09,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:09,811 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:09,811 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:09,811 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:09,813 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:09,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:09,827 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,827 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,827 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,827 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:09,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:09,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:09,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:09,835 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:09,836 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:09,836 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:09,838 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:09,844 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:09,851 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,851 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,852 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,852 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:09,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:09,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:09,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:09,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:09,860 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:09,860 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:09,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:09,868 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:09,875 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,875 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,875 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,876 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:09,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:09,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:09,882 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:09,883 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:09,884 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:09,884 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:09,886 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:09,892 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:09,899 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,899 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,899 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,900 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:09,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:09,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:09,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:09,908 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:09,908 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:09,909 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:09,910 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:09,917 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:09,924 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,924 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,924 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,924 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:09,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:09,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:09,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:09,933 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:09,933 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:09,934 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:09,935 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:09,942 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:09,949 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,949 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,950 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,950 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:09,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:09,954 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:09,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:09,958 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:09,959 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:09,959 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:09,960 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:09,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:09,974 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,974 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,974 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:09,974 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:09,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:09,978 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:09,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:09,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:09,984 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:09,984 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:09,984 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:09,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:09,992 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:09,999 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:09,999 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,000 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,000 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:10,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:10,005 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:10,008 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:10,010 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:10,010 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:10,010 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:10,012 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:10,019 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:10,025 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,026 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,026 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,026 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:10,030 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:10,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:10,035 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:10,036 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:10,037 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:10,037 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:10,038 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:10,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:10,052 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,052 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,053 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,053 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:10,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:10,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:10,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:10,062 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:10,062 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:10,063 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:10,064 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:10,071 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:10,072 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,072 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,072 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,072 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:10,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:10,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:10,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:10,081 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-07 11:15:10,081 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-07 11:15:10,081 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:10,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:10,083 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:10,084 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,084 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,084 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,084 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:10,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:10,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:10,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:10,085 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:10,086 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:10,086 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:10,086 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:10,087 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:10,087 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,087 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,087 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,087 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,088 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:10,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:10,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:10,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:10,116 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:10,117 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:10,117 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:10,124 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:10,125 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:10,125 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:10,125 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,125 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:10,126 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:10,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:10,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:10,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:10,126 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:10,127 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:10,127 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:10,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:10,128 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:10,135 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 38]), 37)
2023-10-07 11:15:10,135 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,135 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 38]), 37)
2023-10-07 11:15:10,135 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,135 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:10,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:10,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:10,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:10,136 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:10,136 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:10,137 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:10,137 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:10,143 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:10,150 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,150 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,150 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,150 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:10,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:10,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:10,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:10,162 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,162 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,162 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:10,164 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:10,171 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:10,178 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,178 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,178 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,178 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:10,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:10,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:10,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:10,188 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,189 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,189 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:10,191 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:10,197 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:10,204 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,204 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,205 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,205 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:10,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:10,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:10,212 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:10,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,214 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:10,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:10,223 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:10,230 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,230 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,230 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,230 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,231 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:10,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:10,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:10,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:10,239 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,240 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,240 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:10,242 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:10,248 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:10,255 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,255 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,256 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,256 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:10,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:10,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:10,263 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:10,265 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,266 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,266 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:10,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:10,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:10,281 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,281 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,281 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,281 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:10,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:10,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:10,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:10,290 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,291 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,291 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:10,292 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:10,299 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:10,306 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,306 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,306 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,307 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:10,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:10,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:10,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:10,316 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,317 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,317 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:10,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:10,325 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:10,332 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,332 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,332 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,332 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:10,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:10,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:10,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:10,341 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,342 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,342 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:10,343 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:10,350 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:10,357 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,357 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,358 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,358 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:10,361 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:10,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:10,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:10,367 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,367 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,367 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:10,369 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:10,376 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:10,384 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,384 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,384 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,385 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:10,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:10,390 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:10,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:10,394 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,394 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,395 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:10,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:10,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:10,411 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,411 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,412 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,412 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:10,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:10,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:10,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:10,421 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,421 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,422 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:10,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:10,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:10,432 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,432 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,432 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,432 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:10,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:10,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:10,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:10,441 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-07 11:15:10,442 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-07 11:15:10,442 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:10,443 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:10,445 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:10,445 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,445 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,445 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,446 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:10,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:10,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:10,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:10,447 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:10,447 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:10,447 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:10,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:10,448 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:10,449 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,449 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,449 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,449 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:10,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:10,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:10,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:10,482 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:10,492 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:10,493 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:10,499 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:10,500 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:10,501 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-07 11:15:10,501 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,501 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-07 11:15:10,502 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-07 11:15:10,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-07 11:15:10,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-07 11:15:10,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-07 11:15:10,503 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:10,504 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:10,504 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-07 11:15:10,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-07 11:15:10,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:10,512 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 39]), 38)
2023-10-07 11:15:10,512 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,512 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 39]), 38)
2023-10-07 11:15:10,512 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-07 11:15:10,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-07 11:15:10,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-07 11:15:10,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-07 11:15:10,514 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:10,514 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:10,514 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-07 11:15:10,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-07 11:15:10,521 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:10,529 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,529 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,529 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,529 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-07 11:15:10,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-07 11:15:10,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-07 11:15:10,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-07 11:15:10,538 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,539 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,539 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-07 11:15:10,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-07 11:15:10,548 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:10,556 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,556 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,556 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,556 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-07 11:15:10,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-07 11:15:10,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-07 11:15:10,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-07 11:15:10,566 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,566 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,566 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-07 11:15:10,568 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-07 11:15:10,576 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:10,583 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,583 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,583 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,583 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,584 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-07 11:15:10,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-07 11:15:10,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-07 11:15:10,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-07 11:15:10,594 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,595 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,595 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-07 11:15:10,597 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-07 11:15:10,604 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:10,612 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,612 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,612 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,612 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-07 11:15:10,616 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-07 11:15:10,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-07 11:15:10,620 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-07 11:15:10,622 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,623 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,623 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-07 11:15:10,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-07 11:15:10,632 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:10,639 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,639 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,640 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,640 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-07 11:15:10,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-07 11:15:10,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-07 11:15:10,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-07 11:15:10,649 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,650 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,650 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-07 11:15:10,652 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-07 11:15:10,659 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:10,666 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,666 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,666 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,667 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,667 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-07 11:15:10,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-07 11:15:10,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-07 11:15:10,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-07 11:15:10,675 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,675 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,676 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-07 11:15:10,677 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-07 11:15:10,684 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:10,692 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,692 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,692 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,692 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-07 11:15:10,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-07 11:15:10,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-07 11:15:10,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-07 11:15:10,702 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,702 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,703 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-07 11:15:10,704 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-07 11:15:10,712 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:10,719 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,719 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,719 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,720 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-07 11:15:10,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-07 11:15:10,726 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-07 11:15:10,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-07 11:15:10,729 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,730 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,730 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-07 11:15:10,731 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-07 11:15:10,739 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:10,747 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,747 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,747 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,747 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-07 11:15:10,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-07 11:15:10,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-07 11:15:10,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-07 11:15:10,756 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,756 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,756 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-07 11:15:10,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-07 11:15:10,765 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:10,773 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,773 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,773 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,773 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-07 11:15:10,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-07 11:15:10,778 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-07 11:15:10,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-07 11:15:10,782 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,782 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,782 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-07 11:15:10,784 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-07 11:15:10,790 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:10,798 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,798 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,798 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,798 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-07 11:15:10,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-07 11:15:10,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-07 11:15:10,806 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-07 11:15:10,807 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,808 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,808 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-07 11:15:10,810 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-07 11:15:10,816 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:10,817 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,818 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,818 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,818 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 39]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-07 11:15:10,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-07 11:15:10,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-07 11:15:10,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-07 11:15:10,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-07 11:15:10,827 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 39, 64]), torch.Size([2, 12, 39, 64])))
2023-10-07 11:15:10,827 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 39, 64]), torch.Size([8, 12, 39, 64])))
2023-10-07 11:15:10,827 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-07 11:15:10,829 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-07 11:15:10,830 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:10,830 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,830 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,831 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,831 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-07 11:15:10,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-07 11:15:10,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-07 11:15:10,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-07 11:15:10,832 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-07 11:15:10,832 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-07 11:15:10,832 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-07 11:15:10,832 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-07 11:15:10,833 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-07 11:15:10,834 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-07 11:15:10,834 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-07 11:15:10,834 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-07 11:15:10,834 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-07 11:15:10,834 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-07 11:15:10,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-07 11:15:10,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-07 11:15:10,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-07 11:15:10,863 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-07 11:15:10,863 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-07 11:15:10,863 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-07 11:15:10,871 [flexgen_test.py:30 in test_hf_gen] INFO - Who are you? Are you conscious?okay, we'll put it right, you're just a simple little brain, we'll just put the words on the back of the fucking keyboard
2023-10-07 11:15:10,871 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:15:10,871 [flexgen_test.py:30 in test_hf_gen] INFO - Where is Deutschland?: An international film festival
The German government has set a new target of filming a foreign cinema festival dubbed Deutschland after a report of an extremist
2023-10-07 11:15:10,871 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:15:10,871 [flexgen_test.py:30 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?X
Huawei Mate 60 Pro: Battery Life
The Huawei Mate 60 Pro has been one of the most popular smartphones in China for many years now
2023-10-07 11:15:10,871 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:15:10,871 [flexgen_test.py:30 in test_hf_gen] INFO - Who are you? Are you conscious?�💀🚁
I'm a student at St. Joseph Catholic Church in Fort Lauderdale, FL!
Ouch, sorry to
2023-10-07 11:15:10,871 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:15:10,872 [flexgen_test.py:30 in test_hf_gen] INFO - Where is Deutschland?><br /><div style="text-align: center;"><img alt="" data-align: center; border="0" src="https
2023-10-07 11:15:10,872 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:15:10,872 [flexgen_test.py:30 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?APP - Latest information
How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a wireless premium smartphone powered by the MediaTek Hel
2023-10-07 11:15:10,872 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:15:10,872 [flexgen_test.py:30 in test_hf_gen] INFO - Who are you? Are you conscious?�
I feel like this is a normal question here, but how do I know?  I guess if I know what the character is feeling,
2023-10-07 11:15:10,872 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:15:10,872 [flexgen_test.py:30 in test_hf_gen] INFO - Where is Deutschland?...
I don't know what that is
2023-10-07 11:15:10,872 [flexgen_test.py:31 in test_hf_gen] INFO - ----------
2023-10-07 11:15:10,881 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-07 11:15:10,881 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-07 11:15:10,881 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-07 11:15:10,881 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-07 11:15:10,881 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-07 11:15:10,881 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-07 11:15:10,882 [flexgen_forward.py:22 in to_old_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-07 11:15:10,883 [flexgen_forward.py:22 in to_old_forward] DEBUG - lm_head from flexgen to old.
