2023-10-11 15:31:04,063 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp0kvk50re
2023-10-11 15:31:04,064 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp0kvk50re/_remote_module_non_scriptable.py
2023-10-11 15:31:04,454 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-11 15:31:04,515 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:31:05,969 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-11 15:31:06,247 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 15:31:06,247 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 15:31:06,247 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 15:31:06,247 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 15:31:07,032 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:31:07,115 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 15:31:07,219 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:31:07,301 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 15:31:07,301 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 15:31:07,308 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 15:31:07,308 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 15:31:07,309 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 15:31:07,310 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 15:31:07,311 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 15:31:07,312 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 15:31:07,313 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 15:31:07,313 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 15:31:07,314 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 15:31:07,315 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 15:31:07,316 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 15:31:07,317 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 15:31:07,318 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 15:31:07,319 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 15:31:07,320 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 15:31:07,320 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 15:31:07,320 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 15:31:07,322 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 15:31:07,323 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 15:31:07,349 [forward.py:48 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 15:31:07,349 [forward.py:48 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 15:31:07,349 [forward.py:48 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 15:31:07,349 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 15:31:07,349 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 15:31:07,349 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 15:31:07,349 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 15:31:07,350 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 15:31:07,350 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 15:31:07,350 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 15:31:07,350 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 15:31:07,350 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 15:31:07,350 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 15:31:07,350 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 15:31:07,350 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 15:31:07,351 [forward.py:48 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 15:31:07,397 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 15:31:07,545 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:07,546 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:07,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:07,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:07,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:07,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:07,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:07,590 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:07,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:07,599 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:07,601 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:07,608 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:07,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:07,617 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:07,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:07,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:07,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:07,635 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:07,637 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:07,644 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:07,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:07,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:07,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:07,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:07,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:07,669 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:07,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:07,679 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:07,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:07,681 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:07,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:07,692 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:07,698 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 15:31:07,698 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:07,706 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 15:31:07,706 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 15:31:07,706 [forward.py:28 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 15:31:07,707 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 15:31:07,707 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 15:31:07,707 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 15:31:07,707 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 15:31:07,707 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 15:31:07,707 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 15:31:07,707 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 15:31:07,708 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 15:31:07,708 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 15:31:07,708 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 15:31:07,708 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 15:31:07,709 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 15:31:07,709 [forward.py:28 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 15:31:07,709 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 15:31:07,709 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 15:31:07,710 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 15:31:07,710 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 15:31:07,710 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 15:31:07,710 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 15:31:07,711 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 15:31:07,711 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 15:31:07,711 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 15:31:07,711 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 15:31:07,712 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 15:31:07,712 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 15:31:07,712 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 15:31:07,712 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 15:31:07,713 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 15:31:07,713 [forward.py:120 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 15:31:07,757 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 15:31:07,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:07,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:07,889 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 15:31:07,889 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:07,890 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:07,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:07,892 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:07,892 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:07,893 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])
2023-10-11 15:31:07,893 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:07,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:07,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:07,898 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-11 15:31:07,899 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:07,899 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:07,900 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:07,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:07,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:07,902 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])
2023-10-11 15:31:07,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:07,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:07,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:07,913 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:07,914 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:07,935 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,949 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,949 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:07,949 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:07,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:07,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:07,957 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:07,957 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:07,963 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,967 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,973 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,977 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,977 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:07,977 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:07,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:07,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:07,986 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:07,986 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:07,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:07,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,005 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,006 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,006 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:08,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:08,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:08,014 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,014 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,034 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,034 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,034 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:08,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:08,039 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:08,043 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,043 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,056 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,069 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:08,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:08,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:08,078 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,078 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,094 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,094 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,094 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:08,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:08,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:08,102 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,103 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,115 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,118 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,119 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:08,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:08,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:08,127 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,127 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,135 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,139 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,143 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,143 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:08,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:08,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:08,151 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,151 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,167 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,167 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,167 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:08,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:08,172 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:08,175 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,176 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,195 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,195 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:08,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:08,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:08,203 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,203 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,208 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,215 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,219 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,219 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:08,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:08,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:08,224 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,224 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,240 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:31:08,240 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:31:08,240 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:08,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:08,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:08,242 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,243 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:08,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:08,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:08,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:08,246 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:31:08,246 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])
2023-10-11 15:31:08,246 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:08,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:08,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:08,247 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:31:08,248 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:08,261 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-11 15:31:08,273 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-11 15:31:08,285 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-11 15:31:08,299 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-11 15:31:08,300 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 50272])
2023-10-11 15:31:08,300 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:08,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:08,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:08,314 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:08,314 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:08,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,318 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:08,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:08,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:08,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:08,323 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 10])", "<class 'int'>: 9")
2023-10-11 15:31:08,324 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:08,324 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,325 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,325 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,326 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,326 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:08,326 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:08,329 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:08,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:08,336 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,336 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,345 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,352 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,352 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:08,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:08,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:08,361 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,361 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,373 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,377 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,377 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,377 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:08,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:08,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:08,386 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,386 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,390 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,400 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,404 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:08,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:08,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:08,412 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,412 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,418 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,422 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,430 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,430 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:08,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:08,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:08,438 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,439 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,443 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,450 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,454 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:08,456 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:08,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:08,462 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,463 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,467 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,474 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,479 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,479 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:08,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:08,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:08,487 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,487 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,503 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:08,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:08,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:08,511 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,511 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,520 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,527 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,528 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,528 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:08,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:08,532 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:08,536 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,536 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,552 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,552 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:08,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:08,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:08,560 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,560 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,575 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,575 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:08,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:08,580 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:08,583 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,584 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,588 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,592 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,598 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,598 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:08,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:08,603 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:08,604 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,604 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,609 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:31:08,619 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:31:08,619 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:08,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:08,621 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:08,621 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,622 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:08,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,625 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,625 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:08,625 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:08,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:08,626 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:08,626 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,626 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:08,635 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:08,642 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:08,650 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:08,658 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:08,658 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:08,658 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:08,665 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:08,665 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:08,665 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:08,666 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:08,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,668 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:08,668 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:08,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:08,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:08,673 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 11])", "<class 'int'>: 10")
2023-10-11 15:31:08,673 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:08,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:08,676 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:08,676 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:08,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:08,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:08,685 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,685 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,724 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,746 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,754 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,754 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:08,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:08,759 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:08,762 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,763 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,778 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:08,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:08,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:08,787 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,787 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,803 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,803 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,803 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:08,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:08,808 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:08,812 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,812 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,816 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,824 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,828 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:08,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:08,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:08,836 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,837 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,854 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,854 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:08,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:08,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:08,863 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,863 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,875 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,879 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,879 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:08,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:08,884 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:08,888 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,888 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,898 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,905 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,905 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:08,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:08,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:08,914 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,914 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,926 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,930 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,930 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:08,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:08,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:08,939 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:08,939 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:08,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,983 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:08,991 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:08,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:08,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:08,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:09,000 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,001 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,018 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,018 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:09,018 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:09,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:09,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:09,027 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,027 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,047 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:09,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:09,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:09,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:09,053 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,053 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:31:09,071 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:31:09,072 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:09,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:09,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:09,074 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,074 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,075 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,076 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,076 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,077 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,078 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,078 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:09,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:09,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:09,079 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,079 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,088 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,096 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,104 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,111 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,112 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:09,112 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:09,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:09,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:09,119 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:09,120 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,123 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,123 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:09,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:09,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:09,128 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 12])", "<class 'int'>: 11")
2023-10-11 15:31:09,128 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,129 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,129 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,131 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:09,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:09,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:09,141 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,141 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,149 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,153 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,156 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,157 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:09,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:09,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:09,165 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,165 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,173 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,199 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,199 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:09,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:09,204 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:09,207 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,207 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,213 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,220 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,224 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,225 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:09,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:09,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:09,234 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,234 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,239 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,247 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,251 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,251 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,252 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:09,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:09,257 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:09,260 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,260 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,273 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,284 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,284 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:09,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:09,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:09,293 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,293 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,298 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,301 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,309 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,309 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:09,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:09,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:09,319 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,319 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,324 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,333 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,336 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,337 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,337 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:09,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:09,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:09,347 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,347 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,352 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,363 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,363 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:09,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:09,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:09,373 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,373 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,382 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,391 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,391 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:09,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:09,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:09,399 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,400 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,416 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,416 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:09,417 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:09,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:09,423 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,424 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,432 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,436 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,440 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,440 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,441 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:09,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:09,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:09,446 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,446 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,489 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,493 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:31:09,493 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:31:09,493 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:09,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:09,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:09,496 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,496 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,499 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,499 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:09,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:09,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:09,500 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,500 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,509 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,516 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,524 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,532 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,532 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:09,533 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:09,538 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:09,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:09,539 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:09,539 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,542 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,542 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:09,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:09,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:09,546 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 13])", "<class 'int'>: 12")
2023-10-11 15:31:09,547 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,549 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,550 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:09,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:09,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:09,559 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,560 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,564 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,576 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:09,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:09,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:09,584 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,584 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,602 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,618 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,618 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:09,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:09,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:09,626 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,627 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,643 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:09,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:09,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:09,651 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,651 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,668 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,668 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:09,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:09,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:09,677 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,677 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,693 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,693 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,693 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:09,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:09,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:09,701 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,701 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,706 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,718 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,718 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,718 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:09,719 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:09,723 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:09,726 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,726 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,731 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,738 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,742 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:09,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:09,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:09,750 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,751 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,759 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,763 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,766 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,766 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,766 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:09,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:09,771 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:09,775 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,775 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,787 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,791 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,791 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,791 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:09,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:09,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:09,799 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,799 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,804 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,812 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,816 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,816 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,816 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:09,817 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:09,820 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:09,824 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,824 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,829 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,840 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,840 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:09,842 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:09,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:09,846 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,846 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,858 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:31:09,862 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:31:09,862 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:09,863 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:09,864 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:09,864 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,864 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,873 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:09,874 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:09,874 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:09,874 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,875 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,889 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,897 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,908 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,919 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:09,920 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:09,920 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:09,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:09,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:09,950 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:09,950 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,952 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,952 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,953 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,953 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,953 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:09,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:09,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:09,958 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 14])", "<class 'int'>: 13")
2023-10-11 15:31:09,958 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:09,959 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,959 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,961 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:09,961 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:09,961 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:09,964 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:09,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:09,971 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:09,971 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:09,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:09,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:09,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,002 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,003 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,003 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:10,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:10,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:10,012 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,012 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,018 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,036 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,036 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,036 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:10,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:10,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:10,044 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,044 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,063 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,063 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:10,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:10,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:10,073 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,073 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,078 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,091 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,092 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,092 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:10,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:10,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:10,101 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,101 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,129 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,133 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,133 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,133 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:10,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:10,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:10,141 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,141 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,150 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,153 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,157 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,157 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:10,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:10,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:10,165 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,165 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,174 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,178 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,182 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,182 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:10,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:10,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:10,189 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,189 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,200 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,205 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,210 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,210 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:10,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:10,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:10,218 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,218 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,235 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,235 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:10,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:10,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:10,244 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,244 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,254 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,262 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,263 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,263 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:10,264 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:10,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:10,271 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,272 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,288 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,289 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,289 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:10,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:10,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:10,294 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,294 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,300 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,314 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,324 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:31:10,324 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:31:10,324 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:10,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:10,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:10,326 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,326 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:10,327 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,330 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:10,330 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:10,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:10,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:10,331 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,331 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:10,340 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:10,347 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:10,355 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:10,362 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:10,362 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:10,362 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:10,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:10,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:10,394 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:10,395 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:10,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,396 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,397 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:10,398 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:10,398 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:10,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:10,402 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 15])", "<class 'int'>: 14")
2023-10-11 15:31:10,402 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:10,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,405 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:10,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:10,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:10,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:10,415 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,416 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,434 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,439 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,439 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:10,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:10,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:10,447 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,447 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,456 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,460 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,465 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,465 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:10,467 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:10,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:10,474 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,474 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,493 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,493 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:10,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:10,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:10,501 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,501 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,506 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,522 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,522 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:10,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:10,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:10,531 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,531 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,536 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,549 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:10,551 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:10,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:10,558 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,559 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,577 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,577 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:10,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:10,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:10,585 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,585 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,603 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,603 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,604 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:10,605 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:10,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:10,612 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,612 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,647 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,647 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,647 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:10,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:10,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:10,656 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,656 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,672 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,672 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,672 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:10,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:10,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:10,681 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,681 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,686 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,694 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,698 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,698 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:10,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:10,702 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:10,705 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,706 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,743 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,743 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,743 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:10,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:10,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:10,749 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,749 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,758 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,763 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:31:10,767 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:31:10,767 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:10,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:10,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:10,770 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,770 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:10,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,772 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,773 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:10,774 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:10,774 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:10,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:10,775 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,775 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:10,787 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:10,795 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:10,802 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:10,810 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:10,810 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:10,811 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:10,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:10,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:10,817 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:10,817 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:10,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,819 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,820 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:10,820 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:10,821 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:10,821 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:10,825 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 16])", "<class 'int'>: 15")
2023-10-11 15:31:10,825 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:10,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:10,828 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:10,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:10,830 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:10,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:10,837 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,837 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,842 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,846 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,854 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:10,854 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:10,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:10,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:10,862 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,862 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,897 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,901 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:10,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:10,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:10,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:10,910 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,910 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,915 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,926 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,926 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:10,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:10,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:10,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:10,935 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,935 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,951 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:10,952 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:10,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:10,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:10,960 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,960 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,976 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:10,976 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:10,978 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:10,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:10,984 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:10,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:10,989 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,993 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:10,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,001 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:11,001 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:11,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:11,005 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:11,009 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,009 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,018 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,022 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,025 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:11,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:11,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:11,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:11,034 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,034 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,050 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:11,050 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:11,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:11,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:11,058 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,058 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,075 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,075 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:11,075 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:11,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:11,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:11,083 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,084 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,088 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,096 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,100 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:11,100 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:11,101 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:11,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:11,108 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,108 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,135 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,136 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:11,136 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:11,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:11,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:11,141 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,141 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,150 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:31:11,158 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:31:11,158 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:11,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:11,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:11,160 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,160 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:11,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,164 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:11,164 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:11,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:11,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:11,165 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,165 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:11,174 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:11,181 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:11,188 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:11,195 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:11,196 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:11,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:11,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:11,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:11,202 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:11,202 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:11,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,205 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,205 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:11,205 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:11,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:11,206 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:11,209 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 17])", "<class 'int'>: 16")
2023-10-11 15:31:11,209 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:11,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,211 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,212 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:11,212 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:11,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:11,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:11,221 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,222 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,230 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,238 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,238 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,238 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:11,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:11,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:11,245 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,246 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,250 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,254 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,262 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,262 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:11,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:11,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:11,270 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,270 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,291 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,295 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,295 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,296 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:11,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:11,300 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:11,304 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,304 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,322 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,322 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,322 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:11,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:11,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:11,330 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,330 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,343 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,349 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,349 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:11,351 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:11,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:11,357 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,357 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,362 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,397 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,401 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,401 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:11,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:11,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:11,410 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,410 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,419 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,423 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,427 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,428 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:11,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:11,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:11,435 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,436 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,440 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,456 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,457 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,457 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:11,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:11,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:11,465 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,465 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,470 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,474 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,482 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,482 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:11,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:11,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:11,491 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,491 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,509 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:11,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:11,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:11,518 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,518 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,529 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,533 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,538 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,538 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,538 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:11,540 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:11,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:11,544 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,544 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:31:11,777 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:31:11,777 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:11,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:11,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:11,779 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,779 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:11,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,788 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,791 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,792 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:11,793 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:11,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:11,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:11,795 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,795 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:11,805 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:11,813 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:11,822 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:11,829 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:11,830 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:11,830 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:11,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:11,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:11,838 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:11,838 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:11,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,841 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,842 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:11,842 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:11,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:11,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:11,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 18])", "<class 'int'>: 17")
2023-10-11 15:31:11,848 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:11,849 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,851 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:11,852 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:11,852 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:11,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:11,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:11,863 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,863 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,885 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,890 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:11,890 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:11,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:11,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:11,898 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,898 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,904 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,916 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:11,916 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:11,918 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:11,921 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:11,925 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,925 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,934 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,938 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,942 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:11,942 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:11,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:11,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:11,950 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,950 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,968 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,969 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:11,969 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:11,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:11,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:11,978 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:11,978 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:11,983 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,992 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:11,996 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:11,996 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:11,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:12,001 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:12,005 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,005 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,018 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,022 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,022 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:12,022 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:12,023 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:12,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:12,031 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,031 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,036 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,040 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,044 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,048 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,049 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:12,049 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:12,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:12,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:12,057 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,057 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,075 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,080 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,080 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:12,080 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:12,081 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:12,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:12,088 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,088 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,105 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,106 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:12,106 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:12,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:12,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:12,114 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,114 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,131 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:12,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:12,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:12,136 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:12,139 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,140 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,188 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,188 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:12,189 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:12,190 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:12,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:12,194 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,194 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,200 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,208 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:31:12,213 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:31:12,213 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:12,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:12,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:12,215 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,215 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:12,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,217 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,218 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:12,218 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:12,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:12,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:12,219 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,219 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:12,230 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:12,241 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:12,252 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:12,262 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:12,262 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:12,263 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:12,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:12,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:12,269 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:12,269 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:12,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,272 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:12,272 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:12,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:12,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:12,280 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 19])", "<class 'int'>: 18")
2023-10-11 15:31:12,280 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:12,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,284 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:12,285 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:12,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:12,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:12,301 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,301 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,308 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,312 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,320 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,320 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:12,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:12,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:12,328 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,328 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,333 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,351 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,351 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:12,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:12,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:12,360 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,360 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,373 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,377 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,377 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,378 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:12,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:12,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:12,386 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,386 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,396 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,400 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,404 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,404 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:12,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:12,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:12,413 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,413 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,418 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,422 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,430 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,431 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:12,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:12,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:12,439 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,439 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,460 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,460 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,460 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:12,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:12,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:12,468 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,469 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,476 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,488 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:12,489 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:12,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:12,495 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,496 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,512 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,515 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,516 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:12,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:12,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:12,523 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,523 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,528 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,532 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,536 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,540 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,540 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:12,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:12,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:12,548 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,548 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,553 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,564 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,564 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,565 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:12,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:12,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:12,573 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,573 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,585 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,598 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,598 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:12,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:12,603 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:12,603 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,603 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,626 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:31:12,626 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:31:12,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:12,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:12,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:12,628 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,628 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:12,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,640 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:12,640 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:12,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:12,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:12,641 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,641 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:12,651 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:12,658 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:12,666 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:12,673 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:12,674 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:12,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:12,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:12,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:12,681 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:12,681 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:12,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,683 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,683 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,684 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:12,684 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:12,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:12,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:12,689 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 20])", "<class 'int'>: 19")
2023-10-11 15:31:12,689 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:12,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:12,692 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:12,692 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:12,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:12,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:12,701 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,701 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,711 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,719 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,719 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:12,720 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:12,724 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:12,728 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,728 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,749 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,758 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,762 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,762 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,762 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:12,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:12,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:12,771 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,771 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,788 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,788 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,788 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:12,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:12,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:12,796 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,796 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,810 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,814 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,814 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,814 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:12,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:12,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:12,822 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,822 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,831 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,835 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,840 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,840 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:12,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:12,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:12,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,848 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,866 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:12,867 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:12,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:12,874 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,874 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,892 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,896 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,896 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,896 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:12,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:12,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:12,905 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,905 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,910 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,923 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,923 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:12,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:12,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:12,931 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,931 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,948 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,949 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,949 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:12,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:12,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:12,957 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,957 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:12,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,983 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:12,991 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:12,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:12,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:12,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:12,999 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:12,999 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,005 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:13,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:13,018 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:13,022 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:13,022 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:13,022 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:13,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:13,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:13,027 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,028 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:13,037 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:13,041 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:13,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:31:13,045 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:31:13,046 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:13,047 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:13,047 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:13,048 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,048 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,048 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,051 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,051 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,051 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:13,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:13,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:13,052 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,052 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,062 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,069 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,076 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,082 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,083 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:13,083 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:13,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:13,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:13,090 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:13,090 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,091 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,091 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,093 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:13,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:13,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:13,097 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 21])", "<class 'int'>: 20")
2023-10-11 15:31:13,097 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,098 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,099 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,101 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,101 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:13,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:13,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:13,110 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,110 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,115 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,146 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,146 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:13,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:13,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:13,155 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,155 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,165 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,169 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,173 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,173 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:13,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:13,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:13,181 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,181 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,195 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,203 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,203 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:13,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:13,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:13,211 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,212 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,217 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,257 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,257 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,257 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:13,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:13,262 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:13,266 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,266 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,283 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,283 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:13,285 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:13,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:13,291 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,292 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,301 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,310 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:13,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:13,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:13,318 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,318 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,323 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,327 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,343 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,343 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,343 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:13,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:13,348 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:13,351 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,351 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,357 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,370 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,370 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:13,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:13,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:13,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,379 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,388 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,392 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,396 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,397 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:13,398 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:13,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:13,406 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,406 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,425 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,433 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,433 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,433 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:13,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:13,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:13,441 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,441 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,451 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,455 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,472 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,472 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:13,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:13,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:13,477 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,478 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:31:13,497 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:31:13,497 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:13,498 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:13,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:13,499 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,500 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,503 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:13,503 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:13,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:13,504 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,504 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,514 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,521 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,529 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,539 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,540 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:13,540 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:13,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:13,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:13,547 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:13,547 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,550 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,550 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,550 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:13,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:13,551 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:13,555 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 22])", "<class 'int'>: 21")
2023-10-11 15:31:13,555 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,555 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,558 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,558 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:13,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:13,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:13,568 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,568 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,578 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,582 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,586 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,586 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,586 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:13,588 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:13,591 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:13,594 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,595 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,600 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,608 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,613 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,613 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:13,614 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:13,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:13,621 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,621 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,626 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,635 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,639 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:13,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:13,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:13,648 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,648 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,657 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,667 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,667 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:13,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:13,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:13,676 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,676 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,681 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,686 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,695 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,695 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,695 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:13,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:13,700 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:13,704 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,704 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,721 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,722 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:13,723 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:13,726 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:13,730 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,730 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,789 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,795 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,795 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:13,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:13,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:13,803 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,803 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,810 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,814 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,823 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,823 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:13,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:13,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:13,830 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,830 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,849 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,849 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,849 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:13,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:13,854 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:13,857 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,857 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,876 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,876 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,876 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:13,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:13,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:13,883 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,884 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,898 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,902 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:13,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:13,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:13,907 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,907 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:13,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,926 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:31:13,926 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:31:13,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:13,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:13,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:13,928 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,928 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,929 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,931 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,932 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:13,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:13,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:13,933 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:13,933 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,945 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,955 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,965 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,981 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:13,981 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:13,982 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:13,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:13,988 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:13,988 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:13,989 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,989 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,992 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:13,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:13,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:13,996 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 23])", "<class 'int'>: 22")
2023-10-11 15:31:13,996 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:13,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,999 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:13,999 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:13,999 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:14,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:14,005 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:14,008 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,008 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,013 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,018 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,022 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,027 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,027 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:14,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:14,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:14,034 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,035 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,040 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,044 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,053 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,053 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,053 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:14,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:14,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:14,061 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,061 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,066 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,071 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,075 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,079 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,079 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,079 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:14,081 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:14,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:14,087 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,087 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,092 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,104 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,105 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:14,106 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:14,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:14,112 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,112 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,119 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,130 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:14,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:14,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:14,138 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,138 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,148 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,155 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,156 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,156 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:14,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:14,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:14,163 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,163 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,168 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,172 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,181 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,181 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:14,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:14,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:14,188 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,188 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,200 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,208 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,209 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,209 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:14,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:14,213 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:14,216 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,216 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,225 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,233 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,233 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:14,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:14,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:14,241 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,241 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,263 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,267 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,275 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,275 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:14,276 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:14,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:14,283 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,283 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,289 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,297 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,301 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,301 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,301 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:14,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:14,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:14,307 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,307 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,325 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:31:14,325 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:31:14,325 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:14,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:14,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:14,327 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,327 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:14,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,330 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:14,330 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:14,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:14,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:14,332 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,332 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:14,344 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:14,352 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:14,360 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:14,368 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:14,368 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:14,368 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:14,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:14,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:14,376 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:14,376 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:14,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,377 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,377 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,378 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:14,378 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:14,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:14,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:14,383 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 24])", "<class 'int'>: 23")
2023-10-11 15:31:14,383 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:14,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,385 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,385 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,386 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,386 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:14,386 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:14,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:14,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:14,395 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,396 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,409 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,414 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,414 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:14,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:14,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:14,422 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,422 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,431 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,435 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,439 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,440 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,440 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:14,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:14,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:14,448 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,448 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,476 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,480 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,480 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,480 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:14,482 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:14,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:14,488 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,488 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,493 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,501 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,505 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:14,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:14,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:14,513 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,513 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,527 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,531 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,531 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:14,532 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:14,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:14,538 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,538 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,553 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,557 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,557 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:14,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:14,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:14,564 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,565 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,569 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,577 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,581 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,581 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,581 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:14,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:14,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:14,588 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,589 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,603 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,606 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,607 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,607 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:14,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:14,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:14,614 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,614 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,628 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,632 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,632 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,632 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:14,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:14,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:14,639 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,639 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,651 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,655 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,661 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:14,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:14,665 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:14,668 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,668 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,673 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,686 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,686 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:14,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:14,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:14,691 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,692 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,704 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,708 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:31:14,709 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:31:14,709 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:14,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:14,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:14,711 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,711 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:14,712 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,714 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:14,714 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:14,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:14,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:14,715 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,715 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:14,729 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:14,742 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:14,753 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:14,762 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:14,762 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:14,763 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:14,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:14,771 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:14,772 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:14,772 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:14,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,774 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,774 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,775 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:14,775 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:14,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:14,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:14,780 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 25])", "<class 'int'>: 24")
2023-10-11 15:31:14,780 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:14,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:14,783 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:14,783 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:14,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:14,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:14,792 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,793 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,807 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,812 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:14,812 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:14,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:14,817 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:14,820 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,821 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,834 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,840 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:14,840 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:14,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:14,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:14,848 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,848 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,858 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,866 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:14,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:14,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:14,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:14,875 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,875 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,880 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,885 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,894 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:14,894 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:14,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:14,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:14,903 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,903 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,920 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:14,921 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:14,922 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:14,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:14,929 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,929 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,934 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,939 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,947 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:14,947 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:14,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:14,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:14,956 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:14,956 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:14,961 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,992 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:14,992 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:14,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:14,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:14,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:15,001 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,001 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,015 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,019 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:15,019 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:15,020 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:15,023 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:15,027 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,028 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,046 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:15,046 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:15,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:15,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:15,054 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,054 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,077 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,081 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:15,081 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:15,083 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:15,086 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:15,090 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,090 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,095 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,099 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,103 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,107 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:15,108 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:15,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:15,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:15,113 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,113 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,118 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,126 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:31:15,131 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:31:15,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:15,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:15,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:15,133 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,133 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:15,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,135 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,137 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:15,137 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:15,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:15,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:15,138 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,138 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:15,147 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:15,154 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:15,162 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:15,168 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:15,169 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:15,169 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:15,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:15,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:15,175 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:15,175 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:15,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,178 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,178 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:15,178 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:15,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:15,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:15,183 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 26])", "<class 'int'>: 25")
2023-10-11 15:31:15,183 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:15,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,186 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:15,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:15,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:15,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:15,196 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,196 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,201 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,215 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,215 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:15,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:15,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:15,223 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,223 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,273 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,273 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,273 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:15,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:15,279 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:15,282 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,282 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,288 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,297 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,302 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:15,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:15,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:15,310 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,311 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,320 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,335 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:15,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:15,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:15,344 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,344 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,358 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,362 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,362 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:15,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:15,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:15,370 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,371 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,380 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,388 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,389 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,389 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:15,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:15,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:15,397 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,397 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,402 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,411 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,415 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,415 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:15,417 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:15,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:15,424 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,424 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,434 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,439 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,443 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,443 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:15,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:15,448 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:15,452 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,452 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,464 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,486 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,490 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,491 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,491 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:15,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:15,496 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:15,499 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,499 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,517 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:15,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:15,522 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:15,525 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,525 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,535 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,543 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,543 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:15,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:15,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:15,548 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,548 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,553 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,566 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:31:15,566 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:31:15,566 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:15,567 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:15,568 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:15,568 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,568 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:15,569 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,570 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,570 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,571 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,571 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:15,571 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:15,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:15,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:15,573 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,573 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:15,582 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:15,589 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:15,595 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:15,602 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:15,603 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:15,603 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:15,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:15,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:15,609 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:15,609 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:15,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,611 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,611 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,612 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:15,612 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:15,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:15,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:15,616 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 27])", "<class 'int'>: 26")
2023-10-11 15:31:15,617 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:15,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:15,619 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:15,619 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:15,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:15,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:15,629 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,629 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,634 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,646 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,646 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,646 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:15,647 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:15,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:15,654 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,654 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,670 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,678 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,679 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:15,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:15,683 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:15,686 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,686 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,730 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,730 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:15,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:15,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:15,738 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,738 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,761 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,775 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,775 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:15,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:15,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:15,783 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,783 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,788 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,797 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,801 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,801 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:15,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:15,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:15,809 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,809 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,814 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,819 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,827 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,827 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:15,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:15,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:15,834 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,834 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,852 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,852 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:15,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:15,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:15,860 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,860 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,877 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,878 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:15,879 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:15,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:15,885 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,885 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,890 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,895 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,899 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,903 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,903 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,903 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:15,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:15,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:15,911 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,911 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,921 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,925 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,929 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,929 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,929 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:15,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:15,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:15,937 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,937 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,955 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:15,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:15,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:15,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:15,960 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:15,960 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:15,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:15,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:16,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:31:16,206 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:31:16,206 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:16,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:16,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:16,208 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,208 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:16,209 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,211 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,212 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:16,212 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:16,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:16,213 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:16,213 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,213 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:16,222 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:16,229 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:16,237 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:16,248 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:16,249 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:16,249 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:16,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:16,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:16,259 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:16,259 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:16,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,261 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,261 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,262 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:16,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:16,262 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:16,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:16,267 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 28])", "<class 'int'>: 27")
2023-10-11 15:31:16,267 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:16,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,270 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:16,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:16,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:16,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:16,285 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,285 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,310 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,316 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,316 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:16,317 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:16,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:16,324 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,325 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,347 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,352 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,357 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,357 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,357 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:16,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:16,364 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:16,370 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,370 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,389 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,393 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,393 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,394 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:16,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:16,398 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:16,401 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,401 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,411 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,416 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,419 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,420 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,420 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:16,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:16,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:16,428 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,428 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,433 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,442 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,446 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,446 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:16,448 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:16,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:16,454 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,455 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,469 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,473 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,474 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,474 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:16,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:16,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:16,482 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,482 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,491 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,495 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,499 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,500 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:16,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:16,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:16,507 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,507 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,525 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,525 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,525 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:16,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:16,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:16,533 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,533 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,538 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,552 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,552 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:16,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:16,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:16,559 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,559 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,569 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,577 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,577 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,577 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:16,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:16,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:16,585 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,585 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,633 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,637 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,637 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,638 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:16,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:16,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:16,643 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,643 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,658 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,662 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:31:16,662 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:31:16,662 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:16,664 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:16,664 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:16,664 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,665 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:16,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,668 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:16,668 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:16,668 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:16,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:16,669 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,669 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:16,680 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:16,688 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:16,696 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:16,703 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:16,704 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:16,704 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:16,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:16,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:16,711 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:16,711 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:16,712 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,712 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,713 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,714 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:16,714 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:16,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:16,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:16,718 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 29])", "<class 'int'>: 28")
2023-10-11 15:31:16,718 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:16,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,720 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:16,721 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:16,721 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:16,724 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:16,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:16,731 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,731 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,737 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,747 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,753 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:16,753 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:16,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:16,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:16,761 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,762 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,780 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:16,780 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:16,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:16,785 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:16,788 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,788 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,806 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:16,806 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:16,808 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:16,811 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:16,815 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,815 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,824 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,926 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,926 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:16,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:16,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:16,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:16,934 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,935 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,967 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,976 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:16,976 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:16,978 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:16,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:16,984 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:16,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:16,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:16,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,004 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:17,004 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:17,005 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:17,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:17,012 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,012 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,021 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,026 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,030 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:17,030 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:17,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:17,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:17,039 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,040 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,064 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:17,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:17,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:17,068 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:17,071 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,072 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,076 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,085 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,103 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,103 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:17,103 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:17,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:17,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:17,112 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,112 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,130 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:17,130 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:17,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:17,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:17,138 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,138 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,156 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:17,156 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:17,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:17,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:17,162 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,162 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,167 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,171 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:31:17,180 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:31:17,180 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:17,181 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:17,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:17,182 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,182 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:17,183 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,186 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:17,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:17,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:17,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:17,187 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,187 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:17,198 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:17,209 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:17,218 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:17,226 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:17,226 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:17,226 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:17,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:17,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:17,233 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:17,233 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:17,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,235 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:17,235 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:17,236 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:17,236 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:17,240 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 30])", "<class 'int'>: 29")
2023-10-11 15:31:17,240 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:17,241 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,241 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,243 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:17,243 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:17,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:17,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:17,252 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,253 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,264 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,273 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,273 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:17,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:17,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:17,281 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,282 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,300 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,300 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,300 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:17,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:17,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:17,308 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,309 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,314 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,318 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,322 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,327 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,327 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:17,329 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:17,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:17,336 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,336 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,355 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,355 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:17,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:17,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:17,364 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,364 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,374 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,379 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,383 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:17,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:17,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:17,392 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,392 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,400 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,418 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,423 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,424 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,424 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:17,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:17,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:17,435 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,435 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,442 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,447 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,458 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,458 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,458 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:17,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:17,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:17,469 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,470 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,476 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,487 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,492 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,492 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:17,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:17,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:17,504 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,504 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,528 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,528 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,528 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:17,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:17,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:17,538 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,538 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,543 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,555 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,564 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,565 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,565 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:17,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:17,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:17,574 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,574 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,580 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,585 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,596 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,596 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:17,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:17,601 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:17,602 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,602 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,608 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,613 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:31:17,624 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:31:17,624 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:17,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:17,626 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:17,626 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,626 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:17,627 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,628 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,630 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,630 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:17,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:17,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:17,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:17,632 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,632 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:17,644 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:17,655 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:17,665 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:17,674 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:17,675 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:17,675 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:17,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:17,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:17,682 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:17,682 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:17,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,683 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,683 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,684 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:17,684 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:17,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:17,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:17,688 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 31])", "<class 'int'>: 30")
2023-10-11 15:31:17,688 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:17,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,690 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:17,692 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:17,692 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:17,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:17,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:17,702 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,702 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,728 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,738 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,738 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,738 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:17,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:17,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:17,746 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,746 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,751 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,756 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,761 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,766 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,766 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:17,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:17,771 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:17,774 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,774 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,781 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,790 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,794 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,794 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,794 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:17,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:17,798 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:17,801 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,802 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,807 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,812 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,817 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,821 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,821 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:17,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:17,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:17,829 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,829 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,835 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,848 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,848 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:17,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:17,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:17,856 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,856 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,875 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,875 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,875 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:17,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:17,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:17,883 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,883 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,893 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,897 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,901 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,901 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:17,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:17,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:17,909 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,909 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,914 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,927 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,928 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:17,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:17,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:17,935 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,935 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,949 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,953 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,953 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,953 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:17,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:17,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:17,961 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:17,961 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:17,967 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,986 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:17,997 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:17,997 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:17,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:18,001 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:18,005 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,005 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:18,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:18,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:18,023 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:18,024 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:18,024 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:18,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:18,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:18,028 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,029 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,034 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:18,038 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:18,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:18,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:31:18,047 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:31:18,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:18,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:18,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:18,049 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,049 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,051 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,052 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,052 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:18,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:18,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:18,053 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,053 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,063 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,071 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,078 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,085 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,085 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:18,085 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:18,092 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:18,092 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:18,092 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:18,093 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,094 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,094 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,095 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,095 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,095 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:18,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:18,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:18,100 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 32])", "<class 'int'>: 31")
2023-10-11 15:31:18,100 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,102 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,102 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:18,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:18,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:18,111 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,112 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,130 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,130 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:18,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:18,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:18,138 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,138 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,159 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,159 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:18,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:18,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:18,167 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,167 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,172 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,185 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,185 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:18,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:18,190 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:18,193 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,193 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,222 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,230 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,231 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,231 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:18,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:18,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:18,239 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,239 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,249 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,253 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,257 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,257 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,258 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:18,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:18,262 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:18,265 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,265 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,271 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,280 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,284 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,284 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:18,285 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:18,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:18,291 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,291 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,297 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,301 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,309 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:18,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:18,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:18,318 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,318 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,323 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,327 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,333 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,337 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,337 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,337 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:18,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:18,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:18,345 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,345 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,355 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,358 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,363 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,363 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:18,364 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:18,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:18,371 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,371 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,385 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,389 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,389 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,390 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:18,391 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:18,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:18,397 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,397 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,416 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,416 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,417 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:18,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:18,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:18,421 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,422 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,432 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,437 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,441 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:31:18,441 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:31:18,441 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:18,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:18,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:18,443 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,443 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,446 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,446 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:18,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:18,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:18,448 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,448 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,460 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,471 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,482 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,491 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,491 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:18,491 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:18,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:18,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:18,498 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:18,498 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,500 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,500 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:18,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:18,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:18,505 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 33])", "<class 'int'>: 32")
2023-10-11 15:31:18,505 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,506 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,507 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,508 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:18,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:18,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:18,517 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,517 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,531 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,535 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,535 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,535 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:18,536 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:18,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:18,543 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,543 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,559 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,567 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,568 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,568 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:18,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:18,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:18,576 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,576 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,581 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,586 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,590 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,594 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,594 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,595 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:18,596 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:18,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:18,602 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,602 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,608 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,620 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,620 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,620 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:18,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:18,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:18,628 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,629 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,634 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,647 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,647 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,648 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:18,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:18,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:18,656 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,656 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,666 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,670 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,675 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,675 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:18,676 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:18,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:18,684 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,685 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,716 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,721 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,725 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,725 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,725 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:18,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:18,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:18,733 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,734 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,743 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,753 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,753 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:18,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:18,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:18,761 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,761 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,772 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,782 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,783 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:18,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:18,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:18,790 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,791 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,796 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,803 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,807 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,812 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,812 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,812 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:18,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:18,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:18,820 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,820 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,835 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,839 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,839 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:18,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:18,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:18,845 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,845 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,855 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,860 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,864 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:31:18,864 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:31:18,865 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:18,866 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:18,866 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:18,867 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,867 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,870 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,870 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:18,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:18,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:18,871 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,871 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,882 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,892 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,902 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,914 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:18,915 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:18,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:18,925 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:18,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:18,926 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:18,926 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,928 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,928 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,929 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,929 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,929 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:18,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:18,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:18,934 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 34])", "<class 'int'>: 33")
2023-10-11 15:31:18,934 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:18,934 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,935 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:18,937 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:18,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:18,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:18,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:18,946 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,946 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,952 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:18,956 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:18,961 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:18,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:18,965 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:18,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:18,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:18,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:18,973 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:18,974 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:18,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:18,984 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:18,989 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:18,993 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:18,994 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:18,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:18,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:18,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:19,001 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,002 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,009 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,025 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,025 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:19,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:19,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:19,033 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,034 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,044 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,054 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,054 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:19,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:19,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:19,062 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,062 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,068 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,073 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,077 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,082 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,082 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:19,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:19,087 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:19,090 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,090 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,096 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,112 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,112 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:19,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:19,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:19,120 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,120 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,139 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,139 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,139 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:19,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:19,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:19,147 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,147 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,153 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,166 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,166 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,166 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:19,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:19,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:19,174 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,174 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,196 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,200 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,205 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,205 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:19,206 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:19,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:19,213 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,213 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,232 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,246 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,246 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,246 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:19,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:19,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:19,255 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,255 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,273 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,273 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,273 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:19,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:19,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:19,278 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,278 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,298 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:31:19,299 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:31:19,299 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:19,300 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:19,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:19,301 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,301 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:19,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,304 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,304 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:19,304 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:19,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:19,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:19,306 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,306 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:19,316 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:19,323 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:19,330 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:19,337 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:19,338 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:19,338 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:19,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:19,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:19,353 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:19,353 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:19,354 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,355 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,356 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:19,356 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:19,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:19,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:19,361 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 35])", "<class 'int'>: 34")
2023-10-11 15:31:19,361 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:19,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,362 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,364 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,364 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:19,364 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:19,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:19,370 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:19,373 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,373 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,379 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,388 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,392 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,393 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,393 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:19,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:19,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:19,401 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,401 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,406 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,419 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,420 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,420 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:19,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:19,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:19,428 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,428 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,434 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,439 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,449 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,449 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,449 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:19,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:19,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:19,459 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,459 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,470 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,490 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,490 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,491 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:19,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:19,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:19,499 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,499 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,504 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,514 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,519 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,519 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:19,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:19,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:19,528 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,528 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,534 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,549 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:19,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:19,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:19,557 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,557 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,566 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,571 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,575 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,575 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:19,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:19,580 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:19,583 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,583 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,598 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,602 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,602 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,602 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:19,604 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:19,607 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:19,610 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,610 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,620 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,629 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,629 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:19,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:19,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:19,637 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,638 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,652 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,657 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,657 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:19,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:19,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:19,665 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,681 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,685 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:19,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:19,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:19,690 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,691 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:31:19,724 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:31:19,724 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:19,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:19,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:19,726 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,726 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:19,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,728 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,729 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:19,729 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:19,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:19,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:19,730 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,730 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:19,740 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:19,747 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:19,754 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:19,761 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:19,762 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:19,762 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:19,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:19,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:19,769 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:19,769 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:19,769 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,770 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,772 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:19,772 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:19,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:19,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:19,776 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 36])", "<class 'int'>: 35")
2023-10-11 15:31:19,776 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:19,777 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,778 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:19,779 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:19,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:19,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:19,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:19,789 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,789 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,804 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,808 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:19,808 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:19,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:19,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:19,817 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,817 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,831 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,836 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:19,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:19,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:19,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:19,845 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,845 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,850 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,860 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,864 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,865 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:19,865 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:19,866 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:19,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:19,873 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,873 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,895 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:19,895 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:19,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:19,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:19,903 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,903 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,909 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,913 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,922 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:19,923 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:19,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:19,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:19,930 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,931 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,937 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,949 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,950 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:19,950 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:19,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:19,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:19,958 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:19,958 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:19,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,968 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,993 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:19,993 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:19,993 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:19,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:19,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:20,002 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,002 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,008 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,016 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,021 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,027 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:20,027 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:20,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:20,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:20,035 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,036 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,041 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,054 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:20,055 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:20,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:20,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:20,062 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,063 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,068 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,087 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,093 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:20,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:20,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:20,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:20,102 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,102 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,108 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,112 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,116 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,121 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:20,121 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:20,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:20,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:20,127 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,128 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:31:20,156 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:31:20,156 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:20,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:20,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:20,159 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,159 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:20,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,163 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:20,163 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:20,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:20,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:20,165 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,165 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:20,173 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:20,181 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:20,188 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:20,195 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:20,196 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:20,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:20,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:20,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:20,203 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:20,203 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:20,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,205 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,206 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:20,206 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:20,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:20,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:20,211 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 37])", "<class 'int'>: 36")
2023-10-11 15:31:20,211 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:20,211 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,213 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,213 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,214 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:20,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:20,217 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:20,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:20,224 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,224 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,248 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,254 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,255 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,255 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:20,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:20,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:20,263 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,263 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,287 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,287 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,287 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:20,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:20,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:20,296 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,296 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,308 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,314 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,319 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:20,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:20,324 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:20,328 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,328 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,345 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,351 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,351 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:20,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:20,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:20,359 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,360 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,377 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,383 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:20,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:20,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:20,391 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,391 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,398 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,422 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,428 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,428 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:20,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:20,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:20,437 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,438 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,450 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,456 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,461 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:20,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:20,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:20,470 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,470 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,476 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,483 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,492 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,492 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:20,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:20,496 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:20,500 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,500 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,513 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,518 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,522 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,523 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:20,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:20,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:20,530 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,531 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,536 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,549 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,549 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:20,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:20,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:20,557 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,557 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,576 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:20,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:20,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:20,582 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,582 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:31:20,599 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:31:20,599 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:20,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:20,601 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:20,601 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,601 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:20,602 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,603 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,605 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:20,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:20,605 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:20,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:20,606 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,606 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:20,615 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:20,624 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:20,633 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:20,640 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:20,640 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:20,640 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:20,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:20,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:20,647 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:31:20,647 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:20,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,648 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,649 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:20,650 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:31:20,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:31:20,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:20,654 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 38])", "<class 'int'>: 37")
2023-10-11 15:31:20,654 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:20,655 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,657 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:20,657 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:20,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:31:20,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:31:20,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:20,666 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,667 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,686 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,695 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,701 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,701 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:31:20,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:31:20,706 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:20,709 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,709 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,733 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,748 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,748 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:31:20,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:31:20,753 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:20,756 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,757 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,762 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,772 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,777 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,777 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,777 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:31:20,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:31:20,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:20,785 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,785 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,790 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,795 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,804 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,804 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,804 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:31:20,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:31:20,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:20,812 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,812 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,831 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,831 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:31:20,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:31:20,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:20,839 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,840 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,849 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,857 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,858 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,858 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:31:20,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:31:20,863 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:20,866 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,867 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,872 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,876 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,881 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,885 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,885 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,886 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:31:20,887 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:31:20,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:20,894 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,894 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,909 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,924 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,931 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,931 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:31:20,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:31:20,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:20,940 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,940 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,946 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,959 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,960 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:20,960 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:31:20,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:31:20,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:20,968 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:20,968 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:20,974 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:20,978 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,002 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,007 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:21,007 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:31:21,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:31:21,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:21,015 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:21,015 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:21,020 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,029 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,034 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,034 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:21,034 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:31:21,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:31:21,038 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:21,039 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:21,039 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:31:21,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:31:21,058 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:31:21,058 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:31:21,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:31:21,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:21,060 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:21,060 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:21,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:21,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:21,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:21,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:31:21,066 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:31:21,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:31:21,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:31:21,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:31:21,067 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:31:21,068 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:31:21,076 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:21,085 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:21,092 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:21,100 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:31:21,100 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:31:21,100 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:31:21,107 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-11 15:31:21,107 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:21,107 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-11 15:31:21,107 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:21,107 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-11 15:31:21,108 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:21,108 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-11 15:31:21,108 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:21,108 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-11 15:31:21,108 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:21,108 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-11 15:31:21,108 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:21,109 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-11 15:31:21,109 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:21,109 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-11 15:31:21,109 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:31:21,117 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-11 15:31:21,117 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-11 15:31:21,118 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-11 15:31:21,118 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-11 15:31:21,118 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-11 15:31:21,118 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-11 15:31:21,118 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-11 15:31:21,118 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-11 15:31:21,118 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-11 15:31:21,118 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-11 15:31:21,119 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-11 15:31:21,119 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-11 15:31:21,119 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-11 15:31:21,119 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-11 15:31:21,119 [forward.py:23 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-11 15:31:21,119 [forward.py:23 in reset_forward] DEBUG - lm_head from flexgen to old.
