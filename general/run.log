2023-10-30 14:11:39,628 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpdx8slwcc
2023-10-30 14:11:39,629 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpdx8slwcc/_remote_module_non_scriptable.py
2023-10-30 14:11:39,742 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-30 14:11:39,808 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 14:11:41,293 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-30 14:11:41,556 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 14:11:41,556 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 14:11:41,556 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 14:11:41,556 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 14:11:42,518 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 14:11:42,604 [model.py:111 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-30 14:11:42,605 [model.py:60 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-30 14:11:42,649 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 14:11:42,737 [model.py:68 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-30 14:11:42,740 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-30 14:11:42,741 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-30 14:11:42,741 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-30 14:11:42,742 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-30 14:11:42,743 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-30 14:11:42,744 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-30 14:11:42,745 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-30 14:11:42,746 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-30 14:11:42,747 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-30 14:11:42,748 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-30 14:11:42,749 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-30 14:11:42,750 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-30 14:11:42,751 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-30 14:11:42,751 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-30 14:11:42,752 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-30 14:11:42,753 [model.py:211 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-30 14:11:42,753 [model.py:215 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-30 14:11:42,755 [model.py:221 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-30 14:11:42,795 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 14:11:42,966 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-30 14:11:42,966 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-30 14:11:42,966 [model.py:298 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-30 14:11:42,966 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-30 14:11:42,966 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-30 14:11:42,967 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-30 14:11:42,967 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-30 14:11:42,967 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-30 14:11:42,967 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-30 14:11:42,967 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-30 14:11:42,967 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-30 14:11:42,967 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-30 14:11:42,967 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-30 14:11:42,968 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-30 14:11:42,968 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-30 14:11:42,968 [model.py:298 in to_test_forward] DEBUG - lm_head to test forward
2023-10-30 14:11:42,971 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:42,973 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-30 14:11:42,974 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:42,975 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-30 14:11:42,975 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:42,988 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-30 14:11:42,991 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:42,997 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-30 14:11:42,999 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:43,006 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-30 14:11:43,008 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:43,015 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-30 14:11:43,017 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:43,024 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-30 14:11:43,026 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:43,032 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-30 14:11:43,035 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:43,040 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-30 14:11:43,042 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:43,048 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-30 14:11:43,050 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:43,057 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-30 14:11:43,059 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:43,065 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-30 14:11:43,067 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:43,073 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-30 14:11:43,075 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:43,082 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-30 14:11:43,084 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:43,085 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-30 14:11:43,085 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:43,093 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-30 14:11:43,098 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-30 14:11:43,099 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-30 14:11:43,099 [model.py:306 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-30 14:11:43,099 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-30 14:11:43,099 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-30 14:11:43,099 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-30 14:11:43,099 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-30 14:11:43,099 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-30 14:11:43,099 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-30 14:11:43,100 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-30 14:11:43,100 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-30 14:11:43,100 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-30 14:11:43,100 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-30 14:11:43,100 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-30 14:11:43,100 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-30 14:11:43,100 [model.py:306 in reset_forward] DEBUG - lm_head from test to old.
2023-10-30 14:11:43,111 [model.py:410 in init_all_weights] DEBUG - init all weights...
2023-10-30 14:11:43,137 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-30 14:11:43,138 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-30 14:11:43,138 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-30 14:11:43,138 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-30 14:11:43,138 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-30 14:11:43,138 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-30 14:11:43,138 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-30 14:11:43,139 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-30 14:11:43,139 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-30 14:11:43,139 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-30 14:11:43,139 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-30 14:11:43,139 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-30 14:11:43,139 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-30 14:11:43,139 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-30 14:11:43,140 [flexgen.py:184 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-30 14:11:43,140 [flexgen.py:184 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-30 14:11:43,200 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 14:11:43,341 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:43,341 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:43,341 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:43,342 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:43,342 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64',)
2023-10-30 14:11:43,342 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:43,342 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64',)
2023-10-30 14:11:43,342 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:43,343 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64',), {})
2023-10-30 14:11:43,344 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,344 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,345 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,345 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-30 14:11:43,346 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:43,346 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:43,349 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:43,350 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:43,350 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64', '0')
2023-10-30 14:11:43,350 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:43,350 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64', '0')
2023-10-30 14:11:43,350 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:43,351 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64', '0'), {})
2023-10-30 14:11:43,352 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,352 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,353 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,353 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-30 14:11:43,355 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:43,356 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:43,359 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:43,362 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:43,362 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,363 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,363 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,363 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,364 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,369 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,372 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,376 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,382 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,383 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:43,384 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:43,388 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:43,391 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:43,391 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,391 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,392 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,392 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,393 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,397 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,401 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,404 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,407 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,407 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:43,409 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:43,412 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:43,416 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:43,416 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,416 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,416 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,417 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,417 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,422 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,425 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,428 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,430 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,431 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:43,432 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:43,435 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:43,438 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:43,438 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,439 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,439 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,439 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,440 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,444 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,447 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,451 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,453 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,453 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:43,455 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:43,458 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:43,461 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:43,461 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,462 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,462 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,462 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,463 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,467 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,470 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,473 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,476 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,477 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:43,478 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:43,481 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:43,484 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:43,484 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,484 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,485 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,485 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,485 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,490 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,493 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,496 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,499 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,499 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:43,500 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:43,504 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:43,507 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:43,508 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,508 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,508 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,508 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,509 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,518 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,521 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,524 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,527 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,527 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:43,529 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:43,532 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:43,535 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:43,535 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,536 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,536 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,536 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,537 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,542 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,545 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,548 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,551 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,551 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:43,552 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:43,556 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:43,559 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:43,559 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,559 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,559 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,559 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,560 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,565 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,568 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,571 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,574 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,574 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:43,575 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:43,578 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:43,581 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:43,581 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,582 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,582 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,582 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,583 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,587 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,590 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,593 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,597 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,597 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:43,598 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:43,602 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:43,605 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:43,605 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,605 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,606 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,606 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,607 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,611 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,614 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,617 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,620 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,620 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:43,622 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:43,622 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:43,626 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:43,626 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,626 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,626 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,626 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,627 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,631 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,636 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,640 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-30 14:11:43,642 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-30 14:11:43,643 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:43,644 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:43,644 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:43,645 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:43,645 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,645 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:43,645 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,645 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:43,646 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-30 14:11:43,648 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,651 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,655 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-30 14:11:43,658 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-30 14:11:43,658 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:43,658 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:43,659 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:43,659 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:43,659 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-30 14:11:43,659 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:43,660 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-30 14:11:43,660 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:43,660 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-30 14:11:43,671 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-30 14:11:43,680 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-30 14:11:43,688 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-30 14:11:43,697 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 9, 50272), torch.float32


2023-10-30 14:11:43,698 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:43,702 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:43,702 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:43,703 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:43,703 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:43,703 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:43,704 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:43,704 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:43,704 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:43,705 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:43,706 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:43,706 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:43,707 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:43,707 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:43,707 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:43,711 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:43,711 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:43,712 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 10), torch.int64', '9')
2023-10-30 14:11:43,712 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:43,712 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 10), torch.int64', '9')
2023-10-30 14:11:43,712 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:43,712 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 10), torch.int64', '9'), {})
2023-10-30 14:11:43,713 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:43,714 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:43,714 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:43,715 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:43,717 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:43,717 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:43,721 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:43,724 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:43,724 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,724 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,724 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,724 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,726 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,730 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,734 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,737 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,742 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,742 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:43,744 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:43,747 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:43,750 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:43,750 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,750 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,751 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,751 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,752 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,757 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,761 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,764 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,767 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,767 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:43,768 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:43,772 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:43,775 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:43,775 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,775 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,775 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,776 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,777 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,781 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,784 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,788 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,790 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,790 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:43,791 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:43,795 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:43,798 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:43,798 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,798 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,798 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,798 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,800 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,804 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,808 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,811 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,813 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,813 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:43,814 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:43,818 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:43,821 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:43,821 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,821 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,821 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,822 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,823 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,828 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,836 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,840 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,841 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,842 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:43,843 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:43,847 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:43,850 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:43,850 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,850 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,850 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,850 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,852 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,856 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,859 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,862 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,865 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,865 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:43,867 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:43,871 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:43,874 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:43,874 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,874 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,875 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,875 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,876 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,880 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,883 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,887 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,889 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,889 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:43,890 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:43,894 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:43,897 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:43,898 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,898 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,898 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,898 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,900 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,904 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,907 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,910 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,912 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,912 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:43,913 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:43,917 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:43,921 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:43,921 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,921 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,921 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,921 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,923 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,928 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,931 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,934 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,936 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,937 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:43,938 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:43,941 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:43,945 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:43,945 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,945 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,945 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,946 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,947 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,951 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,955 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,958 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,960 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,960 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:43,961 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:43,965 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:43,969 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:43,969 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,969 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,969 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,969 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,971 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,975 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,978 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,981 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,983 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:43,983 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:43,985 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:43,985 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:43,989 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:43,989 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:43,989 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,989 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:43,989 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:43,991 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:43,996 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:43,999 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:44,003 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-30 14:11:44,005 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-30 14:11:44,006 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:44,007 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:44,008 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:44,008 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:44,008 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,008 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,009 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,009 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,009 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:44,010 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,011 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,011 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,012 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,012 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:44,012 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:44,013 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:44,013 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:44,013 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,013 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,013 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,013 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,014 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:44,021 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,029 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,035 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,057 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:44,061 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:44,064 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:44,065 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:44,065 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:44,065 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:44,065 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,066 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:44,066 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,066 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:44,067 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,068 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,068 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,069 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,069 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:44,069 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:44,073 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:44,073 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:44,073 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 11), torch.int64', '10')
2023-10-30 14:11:44,073 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,074 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 11), torch.int64', '10')
2023-10-30 14:11:44,074 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,074 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 11), torch.int64', '10'), {})
2023-10-30 14:11:44,075 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,076 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,077 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,077 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,079 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:44,079 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:44,082 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:44,085 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:44,086 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,086 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,086 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,086 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,088 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,118 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,131 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,135 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,143 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,143 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:44,145 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:44,149 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:44,152 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:44,152 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,152 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,152 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,152 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,154 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,159 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,162 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,165 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,168 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,168 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:44,169 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:44,173 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:44,176 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:44,177 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,177 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,177 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,177 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,179 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,183 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,186 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,190 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,192 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,192 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:44,194 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:44,198 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:44,201 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:44,201 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,201 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,202 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,202 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,204 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,208 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,212 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,216 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,218 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,218 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:44,220 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:44,227 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:44,232 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:44,232 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,232 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,233 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,233 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,235 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,250 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,255 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,260 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,262 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,263 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:44,264 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:44,268 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:44,271 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:44,271 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,271 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,272 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,272 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,274 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,278 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,282 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,285 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,287 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,287 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:44,289 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:44,292 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:44,296 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:44,296 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,296 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,297 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,297 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,299 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,303 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,307 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,310 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,313 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,313 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:44,315 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:44,320 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:44,324 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:44,324 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,324 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,324 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,325 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,326 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,335 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,339 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,343 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,345 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,346 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:44,347 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:44,351 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:44,354 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:44,355 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,355 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,355 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,355 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,357 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,361 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,365 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,369 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,371 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,371 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:44,373 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:44,376 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:44,380 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:44,380 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,380 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,380 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,381 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,382 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,387 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,391 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,394 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,396 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,397 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:44,398 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:44,402 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:44,406 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:44,406 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,406 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,406 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,406 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,408 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,412 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,416 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,419 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,421 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,421 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:44,423 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:44,424 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:44,427 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:44,427 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,427 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,428 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,428 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,429 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,434 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,437 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,441 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-30 14:11:44,443 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-30 14:11:44,443 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:44,444 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:44,445 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:44,445 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:44,445 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,445 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,446 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,446 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,446 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:44,447 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,448 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,449 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,449 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,449 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:44,449 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:44,450 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:44,450 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:44,450 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,451 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,451 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,451 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,451 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:44,460 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,467 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,474 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,481 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:44,482 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:44,486 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:44,486 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:44,487 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:44,487 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:44,487 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,487 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:44,487 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,488 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:44,489 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,489 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,490 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,490 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,491 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:44,491 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:44,495 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:44,495 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:44,496 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 12), torch.int64', '11')
2023-10-30 14:11:44,496 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,496 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 12), torch.int64', '11')
2023-10-30 14:11:44,496 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,497 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 12), torch.int64', '11'), {})
2023-10-30 14:11:44,497 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,498 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,499 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,499 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,501 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:44,501 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:44,505 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:44,508 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:44,509 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,509 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,509 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,509 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,511 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,515 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,522 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,525 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,528 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,528 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:44,529 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:44,533 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:44,536 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:44,536 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,536 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,537 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,537 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,539 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,543 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,546 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,550 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,552 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,552 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:44,554 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:44,557 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:44,560 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:44,560 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,561 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,561 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,561 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,563 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,567 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,570 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,573 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,575 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,576 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:44,577 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:44,580 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:44,584 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:44,584 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,584 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,584 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,584 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,586 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,612 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,622 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,626 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,628 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,629 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:44,630 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:44,634 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:44,637 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:44,638 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,638 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,638 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,638 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,640 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,644 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,648 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,651 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,653 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,653 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:44,655 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:44,658 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:44,661 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:44,661 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,662 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,662 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,662 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,664 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,668 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,671 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,675 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,677 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,677 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:44,678 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:44,682 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:44,686 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:44,686 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,686 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,686 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,687 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,688 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,693 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,696 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,700 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,702 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,702 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:44,704 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:44,708 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:44,711 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:44,711 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,711 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,712 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,712 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,713 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,717 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,721 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,724 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,726 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,726 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:44,728 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:44,732 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:44,735 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:44,735 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,736 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,736 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,736 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,738 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,742 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,745 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,749 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,751 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,751 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:44,752 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:44,756 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:44,760 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:44,760 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,760 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,760 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,760 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,762 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,766 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,770 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,773 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,775 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,775 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:44,776 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:44,780 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:44,784 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:44,784 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,784 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,784 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,784 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,786 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,790 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,793 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,797 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,799 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,799 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:44,800 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:44,801 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:44,804 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:44,804 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,805 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,805 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,805 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,807 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,811 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,814 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,817 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-30 14:11:44,819 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-30 14:11:44,819 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:44,821 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:44,821 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:44,822 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:44,822 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,822 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,822 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,822 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,823 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:44,823 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,824 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,825 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,825 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,825 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:44,826 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:44,826 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:44,827 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:44,827 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,827 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,827 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,827 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,828 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:44,838 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,846 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,856 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:44,879 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:44,881 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:44,884 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:44,885 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:44,885 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:44,885 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:44,885 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,886 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:44,886 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,886 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:44,887 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,888 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,888 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,889 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,889 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:44,889 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:44,893 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:44,894 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:44,894 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 13), torch.int64', '12')
2023-10-30 14:11:44,894 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:44,894 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 13), torch.int64', '12')
2023-10-30 14:11:44,894 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:44,895 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 13), torch.int64', '12'), {})
2023-10-30 14:11:44,895 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,896 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,897 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:44,897 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:44,899 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:44,899 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:44,903 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:44,907 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:44,907 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,907 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,908 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,908 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,909 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,914 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,917 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,921 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,923 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:44,923 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:44,925 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:44,928 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:44,931 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:44,931 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,932 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,932 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,932 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,934 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,957 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,966 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,970 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,972 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:44,972 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:44,974 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:44,977 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:44,980 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:44,981 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:44,981 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,981 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:44,981 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:44,983 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:44,987 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,990 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,994 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:44,996 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:44,996 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:44,998 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:45,001 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:45,005 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:45,005 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,005 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,006 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,006 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,007 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,012 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,015 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,019 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,022 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,022 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:45,023 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:45,027 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:45,030 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:45,030 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,031 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,031 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,031 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,033 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,037 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,040 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,043 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,045 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,045 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:45,047 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:45,050 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:45,053 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:45,054 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,054 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,054 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,054 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,056 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,060 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,063 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,066 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,068 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,068 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:45,069 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:45,073 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:45,076 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:45,076 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,076 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,077 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,077 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,078 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,083 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,086 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,089 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,091 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,092 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:45,093 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:45,097 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:45,101 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:45,101 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,101 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,101 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,101 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,103 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,108 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,111 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,114 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,117 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,117 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:45,118 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:45,123 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:45,126 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:45,126 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,126 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,126 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,127 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,128 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,132 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,135 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,139 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,140 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,141 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:45,142 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:45,145 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:45,148 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:45,148 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,148 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,149 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,149 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,150 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,155 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,158 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,161 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,163 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,163 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:45,164 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:45,168 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:45,171 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:45,171 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,171 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,172 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,172 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,173 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,177 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,181 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,184 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,186 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,186 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:45,188 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:45,188 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:45,192 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:45,192 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,192 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,192 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,192 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,194 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,198 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,201 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,205 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-30 14:11:45,208 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-30 14:11:45,208 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:45,210 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:45,211 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:45,212 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:45,212 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,212 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:45,213 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,213 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:45,213 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:45,214 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,215 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,217 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,217 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:45,218 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:45,218 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:45,219 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:45,219 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:45,219 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,220 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:45,220 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,220 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:45,221 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:45,228 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:45,235 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:45,241 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:45,248 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:45,249 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:45,252 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:45,253 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:45,253 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:45,253 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:45,253 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:45,253 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:45,254 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:45,254 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:45,255 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,255 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,256 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,256 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:45,257 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:45,257 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:45,261 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:45,261 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:45,261 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 14), torch.int64', '13')
2023-10-30 14:11:45,261 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:45,261 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 14), torch.int64', '13')
2023-10-30 14:11:45,262 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:45,262 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 14), torch.int64', '13'), {})
2023-10-30 14:11:45,263 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,263 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,264 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:45,265 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:45,266 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:45,267 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:45,270 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:45,273 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:45,273 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,273 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,273 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,274 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,275 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,279 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,283 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,286 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,288 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:45,288 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:45,289 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:45,293 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:45,296 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:45,296 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,296 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,297 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,297 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,299 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,303 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,306 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,309 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,311 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:45,311 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:45,313 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:45,316 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:45,319 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:45,319 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,319 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,320 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,320 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,321 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,325 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,329 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,332 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,340 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:45,340 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:45,342 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:45,345 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:45,348 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:45,348 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,348 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,349 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,349 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,351 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,355 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,358 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,361 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,363 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:45,363 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:45,364 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:45,368 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:45,371 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:45,371 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,371 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,372 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,372 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,373 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:45,377 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,380 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,384 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:45,386 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:45,386 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:45,387 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:45,391 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:45,394 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:45,394 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:45,394 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,394 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:45,394 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:45,396 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,418 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,422 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,426 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,428 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:47,428 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:47,430 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:47,433 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:47,436 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:47,436 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,437 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,437 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,437 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,439 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,443 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,446 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,450 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,452 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:47,452 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:47,453 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:47,457 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:47,460 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:47,460 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,461 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,461 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,461 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,463 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,484 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,487 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,491 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,493 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:47,493 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:47,494 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:47,498 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:47,501 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:47,501 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,501 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,501 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,502 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,504 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,508 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,511 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,514 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,516 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:47,517 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:47,518 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:47,521 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:47,524 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:47,525 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,525 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,525 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,525 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,528 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,532 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,536 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,540 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,543 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:47,543 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:47,544 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:47,549 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:47,552 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:47,553 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,553 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,553 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,553 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,555 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,560 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,564 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,568 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,570 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:47,571 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:47,572 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:47,573 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:47,577 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:47,577 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,578 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,578 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,578 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,580 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,585 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,588 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,592 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-30 14:11:47,594 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-30 14:11:47,594 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:47,595 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:47,596 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:47,596 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:47,596 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,597 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:47,597 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,597 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:47,597 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:47,598 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,599 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,600 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,600 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:47,601 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:47,601 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:47,601 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:47,602 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:47,602 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,602 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:47,602 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,602 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:47,603 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:47,613 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:47,620 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:47,627 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:47,634 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:47,635 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:47,638 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:47,639 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:47,639 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:47,639 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:47,639 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:47,639 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:47,640 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:47,640 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:47,641 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,641 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,642 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,642 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:47,643 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:47,643 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:47,647 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:47,647 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:47,648 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 15), torch.int64', '14')
2023-10-30 14:11:47,648 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:47,648 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 15), torch.int64', '14')
2023-10-30 14:11:47,648 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:47,649 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 15), torch.int64', '14'), {})
2023-10-30 14:11:47,650 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,651 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,652 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:47,653 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:47,654 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:47,655 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:47,658 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:47,661 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:47,661 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,662 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,662 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,662 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,664 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,668 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,672 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,675 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,677 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,677 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:47,679 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:47,682 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:47,686 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:47,686 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,686 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,687 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,687 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,688 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,693 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,696 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,699 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,701 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,702 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:47,703 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:47,706 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:47,710 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:47,710 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,710 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,710 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,710 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,712 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,717 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,747 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,775 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,795 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,796 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:47,797 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:47,801 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:47,804 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:47,804 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,805 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,805 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,805 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,807 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,812 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,816 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,820 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,822 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,822 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:47,824 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:47,828 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:47,832 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:47,832 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,832 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,832 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,833 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,834 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,839 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,843 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,847 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,850 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,850 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:47,852 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:47,856 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:47,859 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:47,859 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,860 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,860 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,860 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,862 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,866 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,870 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,874 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,876 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,877 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:47,878 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:47,882 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:47,885 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:47,885 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,885 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,886 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,886 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,888 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,898 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,902 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,905 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,908 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,908 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:47,909 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:47,913 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:47,917 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:47,917 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,917 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,917 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,917 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,919 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,924 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,928 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,932 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,934 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,934 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:47,936 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:47,939 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:47,943 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:47,943 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,943 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,943 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,944 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,946 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,950 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,954 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,958 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,960 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,960 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:47,962 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:47,965 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:47,969 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:47,969 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,969 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,969 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,969 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,971 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:47,978 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,981 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,985 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:47,987 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:47,988 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:47,989 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:47,993 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:47,996 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:47,997 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:47,997 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,997 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:47,997 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:47,999 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,003 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:48,007 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:48,011 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:48,013 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:48,014 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:48,015 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:48,016 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:48,021 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:48,021 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,021 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,021 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,022 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,023 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,028 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:48,032 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:48,037 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-30 14:11:48,040 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-30 14:11:48,040 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:48,041 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:48,041 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:48,042 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:48,042 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,043 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,043 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,043 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,044 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:48,044 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,045 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,046 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,047 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,047 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:48,047 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:48,048 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:48,048 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:48,048 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,048 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,049 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,049 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,049 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:48,056 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,063 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,070 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,077 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:48,078 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:48,082 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:48,082 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:48,082 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:48,083 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:48,083 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,083 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:48,083 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,084 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:48,085 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,085 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,086 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,087 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,087 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:48,087 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:48,091 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:48,092 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:48,092 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 16), torch.int64', '15')
2023-10-30 14:11:48,092 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,092 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 16), torch.int64', '15')
2023-10-30 14:11:48,092 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,093 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 16), torch.int64', '15'), {})
2023-10-30 14:11:48,094 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,094 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,095 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,096 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,098 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:48,098 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:48,102 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:48,106 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:48,106 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,106 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,107 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,107 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,109 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,113 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,117 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,121 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,123 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,124 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:48,125 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:48,129 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:48,133 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:48,134 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,134 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,134 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,135 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,138 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,143 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,152 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,156 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,158 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,158 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:48,160 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:48,164 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:48,167 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:48,167 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,167 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,168 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,168 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,170 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,174 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,178 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,181 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,183 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,184 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:48,185 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:48,189 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:48,192 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:48,192 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,193 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,193 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,193 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,195 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,201 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,207 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,212 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,215 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,216 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:48,218 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:48,224 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:48,229 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:48,229 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,229 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,229 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,229 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,231 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,235 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,239 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,242 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,244 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,245 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:48,246 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:48,249 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:48,252 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:48,253 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,253 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,253 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,253 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,255 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,259 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,262 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,266 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,268 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,268 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:48,269 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:48,273 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:48,276 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:48,276 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,276 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,277 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,277 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,279 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,283 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,286 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,289 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,292 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,292 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:48,293 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:48,299 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:48,305 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:48,305 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,306 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,306 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,306 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,309 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,315 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,320 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,325 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,328 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,328 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:48,329 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:48,333 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:48,336 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:48,336 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,337 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,337 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,337 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,339 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,343 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,346 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,350 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,352 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,352 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:48,354 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:48,357 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:48,361 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:48,361 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,361 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,361 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,361 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,363 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,367 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,388 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,391 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,393 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,393 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:48,395 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:48,399 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:48,402 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:48,402 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,402 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,403 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,403 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,404 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,409 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,412 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,416 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,418 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,418 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:48,419 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:48,420 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:48,423 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:48,424 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,424 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,424 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,424 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,426 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,430 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,433 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,437 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-30 14:11:48,439 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-30 14:11:48,439 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:48,440 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:48,441 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:48,441 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:48,441 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,441 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,442 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,442 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,442 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:48,443 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,444 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,444 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,445 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,445 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:48,445 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:48,446 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:48,446 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:48,446 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,446 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,446 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,446 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,447 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:48,454 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,461 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,467 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,475 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:48,476 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:48,479 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:48,480 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:48,480 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:48,480 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:48,480 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,481 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:48,481 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,481 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:48,482 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,482 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,483 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,484 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,484 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:48,484 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:48,488 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:48,488 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:48,489 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 17), torch.int64', '16')
2023-10-30 14:11:48,489 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,489 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 17), torch.int64', '16')
2023-10-30 14:11:48,489 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,490 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 17), torch.int64', '16'), {})
2023-10-30 14:11:48,490 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,491 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,492 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,492 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,493 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:48,494 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:48,497 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:48,501 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:48,501 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,501 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,501 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,501 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,503 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,507 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,511 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,514 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,517 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,517 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:48,518 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:48,522 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:48,525 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:48,526 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,526 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,526 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,526 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,528 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,532 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,536 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,539 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,541 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,542 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:48,543 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:48,547 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:48,550 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:48,550 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,551 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,551 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,551 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,553 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,557 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,560 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,563 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,565 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,566 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:48,567 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:48,571 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:48,574 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:48,574 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,574 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,574 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,575 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,576 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,580 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,584 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,587 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,589 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,589 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:48,591 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:48,595 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:48,599 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:48,600 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,600 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,600 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,600 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,602 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,606 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,610 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,614 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,616 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,616 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:48,617 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:48,621 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:48,625 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:48,625 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,625 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,625 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,625 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,627 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,631 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,635 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,660 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,662 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,662 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:48,663 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:48,668 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:48,671 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:48,671 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,671 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,671 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,671 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,673 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,678 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,681 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,685 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,687 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,687 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:48,689 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:48,692 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:48,696 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:48,696 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,696 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,696 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,696 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,698 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,702 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,705 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,709 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,711 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,711 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:48,712 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:48,717 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:48,721 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:48,721 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,721 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,722 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,722 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,723 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,727 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,731 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,734 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,736 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,737 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:48,738 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:48,742 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:48,745 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:48,745 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,745 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,746 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,746 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,748 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,752 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,755 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,759 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,761 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,761 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:48,762 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:48,766 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:48,769 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:48,769 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,769 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,770 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,770 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,771 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,783 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,786 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,790 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,792 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,792 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:48,794 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:48,794 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:48,798 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:48,798 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,798 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,798 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,798 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,800 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,804 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,808 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,811 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-30 14:11:48,813 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-30 14:11:48,813 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:48,814 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:48,815 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:48,816 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:48,816 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,816 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,816 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,816 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,817 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:48,817 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,818 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,819 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,819 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,819 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:48,820 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:48,820 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:48,820 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:48,820 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,821 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,821 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,821 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,821 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:48,829 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,836 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,843 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:48,850 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:48,851 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:48,854 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:48,854 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:48,855 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:48,855 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:48,855 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,855 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:48,855 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,856 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:48,856 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,857 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,858 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,858 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,858 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:48,859 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:48,862 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:48,863 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:48,863 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 18), torch.int64', '17')
2023-10-30 14:11:48,863 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:48,864 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 18), torch.int64', '17')
2023-10-30 14:11:48,864 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:48,866 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 18), torch.int64', '17'), {})
2023-10-30 14:11:48,867 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,868 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,870 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:48,870 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:48,872 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:48,873 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:48,879 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:48,893 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:48,893 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,893 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,893 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,893 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,895 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,913 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,917 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,922 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,927 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:48,927 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:48,929 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:48,933 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:48,937 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:48,937 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,937 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,938 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,938 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,940 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,945 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,949 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,954 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,956 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:48,957 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:48,959 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:48,962 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:48,965 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:48,965 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,965 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,966 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,966 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,968 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:48,973 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,978 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,982 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:48,984 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:48,984 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:48,986 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:48,989 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:48,993 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:48,993 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:48,994 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,994 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:48,994 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:48,996 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,008 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,012 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,017 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,019 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,019 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:49,021 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:49,025 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:49,028 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:49,028 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,029 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,029 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,029 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,031 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,036 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,040 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,049 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,052 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,052 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:49,054 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:49,058 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:49,061 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:49,061 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,061 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,061 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,061 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,063 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,068 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,072 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,076 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,078 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,079 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:49,080 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:49,084 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:49,087 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:49,087 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,088 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,088 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,088 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,090 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,095 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,099 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,103 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,105 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,106 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:49,108 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:49,112 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:49,115 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:49,115 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,116 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,116 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,116 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,118 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,124 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,129 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,135 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,137 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,137 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:49,139 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:49,142 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:49,145 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:49,146 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,146 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,146 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,146 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,148 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,153 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,157 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,161 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,180 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,180 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:49,182 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:49,185 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:49,188 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:49,188 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,188 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,189 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,189 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,190 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,195 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,199 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,202 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,204 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,205 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:49,206 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:49,210 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:49,213 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:49,213 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,213 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,214 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,214 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,216 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,220 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,223 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,227 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,228 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,229 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:49,230 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:49,231 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:49,234 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:49,234 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,234 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,235 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,235 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,236 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,242 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,245 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,248 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-30 14:11:49,250 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-30 14:11:49,250 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:49,251 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:49,252 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:49,252 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:49,252 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,252 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:49,253 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,253 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:49,253 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:49,254 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,255 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,255 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,256 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:49,256 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:49,256 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:49,257 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:49,257 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:49,257 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,257 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:49,257 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,258 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:49,258 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:49,268 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:49,278 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:49,286 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:49,295 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:49,296 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:49,299 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:49,300 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:49,300 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:49,300 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:49,301 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:49,301 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:49,301 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:49,301 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:49,302 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,303 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,303 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,304 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:49,304 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:49,304 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:49,308 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:49,308 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:49,309 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 19), torch.int64', '18')
2023-10-30 14:11:49,309 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:49,309 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 19), torch.int64', '18')
2023-10-30 14:11:49,309 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:49,309 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 19), torch.int64', '18'), {})
2023-10-30 14:11:49,310 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,311 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,311 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,312 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:49,313 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:49,314 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:49,317 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:49,320 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:49,321 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,321 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,321 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,321 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,323 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,327 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,331 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,339 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,342 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,342 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:49,345 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:49,351 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:49,357 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:49,357 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,358 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,358 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,358 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,361 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,369 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,374 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,387 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,399 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,400 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:49,401 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:49,405 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:49,409 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:49,409 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,409 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,409 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,409 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,411 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,416 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,420 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,424 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,426 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,426 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:49,428 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:49,431 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:49,434 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:49,435 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,435 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,435 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,435 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,437 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,442 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,446 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,450 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,452 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,452 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:49,454 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:49,457 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:49,461 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:49,461 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,461 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,461 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,461 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,463 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,468 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,472 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,476 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,478 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,478 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:49,479 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:49,483 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:49,487 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:49,487 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,488 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,488 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,488 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,490 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,495 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,498 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,505 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,507 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,507 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:49,509 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:49,513 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:49,516 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:49,516 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,516 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,517 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,517 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,518 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,523 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,526 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,529 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,532 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,533 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:49,535 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:49,539 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:49,542 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:49,542 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,542 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,542 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,543 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,544 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,548 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,552 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,556 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,558 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,558 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:49,560 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:49,564 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:49,567 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:49,567 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,568 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,568 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,568 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,570 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,574 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,577 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,581 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,583 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,583 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:49,584 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:49,589 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:49,593 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:49,593 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,593 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,593 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,594 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,595 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,600 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,603 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,606 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,608 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,608 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:49,609 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:49,615 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:49,619 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:49,619 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,619 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,620 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,620 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,621 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,626 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,630 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,633 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,635 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,635 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:49,637 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:49,638 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:49,642 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:49,642 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,643 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,643 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,643 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,645 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,666 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,670 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,674 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-30 14:11:49,687 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-30 14:11:49,687 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:49,688 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:49,689 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:49,689 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:49,690 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,690 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:49,690 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,690 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:49,691 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:49,691 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,692 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,693 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,693 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:49,693 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:49,694 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:49,694 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:49,694 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:49,695 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,695 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:49,695 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,695 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:49,695 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:49,703 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:49,710 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:49,716 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:49,723 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:49,724 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:49,728 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:49,728 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:49,729 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:49,729 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:49,729 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:49,729 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:49,729 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:49,730 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:49,730 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,731 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,732 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,732 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:49,732 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:49,733 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:49,737 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:49,737 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:49,737 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 20), torch.int64', '19')
2023-10-30 14:11:49,737 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:49,737 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 20), torch.int64', '19')
2023-10-30 14:11:49,738 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:49,738 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 20), torch.int64', '19'), {})
2023-10-30 14:11:49,739 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,739 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,740 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:49,740 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:49,742 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:49,742 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:49,746 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:49,749 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:49,749 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,750 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,750 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,750 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,752 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,756 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,760 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,764 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,766 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,767 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:49,768 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:49,772 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:49,775 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:49,775 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,775 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,775 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,776 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,777 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,782 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,785 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,788 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,791 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,791 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:49,792 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:49,796 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:49,799 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:49,799 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,800 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,800 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,800 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,802 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,806 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,809 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,813 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,815 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,815 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:49,816 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:49,820 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:49,823 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:49,824 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,824 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,824 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,824 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,826 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,830 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,834 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,838 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,840 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,840 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:49,841 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:49,845 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:49,848 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:49,848 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,849 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,849 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,849 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,851 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,855 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,858 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,879 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,881 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,881 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:49,883 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:49,886 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:49,889 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:49,890 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,890 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,890 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,890 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,892 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,896 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,900 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,903 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,905 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,905 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:49,907 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:49,910 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:49,914 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:49,914 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,914 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,914 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,915 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,916 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,920 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,924 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,928 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,930 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,931 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:49,932 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:49,936 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:49,939 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:49,939 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,939 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,940 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,940 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,941 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,946 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,950 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,953 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,955 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,955 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:49,956 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:49,960 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:49,963 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:49,963 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,963 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,963 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,964 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,965 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,973 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,977 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,981 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:49,983 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:49,983 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:49,984 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:49,988 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:49,991 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:49,991 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:49,991 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,992 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:49,992 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:49,993 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:49,998 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,009 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,013 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,015 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:50,015 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:50,017 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:50,020 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:50,024 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:50,024 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,024 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,025 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,025 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,027 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,031 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,035 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,038 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,040 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:50,040 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:50,042 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:50,043 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:50,046 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:50,046 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,047 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,047 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,047 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,049 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,053 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,057 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,060 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-30 14:11:50,062 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-30 14:11:50,063 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:50,064 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:50,064 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:50,065 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:50,065 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,065 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,065 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,065 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,066 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:50,066 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,067 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,068 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,068 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,069 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:50,069 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:50,069 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:50,070 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:50,070 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,070 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,070 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,070 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,071 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:50,077 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,084 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,091 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,099 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:50,100 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:50,103 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:50,104 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:50,104 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:50,104 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:50,104 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,105 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:50,105 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,105 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:50,106 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,107 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,107 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,108 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,108 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:50,108 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:50,112 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:50,112 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:50,113 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 21), torch.int64', '20')
2023-10-30 14:11:50,113 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,113 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 21), torch.int64', '20')
2023-10-30 14:11:50,113 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,114 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 21), torch.int64', '20'), {})
2023-10-30 14:11:50,114 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,115 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,116 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,116 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,118 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:50,118 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:50,121 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:50,125 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:50,125 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,125 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,125 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,126 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,127 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,150 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,153 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,157 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,159 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,159 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:50,160 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:50,164 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:50,167 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:50,167 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,167 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,168 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,168 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,169 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,177 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,180 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,183 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,185 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,186 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:50,187 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:50,191 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:50,194 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:50,194 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,194 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,194 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,194 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,196 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,200 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,204 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,207 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,209 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,209 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:50,210 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:50,214 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:50,217 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:50,217 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,217 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,218 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,218 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,219 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,224 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,227 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,230 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,232 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,233 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:50,234 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:50,238 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:50,241 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:50,242 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,242 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,242 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,242 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,244 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,248 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,251 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,255 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,256 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,257 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:50,258 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:50,262 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:50,265 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:50,265 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,265 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,265 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,265 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,267 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,279 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,283 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,286 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,288 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,289 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:50,290 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:50,294 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:50,297 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:50,297 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,298 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,298 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,298 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,300 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,304 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,308 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,311 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,313 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,314 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:50,315 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:50,319 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:50,322 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:50,322 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,322 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,323 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,323 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,324 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,329 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,333 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,336 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,338 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,338 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:50,340 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:50,344 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:50,347 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:50,347 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,347 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,347 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,348 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,349 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,354 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,357 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,360 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,362 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,362 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:50,363 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:50,367 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:50,370 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:50,371 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,371 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,371 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,371 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,373 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,377 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,380 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,384 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,386 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,386 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:50,387 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:50,391 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:50,394 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:50,395 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,395 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,395 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,395 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,397 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,401 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,404 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,408 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,410 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,410 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:50,411 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:50,412 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:50,415 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:50,415 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,415 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,416 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,416 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,417 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,422 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,425 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,429 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-30 14:11:50,431 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-30 14:11:50,431 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:50,432 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:50,432 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:50,433 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:50,433 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,433 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,433 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,434 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,434 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:50,435 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,435 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,436 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,437 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,437 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:50,437 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:50,437 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:50,438 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:50,438 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,438 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,438 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,438 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,439 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:50,446 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,453 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,459 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,466 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:50,467 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:50,471 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:50,472 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:50,472 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:50,472 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:50,472 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,472 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:50,472 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,473 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:50,474 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,474 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,475 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,475 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,476 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:50,476 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:50,480 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:50,480 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:50,480 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 22), torch.int64', '21')
2023-10-30 14:11:50,480 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,480 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 22), torch.int64', '21')
2023-10-30 14:11:50,480 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,481 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 22), torch.int64', '21'), {})
2023-10-30 14:11:50,482 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,482 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,483 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,484 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,485 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:50,485 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:50,489 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:50,492 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:50,492 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,492 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,493 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,493 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,494 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,499 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,509 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,513 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,515 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,515 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:50,517 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:50,521 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:50,524 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:50,524 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,525 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,525 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,525 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,527 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,531 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,534 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,537 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,540 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,540 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:50,541 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:50,545 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:50,548 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:50,548 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,548 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,549 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,549 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,551 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,555 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,559 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,562 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,564 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,565 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:50,566 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:50,570 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:50,573 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:50,573 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,573 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,573 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,573 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,575 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,580 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,583 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,586 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,588 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,589 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:50,590 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:50,594 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:50,597 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:50,598 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,598 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,598 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,598 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,600 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,604 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,608 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,612 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,616 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,616 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:50,618 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:50,621 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:50,625 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:50,625 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,625 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,625 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,625 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,627 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,631 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,635 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,639 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,641 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,641 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:50,642 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:50,646 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:50,649 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:50,649 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,649 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,650 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,650 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,652 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,656 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,673 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,680 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,682 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,683 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:50,684 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:50,688 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:50,691 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:50,691 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,691 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,692 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,692 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,694 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,698 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,701 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,705 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,707 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,707 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:50,708 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:50,712 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:50,715 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:50,715 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,715 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,716 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,716 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,718 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,725 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,728 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,732 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,734 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,734 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:50,735 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:50,739 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:50,742 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:50,742 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,742 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,743 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,743 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,745 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,749 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,769 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,788 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,791 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,791 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:50,792 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:50,796 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:50,799 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:50,799 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,799 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,800 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,800 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,802 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,806 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,809 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,813 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,815 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,815 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:50,816 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:50,817 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:50,820 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:50,821 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,821 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,821 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,821 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,823 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,827 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,831 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,834 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-30 14:11:50,836 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-30 14:11:50,836 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:50,838 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:50,838 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:50,839 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:50,839 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,839 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,839 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,839 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,840 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:50,840 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,841 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,842 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,842 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,842 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:50,843 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:50,843 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:50,843 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:50,844 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,844 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,844 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,844 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,845 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:50,851 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,858 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,865 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:50,872 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:50,873 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:50,876 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:50,877 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:50,877 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:50,877 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:50,877 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,877 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:50,878 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,878 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:50,879 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,879 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,880 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,880 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,881 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:50,881 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:50,885 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:50,885 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:50,885 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 23), torch.int64', '22')
2023-10-30 14:11:50,885 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:50,886 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 23), torch.int64', '22')
2023-10-30 14:11:50,886 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:50,886 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 23), torch.int64', '22'), {})
2023-10-30 14:11:50,887 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,887 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,888 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:50,889 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:50,890 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:50,891 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:50,894 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:50,897 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:50,898 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,898 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,898 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,898 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,900 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,904 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,908 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,911 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,913 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:50,913 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:50,915 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:50,918 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:50,921 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:50,922 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,922 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,922 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,922 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,924 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,947 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,950 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,954 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,956 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:50,956 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:50,958 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:50,962 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:50,965 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:50,965 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,965 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,966 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,966 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,967 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,972 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,975 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,979 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,981 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:50,981 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:50,982 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:50,986 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:50,989 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:50,989 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:50,989 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,990 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:50,990 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:50,992 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:50,996 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:50,999 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,017 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,020 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,020 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:51,022 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:51,026 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:51,030 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:51,030 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,030 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,031 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,031 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,033 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,044 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,048 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,051 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,053 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,054 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:51,055 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:51,059 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:51,062 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:51,062 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,062 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,062 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,063 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,064 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,089 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,115 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,118 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,121 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,121 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:51,122 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:51,126 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:51,130 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:51,130 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,130 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,130 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,130 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,132 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,136 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,140 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,144 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,147 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,147 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:51,149 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:51,152 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:51,156 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:51,156 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,156 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,156 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,156 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,158 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,163 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,168 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,171 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,174 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,174 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:51,176 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:51,180 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:51,183 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:51,183 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,183 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,183 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,184 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,185 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,190 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,194 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,198 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,200 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,200 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:51,202 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:51,205 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:51,208 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:51,209 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,209 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,209 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,209 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,211 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,215 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,218 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,222 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,224 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,224 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:51,225 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:51,229 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:51,232 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:51,232 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,232 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,232 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,232 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,234 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,238 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,241 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,245 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,247 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,247 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:51,248 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:51,249 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:51,252 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:51,252 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,252 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,252 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,253 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,254 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,258 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,262 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,265 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-30 14:11:51,287 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-30 14:11:51,287 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:51,288 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:51,288 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:51,289 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:51,289 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,289 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:51,289 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,289 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:51,290 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:51,291 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,291 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,292 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,292 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:51,293 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:51,293 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:51,293 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:51,294 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:51,294 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,294 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:51,294 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,294 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:51,295 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:51,302 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:51,309 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:51,315 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:51,322 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:51,323 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:51,327 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:51,327 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:51,327 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:51,328 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:51,328 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:51,328 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:51,328 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:51,328 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:51,329 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,330 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,330 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,331 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:51,331 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:51,331 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:51,335 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:51,335 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:51,335 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 24), torch.int64', '23')
2023-10-30 14:11:51,335 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:51,336 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 24), torch.int64', '23')
2023-10-30 14:11:51,336 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:51,336 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 24), torch.int64', '23'), {})
2023-10-30 14:11:51,337 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,337 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,338 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,339 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:51,340 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:51,340 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:51,344 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:51,347 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:51,347 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,347 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,348 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,348 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,349 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,354 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,357 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,361 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,363 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,363 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:51,364 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:51,368 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:51,371 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:51,371 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,371 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,372 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,372 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,373 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,378 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,382 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,385 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,387 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,387 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:51,389 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:51,392 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:51,396 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:51,396 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,396 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,396 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,396 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,398 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,402 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,406 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,409 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,411 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,411 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:51,413 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:51,416 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:51,420 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:51,420 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,420 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,420 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,420 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,422 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,426 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,429 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,433 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,435 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,435 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:51,436 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:51,440 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:51,443 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:51,443 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,443 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,444 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,444 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,446 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,450 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,454 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,457 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,459 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,459 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:51,460 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:51,464 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:51,467 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:51,467 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,467 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,468 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,468 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,469 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,474 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,477 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,480 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,482 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,482 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:51,483 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:51,487 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:51,490 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:51,490 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,490 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,490 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,491 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,492 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,496 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,500 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,503 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,505 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,505 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:51,507 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:51,510 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:51,513 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:51,513 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,513 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,513 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,514 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,515 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,528 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,545 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,575 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,602 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,603 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:51,604 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:51,608 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:51,612 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:51,612 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,612 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,613 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,613 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,615 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,650 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,681 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,707 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,748 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,748 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:51,750 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:51,753 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:51,757 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:51,758 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,758 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,758 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,759 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,761 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,773 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,778 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,782 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,784 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,784 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:51,786 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:51,789 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:51,795 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:51,795 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,795 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,796 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,796 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,798 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,814 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,818 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,822 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,824 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,824 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:51,825 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:51,826 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:51,829 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:51,829 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,829 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,830 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,830 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,831 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,835 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,839 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,842 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-30 14:11:51,844 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-30 14:11:51,844 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:51,846 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:51,846 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:51,847 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:51,847 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,847 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:51,847 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,847 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:51,848 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:51,848 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,849 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,850 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,850 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:51,850 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:51,851 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:51,851 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:51,851 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:51,851 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,852 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:51,852 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,852 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:51,852 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:51,860 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:51,866 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:51,873 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:51,879 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:51,881 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:51,884 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:51,884 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:51,885 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:51,885 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:51,885 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:51,885 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:51,885 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:51,886 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:51,886 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,887 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,888 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,888 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:51,888 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:51,889 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:51,892 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:51,893 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:51,893 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 25), torch.int64', '24')
2023-10-30 14:11:51,893 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:51,893 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 25), torch.int64', '24')
2023-10-30 14:11:51,893 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:51,894 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 25), torch.int64', '24'), {})
2023-10-30 14:11:51,894 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,895 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,896 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:51,896 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:51,897 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:51,898 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:51,901 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:51,904 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:51,904 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,905 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,905 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,905 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,907 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,913 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,916 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,919 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,921 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:51,922 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:51,923 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:51,926 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:51,929 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:51,929 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,929 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,930 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,930 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,931 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,936 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,939 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,942 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,944 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:51,944 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:51,946 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:51,949 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:51,952 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:51,952 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,952 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,952 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,952 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,954 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,958 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,962 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,965 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,967 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:51,967 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:51,968 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:51,971 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:51,974 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:51,974 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,975 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,975 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,975 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,977 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:51,981 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,984 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,988 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:51,989 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:51,990 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:51,991 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:51,995 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:51,998 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:51,998 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:51,999 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:51,999 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:51,999 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,001 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,005 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,028 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,034 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,036 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:52,036 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:52,038 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:52,042 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:52,045 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:52,045 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,045 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,045 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,045 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,047 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,052 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,055 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,059 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,061 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:52,061 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:52,062 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:52,066 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:52,069 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:52,069 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,069 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,069 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,070 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,071 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,076 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,079 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,083 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,085 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:52,085 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:52,087 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:52,090 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:52,093 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:52,093 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,094 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,094 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,094 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,096 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,100 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,104 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,107 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,109 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:52,110 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:52,111 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:52,115 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:52,118 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:52,118 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,119 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,119 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,119 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,121 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,125 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,129 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,132 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,134 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:52,134 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:52,136 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:52,139 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:52,143 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:52,143 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,143 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,143 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,144 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,145 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,150 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,154 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,158 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,160 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:52,161 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:52,162 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:52,165 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:52,169 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:52,169 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,169 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,169 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,169 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,171 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,176 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,179 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,183 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,185 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:52,185 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:52,187 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:52,188 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:52,191 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:52,191 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,191 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,192 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,192 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,194 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,199 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,203 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,207 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-30 14:11:52,210 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-30 14:11:52,210 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:52,211 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:52,212 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:52,212 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:52,213 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,213 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:52,213 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,213 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:52,213 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:52,214 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,215 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,216 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,216 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:52,216 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:52,217 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:52,217 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:52,217 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:52,217 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,218 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:52,218 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,218 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:52,218 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:52,227 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:52,235 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:52,243 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:52,255 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:52,257 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:52,260 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:52,260 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:52,261 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:52,261 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:52,261 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:52,261 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:52,261 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:52,262 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:52,262 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,263 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,264 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,264 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:52,264 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:52,265 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:52,268 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:52,269 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:52,269 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 26), torch.int64', '25')
2023-10-30 14:11:52,269 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:52,269 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 26), torch.int64', '25')
2023-10-30 14:11:52,269 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:52,270 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 26), torch.int64', '25'), {})
2023-10-30 14:11:52,270 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,271 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,272 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,272 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:52,274 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:52,274 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:52,277 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:52,281 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:52,281 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,281 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,281 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,281 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,283 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,295 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,299 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,303 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,305 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,306 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:52,307 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:52,311 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:52,314 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:52,314 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,314 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,314 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,314 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,316 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,321 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,324 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,328 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,330 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,330 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:52,332 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:52,335 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:52,338 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:52,338 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,338 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,339 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,339 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,341 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,345 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,349 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,352 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,354 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,355 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:52,356 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:52,359 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:52,363 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:52,363 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,363 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,363 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,363 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,365 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,369 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,373 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,377 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,379 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,379 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:52,381 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:52,384 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:52,387 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:52,388 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,388 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,388 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,388 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,390 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,395 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,398 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,402 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,404 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,404 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:52,406 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:52,409 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:52,412 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:52,412 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,413 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,413 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,413 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,415 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,419 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,423 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,426 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,428 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,428 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:52,430 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:52,433 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:52,436 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:52,436 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,436 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,437 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,437 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,439 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,443 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,447 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,451 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,453 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,453 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:52,455 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:52,459 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:52,462 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:52,462 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,462 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,462 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,463 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,464 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,469 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,473 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,476 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,479 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,479 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:52,480 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:52,484 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:52,487 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:52,487 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,487 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,488 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,488 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,489 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,494 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,497 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,501 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,503 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,503 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:52,504 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:52,508 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:52,511 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:52,512 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,512 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,512 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,512 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,514 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,518 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,522 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,526 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,535 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,535 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:52,536 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:52,540 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:52,543 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:52,543 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,544 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,544 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,544 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,546 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,550 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,554 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,557 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,559 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,559 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:52,561 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:52,561 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:52,565 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:52,565 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,565 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,565 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,565 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,567 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,585 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,589 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,593 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-30 14:11:52,595 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-30 14:11:52,595 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:52,596 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:52,597 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:52,597 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:52,597 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,597 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:52,598 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,598 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:52,598 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:52,599 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,600 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,600 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,601 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:52,601 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:52,601 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:52,602 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:52,602 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:52,602 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,603 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:52,603 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,603 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:52,603 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:52,611 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:52,617 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:52,624 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:52,630 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:52,632 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:52,635 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:52,635 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:52,636 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:52,636 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:52,636 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:52,637 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:52,637 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:52,637 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:52,638 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,638 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,639 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,640 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:52,640 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:52,640 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:52,644 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:52,644 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:52,644 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 27), torch.int64', '26')
2023-10-30 14:11:52,645 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:52,645 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 27), torch.int64', '26')
2023-10-30 14:11:52,645 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:52,645 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 27), torch.int64', '26'), {})
2023-10-30 14:11:52,646 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,647 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,648 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:52,648 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:52,650 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:52,650 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:52,653 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:52,657 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:52,657 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,657 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,657 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,657 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,659 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,663 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,667 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,671 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,673 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:52,673 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:52,674 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:52,678 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:52,681 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:52,681 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,681 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,682 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,682 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,684 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,688 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,692 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,695 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,697 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:52,698 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:52,699 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:52,703 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:52,706 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:52,706 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,706 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,706 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,707 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,708 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,713 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,716 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,720 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,722 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:52,722 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:52,723 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:52,727 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:52,730 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:52,731 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,731 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,731 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,731 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,733 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,737 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,741 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,744 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,746 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:52,747 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:52,748 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:52,752 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:52,755 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:52,755 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,755 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,756 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,756 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,757 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,762 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,765 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,795 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,820 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:52,820 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:52,822 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:52,825 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:52,829 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:52,829 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:52,830 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,830 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:52,830 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:52,832 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:52,890 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,934 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:52,979 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,004 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:53,004 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:53,005 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:53,010 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:53,014 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:53,014 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,015 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,015 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,016 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,019 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,052 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,056 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,060 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,061 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:53,062 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:53,063 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:53,066 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:53,069 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:53,070 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,070 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,070 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,070 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,072 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,076 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,080 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,083 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,085 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:53,085 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:53,087 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:53,090 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:53,093 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:53,093 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,093 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,094 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,094 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,096 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,100 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,104 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,107 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,109 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:53,110 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:53,111 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:53,114 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:53,118 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:53,118 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,118 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,118 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,119 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,120 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,125 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,128 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,132 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,133 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:53,134 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:53,135 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:53,139 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:53,142 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:53,142 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,142 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,142 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,143 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,144 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,149 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,152 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,156 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,157 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:53,158 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:53,159 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:53,160 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:53,163 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:53,163 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,163 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,163 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,163 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,165 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,169 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,172 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,176 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-30 14:11:53,178 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-30 14:11:53,178 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:53,180 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:53,180 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:53,180 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:53,181 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,181 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:53,181 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,181 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:53,181 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:53,182 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,183 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,183 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,184 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:53,184 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:53,184 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:53,185 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:53,185 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:53,185 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,185 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:53,185 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,186 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:53,186 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:53,196 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:53,203 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:53,212 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:53,220 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:53,221 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:53,224 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:53,224 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:53,225 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:53,225 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:53,225 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:53,225 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:53,225 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:53,226 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:53,227 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,227 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,228 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,229 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:53,229 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:53,229 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:53,233 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:53,233 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:53,233 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 28), torch.int64', '27')
2023-10-30 14:11:53,233 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:53,233 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 28), torch.int64', '27')
2023-10-30 14:11:53,234 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:53,234 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 28), torch.int64', '27'), {})
2023-10-30 14:11:53,235 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,236 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,236 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,237 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:53,239 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:53,239 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:53,242 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:53,245 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:53,246 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,246 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,246 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,246 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,248 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,253 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,256 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,260 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,262 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,262 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:53,263 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:53,267 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:53,270 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:53,270 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,270 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,270 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,270 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,273 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,277 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,281 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,284 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,286 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,286 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:53,288 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:53,291 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:53,294 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:53,294 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,295 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,295 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,295 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,297 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,301 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,305 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,309 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,311 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,311 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:53,312 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:53,315 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:53,319 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:53,319 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,319 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,319 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,319 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,321 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,326 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,329 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,333 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,335 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,335 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:53,337 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:53,340 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:53,343 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:53,343 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,343 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,344 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,344 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,346 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,350 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,354 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,358 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,360 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,361 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:53,362 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:53,365 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:53,368 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:53,369 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,369 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,369 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,369 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,371 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,376 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,379 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,383 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,385 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,385 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:53,386 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:53,390 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:53,393 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:53,393 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,393 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,393 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,393 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,395 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,400 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,403 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,407 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,409 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,409 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:53,410 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:53,414 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:53,417 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:53,417 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,417 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,417 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,417 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,419 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,424 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,428 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,431 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,433 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,433 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:53,434 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:53,438 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:53,441 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:53,441 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,441 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,442 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,442 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,444 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,448 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,452 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,455 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,457 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,458 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:53,459 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:53,463 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:53,466 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:53,466 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,466 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,466 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,466 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,468 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,473 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,477 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,481 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,483 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,483 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:53,484 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:53,488 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:53,491 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:53,491 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,491 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,491 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,492 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,493 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,498 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,501 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,516 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,518 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,519 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:53,520 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:53,521 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:53,524 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:53,524 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,524 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,524 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,524 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,526 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,531 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,534 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,538 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-30 14:11:53,540 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-30 14:11:53,540 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:53,541 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:53,542 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:53,542 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:53,543 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,543 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:53,543 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,543 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:53,544 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:53,544 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,545 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,546 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,546 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:53,546 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:53,547 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:53,547 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:53,547 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:53,548 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,548 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:53,548 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,548 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:53,548 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:53,555 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:53,562 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:53,569 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:53,577 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:53,578 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:53,581 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:53,582 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:53,582 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:53,582 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:53,582 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:53,583 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:53,583 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:53,583 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:53,584 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,584 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,585 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,586 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:53,586 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:53,586 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:53,590 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:53,590 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:53,590 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 29), torch.int64', '28')
2023-10-30 14:11:53,590 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:53,590 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 29), torch.int64', '28')
2023-10-30 14:11:53,591 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:53,591 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 29), torch.int64', '28'), {})
2023-10-30 14:11:53,592 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,592 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,593 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:53,594 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:53,596 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:53,596 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:53,599 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:53,603 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:53,603 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,603 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,603 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,603 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,605 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,671 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,727 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,758 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,806 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:53,808 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:53,809 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:53,813 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:53,816 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:53,817 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,817 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,817 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,817 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,820 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,842 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,846 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,850 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,852 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:53,852 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:53,853 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:53,857 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:53,860 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:53,860 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,860 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,860 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,860 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,862 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,867 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,871 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,875 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,878 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:53,878 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:53,880 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:53,883 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:53,886 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:53,886 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,887 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,887 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,887 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,889 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,894 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,897 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,901 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,903 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:53,903 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:53,905 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:53,908 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:53,911 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:53,911 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,912 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,912 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,912 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,914 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,918 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,922 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,926 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,928 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:53,928 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:53,929 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:53,933 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:53,936 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:53,936 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,936 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,937 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,937 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,939 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,943 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,947 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,950 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,952 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:53,953 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:53,954 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:53,958 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:53,961 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:53,961 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,961 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,961 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,961 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,963 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,968 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,972 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,976 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,978 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:53,978 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:53,979 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:53,983 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:53,985 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:53,985 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:53,986 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,986 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:53,986 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:53,988 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:53,992 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,996 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:53,999 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,001 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:54,002 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:54,003 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:54,007 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:54,010 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:54,010 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,010 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,011 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,011 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,013 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,017 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,021 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,024 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,026 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:54,027 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:54,028 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:54,032 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:54,035 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:54,035 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,035 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,036 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,036 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,037 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,042 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,045 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,049 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,052 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:54,052 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:54,053 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:54,057 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:54,060 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:54,060 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,060 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,061 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,061 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,062 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,067 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,070 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,074 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,076 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:54,076 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:54,077 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:54,077 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:54,080 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:54,080 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,081 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,081 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,081 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,082 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,087 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,090 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,119 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-30 14:11:54,121 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-30 14:11:54,122 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:54,123 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:54,123 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:54,124 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:54,124 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,124 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:54,125 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,125 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:54,125 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:54,126 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,127 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,127 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,128 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:54,128 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:54,128 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:54,128 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:54,129 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:54,129 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,129 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:54,129 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,129 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:54,130 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:54,140 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:54,146 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:54,152 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:54,158 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:54,159 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:54,163 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:54,163 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:54,164 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:54,164 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:54,164 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:54,164 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:54,164 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:54,165 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:54,165 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,166 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,167 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,167 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:54,167 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:54,168 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:54,171 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:54,172 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:54,172 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 30), torch.int64', '29')
2023-10-30 14:11:54,172 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:54,172 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 30), torch.int64', '29')
2023-10-30 14:11:54,172 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:54,173 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 30), torch.int64', '29'), {})
2023-10-30 14:11:54,174 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,174 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,175 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,176 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:54,177 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:54,178 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:54,181 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:54,185 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:54,185 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,185 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,185 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,186 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,187 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,192 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,196 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,200 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,202 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,202 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:54,204 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:54,208 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:54,211 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:54,211 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,211 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,211 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,211 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,213 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,218 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,221 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,225 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,227 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,227 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:54,229 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:54,232 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:54,235 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:54,236 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,236 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,236 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,236 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,238 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,242 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,246 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,249 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,251 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,252 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:54,253 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:54,256 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:54,260 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:54,260 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,260 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,260 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,260 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,262 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,266 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,270 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,273 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,275 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,276 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:54,277 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:54,280 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:54,283 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:54,284 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,284 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,284 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,284 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,286 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,290 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,294 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,297 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,306 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,306 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:54,308 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:54,311 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:54,314 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:54,314 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,315 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,315 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,315 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,317 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,321 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,325 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,329 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,331 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,331 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:54,332 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:54,336 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:54,339 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:54,339 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,339 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,339 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,339 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,341 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,345 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,349 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,353 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,355 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,355 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:54,356 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:54,360 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:54,363 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:54,363 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,363 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,363 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,364 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,365 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,394 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,424 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,457 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,489 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,490 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:54,491 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:54,499 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:54,504 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:54,504 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,505 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,505 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,506 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,508 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,543 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,574 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,610 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,619 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,620 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:54,621 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:54,624 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:54,627 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:54,627 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,627 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,628 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,628 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,629 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,634 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,637 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,641 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,643 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,643 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:54,644 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:54,648 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:54,651 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:54,651 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,651 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,651 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,651 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,653 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,658 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,661 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,665 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,667 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,667 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:54,669 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:54,669 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:54,673 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:54,673 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,673 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,673 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,674 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,675 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,680 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,683 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,687 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-30 14:11:54,689 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-30 14:11:54,689 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:54,690 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:54,690 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:54,691 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:54,691 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,691 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:54,691 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,692 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:54,692 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:54,693 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,693 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,694 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,695 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:54,695 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:54,695 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:54,696 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:54,696 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:54,696 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,697 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:54,697 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,697 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:54,697 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:54,705 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:54,711 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:54,717 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:54,724 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:54,725 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:54,728 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:54,729 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:54,729 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:54,729 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:54,729 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:54,730 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:54,730 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:54,730 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:54,731 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,731 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,732 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,732 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:54,733 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:54,733 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:54,738 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:54,739 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:54,739 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 31), torch.int64', '30')
2023-10-30 14:11:54,739 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:54,739 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 31), torch.int64', '30')
2023-10-30 14:11:54,739 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:54,740 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 31), torch.int64', '30'), {})
2023-10-30 14:11:54,741 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,741 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,742 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:54,743 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:54,744 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:54,744 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:54,748 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:54,751 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:54,752 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,752 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,752 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,752 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,754 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,759 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,762 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,766 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,768 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:54,768 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:54,769 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:54,773 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:54,776 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:54,776 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,776 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,776 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,776 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,778 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,783 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,786 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,790 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,792 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:54,792 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:54,793 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:54,796 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:54,800 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:54,800 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,800 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,800 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,800 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,802 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,807 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,811 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,815 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,817 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:54,817 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:54,818 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:54,822 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:54,825 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:54,825 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,825 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,826 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,826 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,827 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,845 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,849 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,852 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,854 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:54,855 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:54,856 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:54,860 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:54,863 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:54,863 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,863 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,864 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,864 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,866 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,871 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,876 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,880 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,883 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:54,883 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:54,885 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:54,890 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:54,894 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:54,894 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,894 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,895 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,895 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,897 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,903 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,908 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,913 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,916 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:54,916 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:54,918 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:54,923 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:54,928 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:54,928 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,929 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,929 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,929 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,932 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,938 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,942 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,948 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,951 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:54,951 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:54,954 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:54,958 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:54,963 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:54,963 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,963 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,964 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,964 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,966 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:54,972 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,977 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,982 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:54,985 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:54,985 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:54,987 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:54,992 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:54,996 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:54,996 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:54,996 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,997 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:54,997 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:54,999 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,003 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,007 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,011 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,013 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:55,013 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:55,014 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:55,018 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:55,022 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:55,022 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,022 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,022 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,023 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,024 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,029 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,033 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,037 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,039 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:55,039 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:55,040 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:55,044 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:55,048 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:55,048 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,048 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,048 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,048 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,050 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,054 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,059 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,063 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,065 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:55,065 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:55,067 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:55,067 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:55,071 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:55,071 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,071 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,071 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,071 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,073 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,078 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,082 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,085 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-30 14:11:55,088 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-30 14:11:55,088 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:55,089 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:55,089 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:55,090 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:55,090 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,090 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,090 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,091 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,091 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:55,092 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,092 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,093 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,094 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,094 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:55,094 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:55,095 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:55,095 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:55,095 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,095 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,095 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,096 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,096 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:55,108 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,117 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,123 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,129 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:55,132 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:55,135 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:55,136 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:55,136 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:55,136 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:55,136 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,137 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:55,137 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,137 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:55,138 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,138 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,139 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,139 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,140 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:55,140 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:55,144 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:55,144 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:55,144 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 32), torch.int64', '31')
2023-10-30 14:11:55,144 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,144 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 32), torch.int64', '31')
2023-10-30 14:11:55,145 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,145 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 32), torch.int64', '31'), {})
2023-10-30 14:11:55,146 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,146 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,147 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,148 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,149 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:55,149 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:55,153 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:55,156 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:55,157 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,157 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,157 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,157 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,159 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,164 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,168 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,171 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,173 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,174 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:55,175 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:55,179 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:55,182 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:55,183 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,183 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,183 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,183 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,185 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,189 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,193 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,197 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,199 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,200 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:55,201 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:55,205 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:55,208 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:55,208 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,208 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,209 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,209 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,211 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,243 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,247 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,251 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,253 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,253 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:55,255 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:55,258 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:55,261 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:55,261 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,261 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,262 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,262 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,264 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,269 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,273 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,277 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,279 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,279 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:55,281 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:55,284 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:55,287 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:55,287 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,288 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,288 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,288 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,290 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,294 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,299 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,303 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,305 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,305 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:55,307 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:55,310 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:55,313 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:55,313 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,313 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,313 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,314 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,315 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,324 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,327 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,330 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,332 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,333 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:55,334 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:55,337 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:55,340 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:55,341 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,341 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,341 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,341 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,344 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,349 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,353 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,368 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,370 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,370 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:55,371 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:55,375 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:55,378 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:55,378 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,378 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,379 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,379 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,380 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,385 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,389 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,392 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,394 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,394 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:55,396 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:55,399 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:55,403 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:55,403 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,403 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,403 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,403 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,405 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,410 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,413 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,417 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,419 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,419 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:55,420 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:55,424 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:55,427 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:55,427 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,427 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,427 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,428 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,429 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,433 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,437 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,440 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,442 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,442 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:55,444 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:55,447 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:55,450 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:55,450 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,450 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,450 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,451 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,452 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,457 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,460 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,466 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,468 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,468 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:55,469 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:55,470 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:55,473 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:55,473 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,474 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,474 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,474 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,476 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,480 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,484 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,487 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-30 14:11:55,489 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-30 14:11:55,489 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:55,490 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:55,491 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:55,491 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:55,491 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,491 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,492 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,492 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,492 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:55,493 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,493 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,494 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,494 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,495 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:55,495 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:55,495 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:55,496 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:55,496 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,496 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,496 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,496 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,497 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:55,504 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,510 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,516 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,522 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:55,524 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:55,527 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:55,527 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:55,528 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:55,528 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:55,528 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,528 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:55,528 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,529 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:55,530 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,530 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,531 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,531 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,532 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:55,532 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:55,536 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:55,536 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:55,536 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 33), torch.int64', '32')
2023-10-30 14:11:55,536 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,536 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 33), torch.int64', '32')
2023-10-30 14:11:55,537 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,537 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 33), torch.int64', '32'), {})
2023-10-30 14:11:55,538 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,538 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,539 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,539 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,541 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:55,541 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:55,545 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:55,548 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:55,548 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,548 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,548 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,549 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,550 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,555 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,558 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,561 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,564 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,564 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:55,565 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:55,569 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:55,572 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:55,572 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,572 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,572 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,572 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,574 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,578 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,582 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,585 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,587 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,588 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:55,589 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:55,592 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:55,595 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:55,596 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,596 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,596 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,596 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,598 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,602 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,606 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,609 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,611 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,612 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:55,613 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:55,616 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:55,619 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:55,620 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,620 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,620 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,620 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,622 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,635 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,639 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,642 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,644 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,645 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:55,646 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:55,650 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:55,653 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:55,653 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,653 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,653 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,653 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,655 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,660 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,663 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,667 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,669 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,669 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:55,671 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:55,674 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:55,677 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:55,678 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,678 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,678 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,678 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,680 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,684 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,688 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,692 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,694 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,694 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:55,695 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:55,699 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:55,702 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:55,703 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,703 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,703 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,703 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,705 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,709 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,713 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,717 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,719 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,719 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:55,721 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:55,724 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:55,727 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:55,727 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,728 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,728 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,728 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,730 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,734 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,738 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,741 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,743 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,744 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:55,745 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:55,749 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:55,752 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:55,752 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,752 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,752 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,753 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,754 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,759 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,763 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,783 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,785 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,785 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:55,786 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:55,790 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:55,793 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:55,794 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,794 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,794 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,794 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,796 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,801 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,805 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,809 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,811 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,812 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:55,813 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:55,816 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:55,821 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:55,821 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,821 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,821 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,821 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,823 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,828 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,849 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,852 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,854 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,855 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:55,856 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:55,857 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:55,860 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:55,860 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,860 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,860 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,861 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,862 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,867 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,870 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,874 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-30 14:11:55,876 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-30 14:11:55,876 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:55,877 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:55,877 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:55,878 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:55,878 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,878 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,878 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,879 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,879 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:55,880 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,880 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,881 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,882 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,882 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:55,882 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:55,882 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:55,883 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:55,883 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,883 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,883 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,883 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,884 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:55,891 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,897 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,903 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:55,909 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:55,911 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:55,914 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:55,914 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:55,914 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:55,915 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:55,915 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,915 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:55,915 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,915 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:55,916 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,917 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,917 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,918 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,918 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:55,918 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:55,922 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:55,922 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:55,923 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 34), torch.int64', '33')
2023-10-30 14:11:55,923 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:55,923 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 34), torch.int64', '33')
2023-10-30 14:11:55,923 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:55,923 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 34), torch.int64', '33'), {})
2023-10-30 14:11:55,924 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,925 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,925 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:55,926 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:55,927 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:55,928 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:55,931 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:55,934 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:55,934 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,934 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,935 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,935 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,937 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,941 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:55,945 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:55,949 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:55,951 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:55,951 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:55,952 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:55,956 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:55,959 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:55,960 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,960 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,960 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,960 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,962 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,967 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:55,971 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:55,974 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:55,977 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:55,977 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:55,978 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:55,982 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:55,986 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:55,986 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:55,986 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,987 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:55,987 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:55,989 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:55,993 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:55,998 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,001 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,004 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,004 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:56,005 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:56,009 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:56,012 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:56,012 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,013 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,013 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,013 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,015 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,019 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,023 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,038 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,040 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,040 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:56,042 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:56,046 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:56,049 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:56,049 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,049 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,049 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,050 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,051 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,056 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,060 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,063 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,065 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,065 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:56,067 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:56,070 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:56,073 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:56,074 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,074 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,074 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,074 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,076 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,080 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,084 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,087 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,090 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,090 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:56,091 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:56,095 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:56,098 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:56,098 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,098 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,099 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,099 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,100 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,116 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,120 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,124 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,126 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,127 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:56,128 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:56,131 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:56,135 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:56,135 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,135 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,135 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,135 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,137 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,142 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,146 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,149 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,151 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,151 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:56,153 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:56,156 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:56,159 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:56,159 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,159 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,159 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,160 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,161 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,166 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,169 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,173 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,175 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,175 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:56,177 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:56,180 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:56,183 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:56,184 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,184 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,184 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,184 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,186 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,191 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,194 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,198 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,200 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,200 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:56,201 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:56,205 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:56,208 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:56,209 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,209 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,209 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,209 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,211 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,215 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,219 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,223 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,239 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,239 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:56,240 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:56,241 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:56,244 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:56,244 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,245 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,245 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,245 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,247 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,251 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,255 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,259 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-30 14:11:56,261 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-30 14:11:56,261 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:56,262 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:56,263 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:56,263 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:56,263 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,263 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:56,264 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,264 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:56,264 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:56,265 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,265 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,266 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,267 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:56,267 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:56,267 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:56,268 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:56,268 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:56,268 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,268 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:56,268 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,268 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:56,269 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:56,278 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:56,285 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:56,291 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:56,297 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:56,298 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:56,301 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:56,302 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:56,302 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:56,302 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:56,303 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:56,303 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:56,303 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:56,303 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:56,304 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,304 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,305 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,305 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:56,306 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:56,306 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:56,310 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:56,310 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:56,311 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 35), torch.int64', '34')
2023-10-30 14:11:56,311 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:56,311 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 35), torch.int64', '34')
2023-10-30 14:11:56,311 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:56,311 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 35), torch.int64', '34'), {})
2023-10-30 14:11:56,312 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,313 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,314 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,314 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:56,316 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:56,316 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:56,319 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:56,323 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:56,323 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,323 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,323 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,323 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,325 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,330 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,333 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,337 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,339 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,339 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:56,341 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:56,344 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:56,347 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:56,347 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,347 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,348 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,348 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,349 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,354 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,358 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,362 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,364 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,364 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:56,366 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:56,369 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:56,372 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:56,372 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,373 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,373 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,373 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,375 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,379 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,384 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,388 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,390 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,390 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:56,392 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:56,395 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:56,399 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:56,399 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,399 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,399 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,399 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,401 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,405 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,409 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,413 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,415 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,415 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:56,416 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:56,420 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:56,423 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:56,423 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,423 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,424 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,424 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,426 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,430 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,434 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,437 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,440 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,440 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:56,441 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:56,444 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:56,447 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:56,448 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,448 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,448 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,448 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,450 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,454 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,458 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,462 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,464 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,464 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:56,466 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:56,469 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:56,472 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:56,472 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,473 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,473 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,473 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,475 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,479 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,483 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,487 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,489 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,489 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:56,491 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:56,495 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:56,498 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:56,498 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,498 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,498 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,498 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,500 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,512 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,515 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,519 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,522 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,522 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:56,523 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:56,527 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:56,530 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:56,531 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,531 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,531 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,531 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,533 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,538 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,542 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,545 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,548 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,548 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:56,549 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:56,553 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:56,556 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:56,556 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,556 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,556 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,556 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,558 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,563 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,567 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,571 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,573 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,573 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:56,575 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:56,578 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:56,581 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:56,581 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,581 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,582 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,582 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,584 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,588 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,592 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,596 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,598 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,598 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:56,599 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:56,600 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:56,603 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:56,603 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,603 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,604 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,604 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,606 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,610 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,614 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,618 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-30 14:11:56,620 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-30 14:11:56,620 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:56,621 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:56,621 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:56,622 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:56,622 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,622 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:56,622 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,622 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:56,623 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:56,624 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,624 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,625 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,625 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:56,626 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:56,626 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:56,626 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:56,627 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:56,627 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,627 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:56,627 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,627 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:56,628 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:56,636 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:56,642 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:56,648 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:56,654 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:56,655 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:56,658 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:56,659 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:56,659 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:56,659 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:56,659 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:56,660 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:56,660 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:56,660 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:56,661 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,661 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,662 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,662 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:56,663 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:56,663 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:56,667 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:56,667 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:56,667 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 36), torch.int64', '35')
2023-10-30 14:11:56,667 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:56,667 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 36), torch.int64', '35')
2023-10-30 14:11:56,668 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:56,668 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 36), torch.int64', '35'), {})
2023-10-30 14:11:56,669 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,669 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,670 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:56,671 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:56,672 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:56,672 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:56,676 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:56,679 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:56,679 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,680 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,680 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,680 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,682 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,686 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,690 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,694 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,696 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,696 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:56,697 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:56,701 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:56,704 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:56,704 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,704 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,704 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,705 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,706 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,711 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,715 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,719 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,721 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,721 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:56,722 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:56,726 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:56,729 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:56,729 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,729 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,729 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,730 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,732 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,738 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,741 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,745 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,747 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,747 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:56,749 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:56,752 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:56,755 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:56,755 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,756 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,756 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,756 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,758 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,762 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,766 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,770 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,772 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,772 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:56,774 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:56,777 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:56,780 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:56,780 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,780 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,781 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,781 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,783 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,787 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,791 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,795 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,797 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,798 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:56,800 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:56,803 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:56,807 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:56,807 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,807 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,807 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,808 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,810 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,829 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,833 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,836 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,839 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,839 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:56,841 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:56,848 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:56,852 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:56,852 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,852 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,852 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,852 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,854 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,859 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,863 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,867 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,869 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,869 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:56,871 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:56,874 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:56,877 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:56,877 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,877 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,877 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,878 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,880 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,884 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,888 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,892 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,894 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,894 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:56,895 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:56,899 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:56,902 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:56,902 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,902 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,903 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,903 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,905 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,910 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,914 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,918 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,923 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,923 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:56,924 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:56,928 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:56,931 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:56,931 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,931 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,932 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,932 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,934 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,939 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,943 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,947 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:56,949 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:56,949 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:56,950 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:56,955 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:56,958 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:56,958 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:56,958 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,958 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:56,959 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:56,961 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:56,995 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:57,004 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:57,008 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:57,011 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:57,012 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:57,014 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:57,014 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:57,018 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:57,018 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,018 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,018 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,019 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,021 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,027 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:57,031 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:57,036 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-30 14:11:57,038 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-30 14:11:57,039 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:57,040 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:57,041 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:57,041 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:57,041 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,041 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,042 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,042 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,043 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:57,044 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,045 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,045 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,046 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:57,046 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:57,047 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:57,047 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:57,047 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:57,048 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,048 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,048 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,048 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,049 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:57,058 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,065 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,073 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,095 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:57,096 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:57,100 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:57,100 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:57,101 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:57,101 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:57,101 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,101 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:57,101 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,102 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:57,103 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,103 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,104 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,104 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:57,105 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:57,105 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:57,109 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:57,109 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:57,109 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 37), torch.int64', '36')
2023-10-30 14:11:57,109 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,109 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 37), torch.int64', '36')
2023-10-30 14:11:57,110 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,110 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 37), torch.int64', '36'), {})
2023-10-30 14:11:57,111 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,112 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,112 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,113 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:57,115 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:57,115 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:57,119 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:57,123 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:57,123 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,123 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,123 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,123 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,125 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,130 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,134 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,139 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,141 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,141 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:57,143 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:57,146 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:57,150 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:57,150 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,150 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,151 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,151 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,153 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,158 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,162 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,166 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,168 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,168 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:57,170 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:57,174 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:57,177 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:57,178 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,178 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,178 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,178 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,181 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,186 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,190 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,196 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,198 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,199 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:57,200 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:57,204 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:57,207 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:57,207 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,208 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,208 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,208 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,210 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,215 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,219 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,223 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,225 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,226 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:57,227 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:57,231 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:57,234 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:57,235 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,235 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,235 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,235 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,237 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,242 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,246 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,250 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,253 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,253 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:57,254 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:57,257 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:57,261 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:57,261 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,261 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,262 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,262 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,264 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,269 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,273 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,277 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,279 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,279 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:57,280 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:57,284 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:57,288 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:57,288 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,288 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,288 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,288 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,290 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,306 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,310 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,313 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,315 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,316 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:57,318 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:57,321 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:57,325 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:57,325 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,325 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,325 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,325 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,327 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,332 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,336 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,340 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,342 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,342 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:57,344 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:57,348 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:57,351 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:57,351 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,351 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,351 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,351 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,353 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,358 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,362 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,365 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,368 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,368 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:57,369 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:57,373 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:57,376 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:57,376 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,376 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,377 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,377 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,379 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,383 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,387 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,391 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,394 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,394 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:57,395 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:57,399 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:57,403 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:57,403 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,403 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,403 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,403 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,405 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,410 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,414 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,418 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,420 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,420 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:57,421 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:57,422 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:57,425 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:57,425 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,425 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,426 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,426 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,428 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,432 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,436 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,440 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-30 14:11:57,442 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-30 14:11:57,442 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:57,443 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:57,444 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:57,444 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:57,444 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,444 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,445 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,445 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,445 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:57,446 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,446 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,447 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,448 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:57,448 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:57,448 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:57,448 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:57,449 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:57,449 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,449 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,449 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,449 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,450 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:57,457 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,463 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,470 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,476 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:57,477 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-30 14:11:57,481 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:57,481 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:57,482 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-30 14:11:57,482 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-30 14:11:57,482 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,482 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-30 14:11:57,482 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,483 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-30 14:11:57,483 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,484 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,484 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,485 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:57,485 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-30 14:11:57,485 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:57,489 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 14:11:57,489 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-30 14:11:57,490 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 38), torch.int64', '37')
2023-10-30 14:11:57,490 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,490 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 38), torch.int64', '37')
2023-10-30 14:11:57,490 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,491 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 38), torch.int64', '37'), {})
2023-10-30 14:11:57,491 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,492 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,493 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,493 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:57,495 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-30 14:11:57,495 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:57,498 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 14:11:57,502 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-30 14:11:57,502 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,502 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,503 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,503 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,505 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,510 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,514 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,518 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,520 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,520 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-30 14:11:57,522 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:57,525 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 14:11:57,529 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-30 14:11:57,529 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,529 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,529 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,529 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,531 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,536 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,540 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,543 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,545 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,546 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-30 14:11:57,547 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:57,550 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 14:11:57,553 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-30 14:11:57,554 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,554 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,554 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,554 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,556 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,575 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,579 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,582 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,584 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,585 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-30 14:11:57,586 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:57,589 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 14:11:57,593 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-30 14:11:57,593 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,593 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,594 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,594 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,596 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,600 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,604 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,608 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,610 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,610 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-30 14:11:57,612 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:57,616 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 14:11:57,620 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-30 14:11:57,620 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,620 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,620 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,620 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,622 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,628 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,631 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,635 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,637 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,638 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-30 14:11:57,639 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:57,643 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 14:11:57,646 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-30 14:11:57,646 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,646 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,647 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,647 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,649 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,653 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,657 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,661 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,663 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,663 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-30 14:11:57,665 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:57,668 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 14:11:57,671 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-30 14:11:57,671 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,672 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,672 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,672 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,674 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,679 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,683 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,687 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,689 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,690 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-30 14:11:57,691 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:57,695 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 14:11:57,698 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-30 14:11:57,698 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,699 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,699 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,699 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,701 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,706 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,710 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,714 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,716 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,717 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-30 14:11:57,718 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:57,722 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 14:11:57,725 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-30 14:11:57,725 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,726 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,726 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,726 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,728 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,733 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,737 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,741 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,744 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,744 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-30 14:11:57,745 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:57,749 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 14:11:57,753 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-30 14:11:57,753 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,753 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,753 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,753 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,755 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,773 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,777 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,781 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,783 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,783 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-30 14:11:57,784 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:57,788 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 14:11:57,792 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-30 14:11:57,792 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,792 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,792 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,792 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,794 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,799 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,803 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,807 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,809 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,810 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-30 14:11:57,811 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:57,812 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 14:11:57,815 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-30 14:11:57,816 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,816 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,816 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,816 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-30 14:11:57,818 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-30 14:11:57,830 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,833 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,838 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-30 14:11:57,840 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-30 14:11:57,840 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-30 14:11:57,841 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:57,842 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 14:11:57,842 [flexgen.py:112 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-30 14:11:57,842 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,843 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,843 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,843 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,843 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:57,844 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,845 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,846 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-30 14:11:57,847 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-30 14:11:57,847 [model.py:402 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-30 14:11:57,847 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 14:11:57,848 [model.py:392 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 14:11:57,848 [flexgen.py:112 in flexgen_forward] DEBUG - layer: lm_head
2023-10-30 14:11:57,848 [flexgen.py:113 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-30 14:11:57,848 [flexgen.py:114 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 14:11:57,849 [flexgen.py:122 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-30 14:11:57,849 [flexgen.py:123 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-30 14:11:57,849 [flexgen.py:132 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-30 14:11:57,858 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,864 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,870 [flexgen.py:129 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-30 14:11:57,877 [flexgen.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-30 14:11:57,879 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-30 14:11:57,879 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 14:11:57,879 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-30 14:11:57,880 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 14:11:57,880 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-30 14:11:57,880 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 14:11:57,880 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-30 14:11:57,880 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 14:11:57,887 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-30 14:11:57,888 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-30 14:11:57,888 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-30 14:11:57,888 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-30 14:11:57,888 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-30 14:11:57,888 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-30 14:11:57,888 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-30 14:11:57,888 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-30 14:11:57,888 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-30 14:11:57,889 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-30 14:11:57,889 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-30 14:11:57,889 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-30 14:11:57,889 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-30 14:11:57,889 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-30 14:11:57,889 [flexgen.py:69 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-30 14:11:57,889 [flexgen.py:69 in layer_reset] DEBUG - lm_head from flexgen to old.
