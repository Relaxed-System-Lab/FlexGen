2023-10-31 06:07:22,155 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp0nyxnv1_
2023-10-31 06:07:22,156 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp0nyxnv1_/_remote_module_non_scriptable.py
2023-10-31 06:07:22,302 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-31 06:07:22,437 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:07:24,175 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-31 06:07:24,546 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-31 06:07:24,547 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-31 06:07:24,547 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-31 06:07:24,547 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-31 06:07:25,457 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:07:25,551 [model.py:130 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-31 06:07:25,551 [model.py:69 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-31 06:07:25,593 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 06:07:25,688 [model.py:79 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-31 06:07:25,691 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-31 06:07:25,692 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-31 06:07:25,692 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-31 06:07:25,693 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-31 06:07:25,694 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-31 06:07:25,695 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-31 06:07:25,696 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-31 06:07:25,697 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-31 06:07:25,698 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-31 06:07:25,699 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-31 06:07:25,700 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-31 06:07:25,701 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-31 06:07:25,702 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-31 06:07:25,703 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-31 06:07:25,704 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 06:07:25,704 [model.py:261 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 06:07:25,704 [model.py:267 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-31 06:07:25,707 [model.py:303 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-31 06:07:25,749 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 06:07:25,955 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-31 06:07:25,955 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-31 06:07:25,955 [model.py:393 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-31 06:07:25,956 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-31 06:07:25,956 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-31 06:07:25,956 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-31 06:07:25,956 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-31 06:07:25,956 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-31 06:07:25,956 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-31 06:07:25,956 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-31 06:07:25,957 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-31 06:07:25,957 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-31 06:07:25,957 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-31 06:07:25,957 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-31 06:07:25,957 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-31 06:07:25,957 [model.py:393 in to_test_forward] DEBUG - lm_head to test forward
2023-10-31 06:07:25,961 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:25,963 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-31 06:07:25,964 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:25,965 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-31 06:07:25,965 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:25,978 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-31 06:07:25,981 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:25,989 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-31 06:07:25,992 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:25,999 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-31 06:07:26,002 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:26,009 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-31 06:07:26,011 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:26,018 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-31 06:07:26,020 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:26,027 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-31 06:07:26,029 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:26,036 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-31 06:07:26,038 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:26,045 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-31 06:07:26,047 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:26,053 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-31 06:07:26,056 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:26,063 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-31 06:07:26,065 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:26,072 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-31 06:07:26,074 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:26,081 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-31 06:07:26,083 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:26,084 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-31 06:07:26,084 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:26,093 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-31 06:07:26,098 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-31 06:07:26,098 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-31 06:07:26,099 [model.py:401 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-31 06:07:26,099 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-31 06:07:26,099 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-31 06:07:26,099 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-31 06:07:26,099 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-31 06:07:26,099 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-31 06:07:26,099 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-31 06:07:26,099 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-31 06:07:26,100 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-31 06:07:26,100 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-31 06:07:26,100 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-31 06:07:26,100 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-31 06:07:26,100 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-31 06:07:26,100 [model.py:401 in reset_forward] DEBUG - lm_head from test to old.
2023-10-31 06:07:26,114 [model.py:516 in init_all_weights] DEBUG - init all weights...
2023-10-31 06:07:26,142 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-31 06:07:26,143 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-31 06:07:26,143 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-31 06:07:26,143 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-31 06:07:26,143 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-31 06:07:26,143 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-31 06:07:26,143 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-31 06:07:26,144 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-31 06:07:26,144 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-31 06:07:26,144 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-31 06:07:26,144 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-31 06:07:26,144 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-31 06:07:26,144 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-31 06:07:26,144 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-31 06:07:26,145 [wrapper.py:264 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-31 06:07:26,145 [wrapper.py:264 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-31 06:07:26,191 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 06:07:26,374 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:26,375 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64',)
2023-10-31 06:07:26,375 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:26,375 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:26,375 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:26,375 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:26,376 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64',)
2023-10-31 06:07:26,376 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:26,377 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64',), {})
2023-10-31 06:07:26,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,379 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,380 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:07:26,380 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:26,381 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64', '0')
2023-10-31 06:07:26,381 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:26,381 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:26,381 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:26,385 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:26,385 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64', '0')
2023-10-31 06:07:26,386 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:26,386 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64', '0'), {})
2023-10-31 06:07:26,387 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,387 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,388 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,389 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:07:26,391 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:26,391 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,391 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,391 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:26,392 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:26,395 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:26,399 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,399 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,400 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,407 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,411 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,423 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,424 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:26,424 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,424 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,424 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:26,426 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:26,430 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:26,435 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,435 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,436 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,441 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,444 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,448 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,451 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,452 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:26,452 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,452 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,452 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:26,454 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:26,458 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:26,462 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,462 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,463 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,472 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,479 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,479 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:26,479 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,479 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,480 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:26,481 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:26,486 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:26,490 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,491 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,491 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,496 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,500 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,504 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,507 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,507 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:26,507 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,507 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,507 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:26,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:26,513 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:26,517 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,517 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,518 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,522 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,526 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,530 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,532 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,533 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:26,533 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,533 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,533 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:26,535 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:26,539 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:26,543 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,543 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,544 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,559 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,562 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,562 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:26,562 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,563 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,563 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:26,564 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:26,568 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:26,572 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,572 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,573 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,581 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,585 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,587 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,588 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:26,588 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,588 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,588 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:26,590 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:26,593 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:26,598 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,598 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,599 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,604 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,608 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,615 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,615 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:26,616 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,616 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,616 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:26,618 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:26,621 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:26,626 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,626 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,627 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,631 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,635 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,641 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,642 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:26,642 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,642 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,643 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:26,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:26,648 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:26,652 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,652 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,653 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,658 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,661 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,665 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,668 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,668 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:26,668 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,668 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,668 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:26,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:26,674 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:26,678 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,678 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,679 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,685 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,692 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,695 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,695 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:26,695 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,696 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,696 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:26,697 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:26,698 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:26,702 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,702 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,703 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,712 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,716 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 06:07:26,719 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 06:07:26,719 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:26,719 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,719 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:26,719 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:26,720 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:26,721 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:26,722 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,722 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:26,722 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 06:07:26,723 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,725 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 06:07:26,726 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 06:07:26,726 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:26,726 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 06:07:26,726 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:26,726 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:26,727 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:26,727 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:26,728 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 06:07:26,728 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:26,728 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 06:07:26,740 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:07:26,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:07:26,760 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 06:07:26,773 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 9, 50272), torch.float32


2023-10-31 06:07:26,774 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:26,774 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:26,774 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:26,774 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:26,778 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:26,778 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:26,779 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:26,779 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:26,780 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:26,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:26,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:26,782 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:26,782 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:26,783 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:26,783 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 10), torch.int64', '9')
2023-10-31 06:07:26,783 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:26,783 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:26,783 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:26,787 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:26,788 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 10), torch.int64', '9')
2023-10-31 06:07:26,788 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:26,789 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 10), torch.int64', '9'), {})
2023-10-31 06:07:26,789 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:26,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:26,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:26,791 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:26,793 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:26,793 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,793 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,794 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:26,794 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:26,798 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:26,803 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:26,803 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,805 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,812 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,816 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,819 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,821 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:26,822 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:26,822 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,822 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,822 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:26,824 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:26,827 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:26,831 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:26,831 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,832 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,837 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,840 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,844 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,846 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:26,847 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:26,847 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,847 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,847 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:26,849 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:26,852 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:26,856 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:26,856 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,857 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,862 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,865 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,869 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,872 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:26,872 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:26,872 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,872 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,872 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:26,873 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:26,877 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:26,880 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:26,880 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,882 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,886 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,890 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,893 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,896 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:26,896 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:26,896 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,896 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,896 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:26,898 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:26,901 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:26,905 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:26,905 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,906 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,911 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,915 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,921 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:26,922 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:26,922 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,923 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,923 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:26,924 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:26,928 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:26,931 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:26,931 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,933 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,937 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,941 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,946 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:26,946 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:26,947 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,947 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,947 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:26,948 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:26,952 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:26,955 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:26,956 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,957 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,961 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,965 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,968 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,970 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:26,971 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:26,971 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,971 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,971 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:26,972 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:26,976 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:26,979 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:26,979 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,981 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:26,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,988 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,992 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:26,994 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:26,994 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:26,994 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:26,995 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:26,995 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:26,996 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:26,999 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:27,003 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,003 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,004 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,009 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,015 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,017 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:27,018 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:27,018 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,018 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,018 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:27,020 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:27,023 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:27,026 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,027 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,028 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,033 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,036 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,041 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:27,042 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:27,042 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,042 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,042 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:27,043 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:27,047 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:27,050 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,050 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,052 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,056 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,059 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,065 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:27,065 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:27,065 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,065 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,065 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:27,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:27,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:27,071 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,071 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,072 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,076 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,080 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,083 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 06:07:27,085 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 06:07:27,086 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:27,086 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,086 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,086 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:27,087 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:27,088 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:27,088 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,088 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,089 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:27,090 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,090 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,092 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,092 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:27,092 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,092 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,092 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:27,093 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:27,093 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:27,093 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,093 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,094 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:27,102 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,109 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,115 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,136 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:27,142 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:27,143 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:27,143 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,143 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:27,147 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:27,147 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:27,147 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:27,148 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,148 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:27,149 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,149 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,150 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,150 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,151 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:27,151 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 11), torch.int64', '10')
2023-10-31 06:07:27,151 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,151 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:27,151 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:27,155 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:27,156 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 11), torch.int64', '10')
2023-10-31 06:07:27,156 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,156 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 11), torch.int64', '10'), {})
2023-10-31 06:07:27,157 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,159 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,159 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,161 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:27,161 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,161 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,161 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:27,161 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:27,165 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:27,168 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,169 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,170 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,175 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,178 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,182 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,184 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,185 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:27,185 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,185 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,185 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:27,187 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:27,191 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:27,194 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,194 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,196 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,201 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,204 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,208 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,210 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,210 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:27,210 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,211 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,211 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:27,212 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:27,216 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:27,219 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,219 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,221 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,226 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,229 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,234 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,237 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,237 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:27,237 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,237 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,237 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:27,239 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:27,242 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:27,246 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,246 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,248 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,252 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,255 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,259 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,261 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,261 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:27,261 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,262 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,262 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:27,263 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:27,267 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:27,270 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,270 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,272 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,277 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,283 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,285 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,286 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:27,286 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,286 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,286 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:27,287 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:27,291 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:27,295 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,295 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,296 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,301 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,308 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,310 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,310 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:27,310 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,310 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,311 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:27,312 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:27,315 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:27,319 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,319 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,321 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,325 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,333 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,337 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,339 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,339 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:27,339 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,339 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,340 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:27,341 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:27,344 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:27,348 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,348 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,350 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,361 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,363 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,363 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:27,363 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,364 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,364 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:27,365 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:27,369 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:27,372 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,372 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,374 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,381 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,385 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,387 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,387 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:27,387 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,388 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,388 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:27,389 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:27,393 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:27,396 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,396 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,398 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,418 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,421 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,425 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,427 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,427 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:27,427 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,427 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,427 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:27,429 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:27,432 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:27,436 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,437 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,438 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,442 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,446 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,452 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,452 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:27,452 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,452 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,452 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:27,454 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:27,455 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:27,458 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,458 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,460 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,464 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 06:07:27,473 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 06:07:27,473 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:27,473 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,474 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,474 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:27,475 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:27,475 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:27,476 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,476 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,477 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:27,477 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,478 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,479 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,479 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,480 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:27,480 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,480 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,480 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:27,480 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:27,481 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:27,481 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,481 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,482 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:27,493 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,500 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,512 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:27,514 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:27,514 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:27,514 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,514 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:27,517 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:27,517 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:27,518 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:27,518 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,519 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:27,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,520 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,521 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,522 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:27,522 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 12), torch.int64', '11')
2023-10-31 06:07:27,522 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,522 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:27,522 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:27,526 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:27,526 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 12), torch.int64', '11')
2023-10-31 06:07:27,527 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,527 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 12), torch.int64', '11'), {})
2023-10-31 06:07:27,528 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,528 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,529 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,530 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,532 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:27,532 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,532 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,532 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:27,532 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:27,536 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:27,540 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,540 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,542 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,553 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,555 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,555 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:27,555 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,556 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,556 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:27,557 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:27,561 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:27,564 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,564 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,566 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,571 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,574 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,578 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,580 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,580 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:27,580 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,580 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,581 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:27,582 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:27,586 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:27,589 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,589 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,591 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,611 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,614 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,616 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,616 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:27,616 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,617 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,617 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:27,618 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:27,622 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:27,625 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,625 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,627 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,633 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,636 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,640 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,642 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,642 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:27,642 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,642 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,643 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:27,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:27,648 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:27,651 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,651 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,653 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,657 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,661 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,665 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,667 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,667 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:27,667 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,668 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,668 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:27,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:27,673 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:27,676 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,676 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,678 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,682 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,686 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,689 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,692 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,692 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:27,692 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,692 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,692 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:27,693 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:27,697 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:27,700 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,700 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,702 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,707 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,710 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,714 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,716 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,716 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:27,716 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,717 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,717 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:27,718 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:27,721 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:27,725 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,725 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,727 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,731 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,735 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,738 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,740 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,741 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:27,741 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,741 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,741 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:27,742 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:27,746 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:27,749 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,750 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,751 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,756 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,759 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,762 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,765 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,765 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:27,765 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,765 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,765 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:27,767 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:27,770 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:27,774 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,774 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,776 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,784 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,787 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,790 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,790 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:27,790 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,790 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,790 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:27,791 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:27,795 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:27,799 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,799 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,800 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,808 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,812 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,814 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,814 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:27,815 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,815 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,815 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:27,816 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:27,817 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:27,820 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,820 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,822 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,834 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 06:07:27,840 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 06:07:27,840 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:27,840 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,841 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,841 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:27,842 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:27,842 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:27,843 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,843 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,844 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:27,844 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,845 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,846 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,846 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,847 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:27,847 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,847 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,847 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:27,847 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:27,848 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:27,848 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,848 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,849 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:27,857 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,869 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,882 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:27,896 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:27,898 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:27,899 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:27,899 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,900 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:27,906 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:27,907 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:27,908 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:27,908 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,909 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:27,910 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,911 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,912 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,912 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:27,913 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 13), torch.int64', '12')
2023-10-31 06:07:27,913 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:27,913 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:27,913 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:27,917 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:27,918 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 13), torch.int64', '12')
2023-10-31 06:07:27,918 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:27,918 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 13), torch.int64', '12'), {})
2023-10-31 06:07:27,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:27,921 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:27,922 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:27,923 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,923 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,923 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:27,923 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:27,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:27,930 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,931 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,932 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,939 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:27,943 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:27,947 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:27,949 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:27,949 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:27,949 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,949 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,950 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:27,951 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:27,955 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:27,959 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,959 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,961 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,966 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:27,969 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:27,973 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:27,977 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:27,977 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:27,977 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:27,978 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,978 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:27,979 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:27,983 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:27,987 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:27,987 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:27,988 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:27,993 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:27,996 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,000 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,002 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,003 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:28,003 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,003 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,003 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:28,004 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:28,008 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:28,011 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,011 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,012 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,024 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,026 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,026 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:28,027 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,027 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,027 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:28,028 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:28,032 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:28,035 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,035 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,037 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,042 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,048 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,051 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,051 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:28,051 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,051 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,051 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:28,052 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:28,056 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:28,059 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,060 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,061 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,066 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,069 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,075 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,075 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:28,075 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,075 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,076 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:28,077 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:28,080 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:28,084 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,084 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,085 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,102 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,106 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,109 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,111 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,111 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:28,112 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,112 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,112 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:28,113 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:28,117 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:28,120 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,120 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,122 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,126 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,129 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,133 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,135 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,135 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:28,135 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,135 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,136 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:28,137 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:28,140 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:28,143 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,144 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,145 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,173 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,215 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,218 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,218 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:28,218 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,219 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,219 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:28,221 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:28,225 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:28,228 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,228 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,230 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,234 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,238 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,242 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,244 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,245 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:28,245 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,245 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,245 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:28,247 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:28,251 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:28,254 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,254 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,256 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,260 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,264 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,267 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,269 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,270 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:28,270 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,270 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,271 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:28,272 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:28,273 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:28,276 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,277 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,278 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,284 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,287 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,291 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 06:07:28,293 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 06:07:28,294 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:28,294 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,294 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:28,295 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:28,296 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:28,296 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:28,297 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,297 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:28,298 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:28,298 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,300 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,300 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:28,301 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:28,301 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,301 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:28,301 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:28,301 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:28,302 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:28,302 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,302 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:28,303 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:28,311 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:28,318 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:28,325 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:28,338 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:28,339 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:28,340 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:28,340 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:28,340 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:28,343 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:28,344 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:28,345 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:28,345 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:28,346 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:28,346 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,347 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,348 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,349 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:28,349 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:28,350 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 14), torch.int64', '13')
2023-10-31 06:07:28,350 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:28,350 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:28,351 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:28,354 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:28,355 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 14), torch.int64', '13')
2023-10-31 06:07:28,355 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:28,356 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 14), torch.int64', '13'), {})
2023-10-31 06:07:28,357 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,358 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,359 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:28,361 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:28,362 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,362 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,362 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:28,362 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:28,366 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:28,369 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,370 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,371 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,376 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,379 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,383 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,385 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,385 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:28,385 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,385 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,385 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:28,387 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:28,391 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:28,394 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,394 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,396 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,400 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,404 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,407 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,409 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,409 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:28,410 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,410 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,410 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:28,411 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:28,415 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:28,418 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,418 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,420 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,424 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,428 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,433 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,433 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:28,434 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,434 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,434 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:28,435 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:28,439 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:28,442 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,442 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,444 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,453 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,458 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,464 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,467 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,467 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:28,467 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,468 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,468 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:28,470 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:28,476 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:28,481 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,482 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,484 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,491 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,497 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,497 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:28,497 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,497 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,497 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:28,498 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:28,502 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:28,505 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,505 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,507 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,514 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,521 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,521 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:28,522 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,522 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,522 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:28,523 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:28,527 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:28,530 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,530 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,531 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,536 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,539 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,544 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,544 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:28,544 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,544 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,545 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:28,546 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:28,549 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:28,552 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,552 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,554 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,558 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,562 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,565 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,567 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,567 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:28,567 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,568 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,568 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:28,569 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:28,572 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:28,576 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,576 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,578 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,589 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,592 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,596 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,598 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,599 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:28,599 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,599 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,600 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:28,601 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:28,604 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:28,608 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,608 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,609 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,614 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,617 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,620 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,622 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,623 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:28,623 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,623 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,623 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:28,624 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:28,628 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:28,631 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,631 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,633 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,637 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,641 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,644 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,646 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,646 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:28,646 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,647 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,647 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:28,648 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:28,649 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:28,652 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,652 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,654 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,658 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,661 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,665 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 06:07:28,667 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 06:07:28,667 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:28,667 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,667 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:28,667 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:28,668 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:28,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:28,670 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,670 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:28,670 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:28,671 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,673 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,673 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:28,673 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:28,673 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,673 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:28,674 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:28,674 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:28,674 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:28,675 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,675 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:28,675 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:28,683 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:28,690 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:28,696 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:28,704 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:28,705 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:28,705 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:28,705 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:28,705 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:28,708 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:28,709 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:28,709 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:28,709 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:28,710 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:28,711 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,711 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,712 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,712 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:28,713 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:28,713 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 15), torch.int64', '14')
2023-10-31 06:07:28,713 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:28,713 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:28,713 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:28,717 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:28,717 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 15), torch.int64', '14')
2023-10-31 06:07:28,717 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:28,718 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 15), torch.int64', '14'), {})
2023-10-31 06:07:28,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,720 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:28,721 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:28,722 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:28,722 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,723 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,723 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:28,723 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:28,726 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:28,730 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,730 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,732 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,736 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,739 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,743 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,745 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,745 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:28,745 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,745 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,745 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:28,747 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:28,750 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:28,754 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,754 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,755 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,760 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,763 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,766 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,768 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,768 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:28,769 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,769 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,769 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:28,770 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:28,774 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:28,777 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,777 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,779 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,786 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,790 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,792 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,792 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:28,792 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,792 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,792 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:28,794 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:28,797 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:28,800 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,800 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,802 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,806 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,810 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,813 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,815 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,815 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:28,815 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,815 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,816 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:28,817 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:28,821 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:28,824 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,824 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,826 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,867 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,870 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,871 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:28,871 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,871 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,872 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:28,874 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:28,879 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:28,884 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,884 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,886 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,892 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,896 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,901 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,904 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,904 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:28,904 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,905 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,905 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:28,906 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:28,910 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:28,913 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,914 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,915 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,923 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,926 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,929 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,929 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:28,929 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,929 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,929 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:28,931 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:28,934 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:28,938 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,938 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,940 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,947 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,953 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,953 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:28,954 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,954 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,954 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:28,955 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:28,959 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:28,962 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,963 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,964 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,969 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,972 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,978 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:28,978 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:28,978 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:28,978 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,979 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:28,980 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:28,984 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:28,987 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:28,987 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:28,989 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:28,993 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:28,997 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:29,000 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:29,002 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:29,003 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:29,003 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,003 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,003 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:29,004 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:29,008 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:29,011 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,012 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,013 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:29,021 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:29,024 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:29,026 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:29,027 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:29,027 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,027 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,027 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:29,028 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:29,029 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:29,032 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,032 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,034 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,038 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:29,042 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:29,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 06:07:29,047 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 06:07:29,048 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:29,048 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,048 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,048 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:29,049 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:29,050 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:29,050 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,050 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,051 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:29,052 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,052 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,053 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,054 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,054 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:29,054 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,054 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,054 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:29,055 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:29,055 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:29,055 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,056 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,056 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:29,064 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,071 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,078 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,100 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:29,104 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:29,104 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:29,104 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,104 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:29,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:29,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:29,109 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:29,109 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,109 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:29,110 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,111 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,111 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,112 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,112 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:29,112 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 16), torch.int64', '15')
2023-10-31 06:07:29,112 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,112 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:29,113 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:29,116 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:29,117 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 16), torch.int64', '15')
2023-10-31 06:07:29,117 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,117 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 16), torch.int64', '15'), {})
2023-10-31 06:07:29,118 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,119 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,119 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,120 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,122 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:29,122 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,122 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,122 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:29,122 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:29,126 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:29,129 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,129 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,131 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,143 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,146 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,146 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:29,146 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,146 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,146 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:29,148 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:29,151 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:29,155 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,155 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,156 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,161 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,168 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,170 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,170 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:29,170 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,170 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,170 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:29,172 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:29,175 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:29,179 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,179 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,180 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,185 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,192 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,194 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,194 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:29,195 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,195 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,195 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:29,196 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:29,200 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:29,203 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,203 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,205 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,213 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,219 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,219 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:29,219 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,220 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,220 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:29,221 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:29,225 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:29,228 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,229 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,230 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,235 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,239 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,243 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,245 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,245 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:29,245 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,245 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,246 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:29,247 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:29,250 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:29,254 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,254 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,255 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,260 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,263 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,267 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,270 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,270 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:29,270 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,270 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,270 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:29,271 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:29,275 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:29,279 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,279 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,281 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,285 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,289 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,323 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,323 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:29,324 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,324 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,324 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:29,326 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:29,330 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:29,333 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,334 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,336 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,340 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,344 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,348 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,374 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,374 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:29,374 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,375 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,375 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:29,376 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:29,380 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:29,383 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,383 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,385 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,389 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,392 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,396 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,398 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,398 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:29,398 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,398 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,399 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:29,400 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:29,403 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:29,406 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,407 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,408 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,413 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,416 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,422 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,422 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:29,422 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,423 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,423 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:29,424 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:29,428 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:29,431 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,431 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,432 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,437 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,443 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,445 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,445 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:29,446 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,446 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,446 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:29,447 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:29,448 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:29,451 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,451 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,452 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,457 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,460 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,464 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 06:07:29,466 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 06:07:29,466 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:29,467 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,467 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,467 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:29,468 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:29,468 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:29,469 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,469 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,469 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:29,470 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,472 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,472 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,472 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:29,473 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,473 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,473 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:29,473 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:29,473 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:29,474 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,474 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,474 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:29,484 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,497 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,504 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:29,506 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:29,506 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:29,506 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,506 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:29,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:29,510 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:29,510 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:29,510 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,511 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:29,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,513 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,513 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:29,513 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 17), torch.int64', '16')
2023-10-31 06:07:29,513 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,513 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:29,514 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:29,517 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:29,518 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 17), torch.int64', '16')
2023-10-31 06:07:29,518 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,518 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 17), torch.int64', '16'), {})
2023-10-31 06:07:29,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,520 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,521 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,522 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:29,523 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,523 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,523 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:29,523 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:29,527 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:29,530 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,530 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,531 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,535 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,539 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,544 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,544 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:29,544 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,545 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,545 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:29,546 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:29,550 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:29,553 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,553 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,554 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,559 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,562 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,566 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,568 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,568 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:29,568 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,569 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,569 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:29,570 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:29,573 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:29,576 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,577 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,578 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,582 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,603 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,610 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,610 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:29,610 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,611 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,611 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:29,612 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:29,616 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:29,620 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,620 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,622 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,627 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,631 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,634 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,637 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,637 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:29,637 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,637 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,638 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:29,639 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:29,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:29,648 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,648 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,650 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,654 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,659 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,662 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,665 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,665 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:29,665 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,666 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,666 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:29,667 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:29,671 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:29,675 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,675 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,677 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,682 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,686 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,690 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,692 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,692 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:29,692 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,692 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,693 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:29,694 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:29,698 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:29,702 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,702 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,704 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,709 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,713 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,717 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,719 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,719 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:29,719 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,719 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,720 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:29,721 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:29,725 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:29,729 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,729 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,731 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,736 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,740 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,744 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,746 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,746 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:29,746 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,746 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,747 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:29,748 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:29,752 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:29,756 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,756 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,758 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,763 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,767 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,771 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,774 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,774 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:29,774 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,774 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,774 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:29,776 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:29,780 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:29,784 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,784 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,786 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,796 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,799 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,802 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,802 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:29,802 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,802 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,803 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:29,804 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:29,809 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:29,813 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,813 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,815 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,820 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,824 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,827 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,830 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,830 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:29,830 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,830 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,830 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:29,832 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:29,833 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:29,837 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,837 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,839 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,843 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,847 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,851 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 06:07:29,873 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 06:07:29,873 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:29,874 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,874 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,874 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:29,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:29,876 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:29,877 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,877 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,877 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:29,878 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,880 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,880 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,880 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:29,881 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,881 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,881 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:29,881 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:29,882 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:29,882 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,882 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,883 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:29,891 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,905 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:29,912 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:29,914 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:29,914 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:29,914 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,914 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:29,918 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:29,918 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:29,919 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:29,919 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,920 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:29,920 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,921 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,922 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,922 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,923 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:29,923 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 18), torch.int64', '17')
2023-10-31 06:07:29,923 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:29,923 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:29,924 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:29,928 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:29,928 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 18), torch.int64', '17')
2023-10-31 06:07:29,928 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:29,929 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 18), torch.int64', '17'), {})
2023-10-31 06:07:29,929 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,931 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:29,932 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:29,933 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:29,933 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,933 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,934 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:29,934 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:29,938 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:29,942 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,942 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,944 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,948 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:29,952 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:29,956 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:29,959 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:29,959 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:29,959 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,959 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,959 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:29,961 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:29,965 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:29,968 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,969 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,971 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:29,975 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:29,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:29,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:29,985 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:29,985 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:29,985 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:29,985 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,986 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:29,987 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:29,991 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:29,995 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:29,995 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:29,997 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,005 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,009 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,015 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,015 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:30,015 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,015 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,015 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:30,017 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:30,021 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:30,025 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,025 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,027 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,042 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,042 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:30,042 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,043 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,043 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:30,044 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:30,049 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:30,053 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,053 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,055 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,059 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,067 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,069 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,069 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:30,070 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,070 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,070 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:30,071 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:30,076 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:30,079 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,080 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,081 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,102 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,106 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,110 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,112 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,112 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:30,112 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,112 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,112 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:30,114 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:30,118 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:30,122 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,122 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,124 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,129 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,132 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,139 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,139 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:30,139 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,139 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,139 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:30,141 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:30,145 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:30,149 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,149 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,151 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,156 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,160 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,163 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,166 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,166 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:30,166 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,166 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,167 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:30,168 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:30,172 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:30,176 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,176 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,178 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,183 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,187 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,190 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,193 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,193 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:30,193 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,193 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,194 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:30,195 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:30,199 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:30,203 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,203 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,205 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,210 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,215 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,218 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,221 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,221 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:30,221 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,221 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,222 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:30,223 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:30,227 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:30,231 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,231 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,233 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,239 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,243 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,246 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,249 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,249 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:30,249 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,249 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,249 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:30,251 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:30,251 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:30,255 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,256 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,258 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,262 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,266 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,270 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 06:07:30,272 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 06:07:30,272 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:30,273 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,273 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:30,273 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:30,275 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:30,275 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:30,276 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,276 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:30,276 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:30,277 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,278 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,279 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,279 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:30,279 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:30,280 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,280 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:30,280 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:30,280 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:30,281 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:30,281 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,281 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:30,282 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:30,290 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:30,297 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:30,305 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:30,312 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:30,313 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:30,313 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:30,314 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:30,314 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:30,317 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:30,318 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:30,319 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:30,319 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:30,320 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:30,320 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,321 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,322 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,322 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:30,323 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:30,323 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 19), torch.int64', '18')
2023-10-31 06:07:30,323 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:30,323 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:30,323 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:30,328 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:30,328 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 19), torch.int64', '18')
2023-10-31 06:07:30,328 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:30,329 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 19), torch.int64', '18'), {})
2023-10-31 06:07:30,330 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,332 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:30,334 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:30,334 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,334 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,334 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:30,334 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:30,338 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:30,342 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,342 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,344 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,354 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,372 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,376 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,378 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,379 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:30,379 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,379 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,379 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:30,381 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:30,384 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:30,388 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,388 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,390 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,394 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,398 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,402 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,403 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,404 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:30,404 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,404 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,404 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:30,405 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:30,409 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:30,412 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,412 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,414 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,419 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,422 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,428 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,428 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:30,429 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,429 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,429 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:30,430 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:30,434 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:30,437 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,437 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,439 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,443 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,450 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,452 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,452 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:30,452 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,453 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,453 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:30,454 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:30,458 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:30,461 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,461 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,463 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,468 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,472 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,475 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,477 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,478 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:30,478 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,478 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,478 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:30,480 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:30,483 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:30,487 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,487 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,489 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,493 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,497 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,500 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,502 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,503 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:30,503 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,503 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,503 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:30,504 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:30,508 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:30,511 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,511 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,513 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,517 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,524 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,526 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,526 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:30,527 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,527 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,527 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:30,528 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:30,531 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:30,535 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,535 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,537 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,545 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,549 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,551 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,551 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:30,551 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,552 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,552 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:30,553 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:30,557 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:30,560 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,560 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,562 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,566 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,587 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,591 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,594 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,594 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:30,594 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,594 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,594 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:30,596 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:30,600 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:30,604 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,604 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,606 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,611 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,615 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,618 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,621 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,621 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:30,621 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,621 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,621 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:30,623 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:30,627 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:30,631 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,631 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,633 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,642 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,645 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,647 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,648 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:30,648 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,648 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,648 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:30,650 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:30,650 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:30,654 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,654 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,656 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,661 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,665 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,668 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 06:07:30,671 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 06:07:30,671 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:30,672 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,672 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:30,672 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:30,673 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:30,673 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:30,674 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,674 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:30,675 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:30,676 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,676 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,677 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,678 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:30,678 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:30,678 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,678 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:30,678 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:30,679 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:30,679 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:30,679 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,680 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:30,680 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:30,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:30,695 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:30,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:30,710 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:30,711 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:30,711 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:30,711 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:30,711 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:30,715 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:30,716 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:30,716 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:30,716 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:30,717 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:30,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,719 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,720 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:30,720 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:30,720 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 20), torch.int64', '19')
2023-10-31 06:07:30,720 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:30,720 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:30,721 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:30,725 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:30,725 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 20), torch.int64', '19')
2023-10-31 06:07:30,726 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:30,726 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 20), torch.int64', '19'), {})
2023-10-31 06:07:30,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:30,729 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:30,731 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:30,731 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,731 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,731 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:30,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:30,735 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:30,739 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,739 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,741 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,746 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,750 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,754 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,757 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:30,757 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:30,757 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,757 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,757 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:30,759 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:30,763 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:30,767 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,767 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,769 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,774 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,778 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,782 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,784 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:30,784 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:30,784 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,785 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,785 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:30,786 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:30,790 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:30,794 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,795 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,797 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,801 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,809 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,811 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:30,811 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:30,811 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,812 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,812 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:30,813 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:30,817 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:30,821 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,821 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,823 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,828 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,832 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,860 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,875 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:30,875 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:30,875 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,875 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,876 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:30,878 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:30,883 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:30,886 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,887 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,889 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,894 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,905 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:30,905 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:30,906 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,906 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,906 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:30,908 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:30,912 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:30,916 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,917 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,919 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,924 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,928 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,932 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,934 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:30,934 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:30,934 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,935 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,935 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:30,936 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:30,940 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:30,944 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,944 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,946 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,955 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,959 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,962 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:30,962 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:30,962 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,962 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,963 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:30,964 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:30,968 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:30,972 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,972 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,974 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:30,979 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:30,989 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:30,989 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:30,989 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:30,989 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:30,989 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:30,991 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:30,995 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:30,999 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:30,999 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,001 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,006 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,010 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,014 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,016 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:31,017 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:31,017 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,017 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,017 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:31,019 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:31,023 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:31,027 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,027 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,029 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,034 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,038 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,042 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,044 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:31,044 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:31,044 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,045 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,045 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:31,046 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:31,050 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:31,054 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,054 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,056 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,062 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,066 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,071 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,074 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:31,074 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:31,074 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,074 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,074 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:31,076 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:31,077 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:31,081 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,081 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,083 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,105 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,108 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,113 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 06:07:31,115 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 06:07:31,116 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:31,116 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,116 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:31,116 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:31,117 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:31,118 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:31,119 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,119 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:31,119 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:31,120 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,121 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,122 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:31,123 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:31,123 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,123 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:31,123 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:31,123 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:31,124 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:31,124 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,125 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:31,125 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:31,133 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:31,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:31,147 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:31,155 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:31,156 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:31,156 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:31,156 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:31,156 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:31,160 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:31,161 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:31,161 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:31,162 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:31,162 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:31,163 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,165 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,165 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:31,166 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:31,166 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 21), torch.int64', '20')
2023-10-31 06:07:31,166 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:31,166 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:31,166 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:31,170 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:31,171 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 21), torch.int64', '20')
2023-10-31 06:07:31,171 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:31,172 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 21), torch.int64', '20'), {})
2023-10-31 06:07:31,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,173 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,174 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,175 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:31,176 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:31,176 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,177 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,177 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:31,177 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:31,181 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:31,185 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,185 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,187 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,192 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,196 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,200 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,203 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,203 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:31,203 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,203 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,203 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:31,205 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:31,209 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:31,213 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,213 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,215 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,224 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,227 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,230 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,230 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:31,230 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,231 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,231 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:31,232 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:31,236 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:31,240 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,240 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,242 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,247 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,251 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,257 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,257 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:31,257 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,258 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,258 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:31,259 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:31,263 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:31,267 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,267 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,269 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,274 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,278 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,282 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,284 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,284 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:31,284 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,284 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,284 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:31,286 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:31,290 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:31,294 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,294 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,296 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,301 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,305 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,310 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,312 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,312 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:31,313 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,313 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,313 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:31,314 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:31,318 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:31,322 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,323 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,325 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,329 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,333 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,342 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,344 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,344 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:31,344 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,345 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,345 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:31,346 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:31,350 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:31,354 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,354 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,356 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,375 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,379 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,383 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,385 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,385 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:31,385 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,386 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,386 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:31,387 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:31,391 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:31,395 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,395 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,397 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,402 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,406 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,410 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,412 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,413 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:31,413 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,413 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,413 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:31,414 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:31,419 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:31,423 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,423 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,425 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,430 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,443 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,443 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:31,443 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,443 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,443 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:31,445 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:31,449 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:31,453 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,453 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,455 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,460 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,464 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,467 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,470 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,470 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:31,471 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,471 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,471 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:31,472 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:31,477 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:31,481 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,481 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,483 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,491 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,495 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,498 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,498 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:31,498 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,498 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,498 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:31,500 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:31,500 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:31,504 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,505 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,506 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 06:07:31,522 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 06:07:31,522 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:31,522 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,523 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:31,523 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:31,526 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:31,527 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:31,528 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,529 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:31,529 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:31,531 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,532 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,533 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,534 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:31,534 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:31,534 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,535 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:31,535 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:31,535 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:31,536 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:31,537 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,537 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:31,538 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:31,555 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:31,565 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:31,575 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:31,587 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:31,589 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:31,589 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:31,589 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:31,590 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:31,593 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:31,594 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:31,594 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:31,595 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:31,595 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:31,596 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,597 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,597 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,598 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:31,598 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:31,598 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 22), torch.int64', '21')
2023-10-31 06:07:31,599 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:31,599 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:31,599 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:31,603 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:31,604 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 22), torch.int64', '21')
2023-10-31 06:07:31,604 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:31,605 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 22), torch.int64', '21'), {})
2023-10-31 06:07:31,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,606 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,607 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:31,607 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:31,609 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:31,609 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,609 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,610 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:31,610 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:31,614 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:31,618 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,618 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,620 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,625 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,629 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,636 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,638 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,638 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:31,639 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,639 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,639 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:31,641 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:31,644 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:31,648 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,649 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,651 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,655 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,659 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,663 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,666 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,666 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:31,666 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,666 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,666 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:31,668 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:31,672 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:31,675 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,676 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,677 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,682 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,698 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,728 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,729 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:31,729 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,729 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,729 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:31,731 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:31,735 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:31,739 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,739 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,741 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,748 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,758 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,763 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,766 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,766 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:31,766 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,767 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,767 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:31,769 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:31,776 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:31,780 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,781 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,783 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,787 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,795 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,797 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,798 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:31,798 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,798 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,798 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:31,800 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:31,805 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:31,809 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,809 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,811 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,819 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,823 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,825 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,826 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:31,826 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,826 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,826 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:31,827 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:31,831 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:31,835 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,835 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,837 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,870 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,873 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,873 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:31,873 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,873 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,874 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:31,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:31,879 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:31,883 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,883 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,885 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,890 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,893 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,899 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,901 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,901 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:31,901 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,902 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,902 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:31,903 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:31,907 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:31,911 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,911 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,913 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,918 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,922 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,926 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,928 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,928 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:31,928 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,928 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,929 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:31,930 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:31,934 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:31,938 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,938 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,940 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,948 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,952 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,954 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,954 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:31,954 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,955 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,955 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:31,956 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:31,960 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:31,964 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,964 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,966 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,972 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,980 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,982 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:31,982 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:31,982 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:31,982 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,983 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:31,984 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:31,985 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:31,988 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:31,989 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:31,990 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:31,995 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:31,998 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:32,002 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 06:07:32,005 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 06:07:32,005 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:32,005 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,005 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,005 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:32,006 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:32,007 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:32,008 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,008 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,008 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:32,009 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,010 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,011 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,011 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:32,012 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,012 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,012 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:32,012 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:32,012 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:32,013 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,013 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,014 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:32,025 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,032 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,047 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:32,048 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:32,048 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:32,048 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,048 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:32,052 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:32,052 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:32,053 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:32,053 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,054 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:32,054 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,055 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,056 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,056 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,057 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:32,057 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 23), torch.int64', '22')
2023-10-31 06:07:32,057 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,057 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:32,057 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:32,061 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:32,062 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 23), torch.int64', '22')
2023-10-31 06:07:32,062 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,063 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 23), torch.int64', '22'), {})
2023-10-31 06:07:32,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,064 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,065 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,065 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,067 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:32,067 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,067 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,067 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:32,067 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:32,071 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:32,075 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,075 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,077 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,095 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,101 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,107 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,110 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,110 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:32,110 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,111 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,111 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:32,112 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:32,117 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:32,121 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,121 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,123 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,128 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,132 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,139 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,139 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:32,139 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,140 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,140 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:32,141 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:32,145 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:32,149 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,150 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,152 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,156 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,160 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,166 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,167 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:32,167 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,167 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,167 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:32,169 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:32,173 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:32,177 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,177 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,179 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,184 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,188 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,192 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,195 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,195 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:32,195 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,195 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,195 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:32,197 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:32,201 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:32,205 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,206 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,208 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,212 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,217 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,223 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,224 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:32,224 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,224 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,224 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:32,226 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:32,230 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:32,234 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,235 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,237 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,241 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,246 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,250 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,252 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,252 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:32,253 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,253 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,253 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:32,254 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:32,259 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:32,264 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,264 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,266 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,272 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,277 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,281 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,284 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,284 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:32,284 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,284 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,285 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:32,286 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:32,291 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:32,295 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,295 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,297 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,302 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,312 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,316 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,319 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,319 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:32,319 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,319 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,320 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:32,321 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:32,325 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:32,329 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,330 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,332 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,338 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,345 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,372 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,373 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:32,373 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,373 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,374 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:32,376 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:32,380 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:32,385 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,385 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,387 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,391 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,396 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,400 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,402 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,403 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:32,403 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,403 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,403 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:32,405 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:32,410 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:32,414 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,415 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,417 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,422 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,432 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,434 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,435 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:32,435 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,435 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,435 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:32,437 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:32,438 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:32,442 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,442 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,444 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,454 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,458 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,462 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 06:07:32,464 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 06:07:32,465 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:32,465 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,465 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,465 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:32,467 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:32,467 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:32,468 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,468 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,468 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:32,469 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,470 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,471 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,471 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,472 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:32,472 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,472 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,472 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:32,472 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:32,473 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:32,473 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,473 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,474 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:32,482 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,489 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,496 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,504 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:32,505 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:32,505 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:32,505 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,505 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:32,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:32,509 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:32,510 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:32,510 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,511 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:32,511 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,513 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,513 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,514 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:32,514 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 24), torch.int64', '23')
2023-10-31 06:07:32,514 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,514 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:32,514 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:32,518 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:32,519 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 24), torch.int64', '23')
2023-10-31 06:07:32,519 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,520 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 24), torch.int64', '23'), {})
2023-10-31 06:07:32,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,522 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,523 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,524 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:32,525 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,525 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,525 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:32,525 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:32,530 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:32,533 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,533 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,535 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,540 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,544 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,548 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,551 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,551 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:32,551 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,551 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,551 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:32,553 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:32,557 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:32,562 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,562 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,564 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,568 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,572 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,576 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,594 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,595 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:32,595 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,595 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,596 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:32,597 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:32,601 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:32,605 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,605 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,607 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,616 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,620 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,622 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,623 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:32,623 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,623 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,623 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:32,625 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:32,628 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:32,632 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,633 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,634 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,639 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,643 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,647 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,649 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,649 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:32,650 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,650 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,650 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:32,652 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:32,657 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:32,660 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,660 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,662 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,667 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,671 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,675 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,677 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,678 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:32,678 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,678 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,678 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:32,680 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:32,684 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:32,688 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,688 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,690 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,694 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,698 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,705 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,705 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:32,705 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,705 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,705 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:32,706 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:32,711 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:32,715 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,715 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,717 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,722 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,730 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,733 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,736 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,737 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:32,737 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,737 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,737 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:32,739 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:32,742 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:32,746 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,746 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,748 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,753 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,756 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,760 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,762 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,763 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:32,763 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,763 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,763 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:32,764 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:32,768 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:32,772 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,772 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,774 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,779 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,782 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,786 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,788 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,788 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:32,789 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,789 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,789 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:32,790 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:32,794 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:32,797 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,798 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,799 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,804 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,807 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,811 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,813 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,813 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:32,813 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,814 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,814 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:32,815 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:32,819 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:32,822 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,822 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,824 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,829 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,833 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,849 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,864 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,864 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:32,864 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,865 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,865 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:32,867 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:32,868 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:32,872 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,873 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,875 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,880 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,884 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,890 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 06:07:32,892 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 06:07:32,892 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:32,892 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,893 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,893 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:32,894 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:32,894 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:32,895 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,895 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,896 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:32,897 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,899 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,899 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:32,899 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,899 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,899 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:32,900 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:32,900 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:32,901 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,901 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,901 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:32,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,919 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,926 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:32,945 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:32,947 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:32,947 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:32,947 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,947 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:32,951 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:32,952 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:32,953 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:32,953 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,953 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:32,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,955 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,955 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,956 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,956 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:32,956 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 25), torch.int64', '24')
2023-10-31 06:07:32,957 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:32,957 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:32,957 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:32,961 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:32,962 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 25), torch.int64', '24')
2023-10-31 06:07:32,962 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:32,963 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 25), torch.int64', '24'), {})
2023-10-31 06:07:32,963 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,964 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,965 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:32,966 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:32,967 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:32,967 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,967 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,968 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:32,968 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:32,972 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:32,976 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:32,976 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,978 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:32,983 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:32,987 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:32,991 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:32,994 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:32,994 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:32,994 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:32,994 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:32,995 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:32,996 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:33,001 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:33,004 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,005 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,006 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,011 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,016 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,019 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,023 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,023 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:33,023 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,024 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,024 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:33,025 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:33,029 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:33,033 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,033 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,035 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,040 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,048 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,051 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,051 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:33,051 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,052 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,052 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:33,054 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:33,058 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:33,062 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,062 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,064 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,069 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,077 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,080 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,080 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:33,080 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,080 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,080 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:33,082 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:33,086 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:33,090 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,090 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,092 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,097 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,101 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,105 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,108 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,109 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:33,109 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,109 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,109 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:33,111 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:33,117 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:33,123 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,123 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,126 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,134 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,142 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,168 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,172 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,172 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:33,172 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,172 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,173 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:33,174 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:33,179 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:33,183 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,183 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,185 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,190 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,195 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,202 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,202 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:33,202 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,203 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,203 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:33,205 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:33,208 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:33,213 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,213 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,215 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,220 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,224 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,228 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,230 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,230 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:33,231 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,231 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,231 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:33,232 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:33,237 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:33,242 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,242 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,244 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,250 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,258 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,261 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,261 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:33,261 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,262 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,262 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:33,263 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:33,268 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:33,272 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,272 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,275 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,284 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,288 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,291 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,291 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:33,291 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,291 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,291 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:33,293 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:33,297 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:33,301 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,301 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,303 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,308 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,317 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,320 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,320 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:33,320 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,320 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,320 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:33,323 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:33,323 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:33,328 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,328 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,330 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,334 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,338 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,372 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 06:07:33,375 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 06:07:33,376 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:33,376 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,376 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:33,376 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:33,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:33,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:33,379 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,379 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:33,380 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:33,381 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,381 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,382 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,383 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:33,383 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:33,383 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,383 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:33,383 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:33,383 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:33,384 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:33,384 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,385 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:33,385 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:33,393 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:33,401 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:33,408 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:33,429 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:33,433 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:33,433 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:33,434 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:33,434 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:33,437 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:33,438 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:33,439 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:33,439 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:33,439 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:33,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,441 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,442 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,443 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:33,443 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:33,444 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 26), torch.int64', '25')
2023-10-31 06:07:33,444 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:33,444 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:33,444 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:33,450 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:33,451 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 26), torch.int64', '25')
2023-10-31 06:07:33,451 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:33,452 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 26), torch.int64', '25'), {})
2023-10-31 06:07:33,452 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,453 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,454 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,455 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:33,457 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:33,457 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,457 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,457 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:33,458 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:33,462 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:33,467 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,467 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,469 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,475 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,480 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,484 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,486 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,487 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:33,487 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,487 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,487 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:33,489 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:33,493 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:33,498 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,498 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,500 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,516 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,519 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,519 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:33,520 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,520 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,520 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:33,522 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:33,526 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:33,530 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,530 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,532 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,537 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,541 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,548 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,549 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:33,549 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,549 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,549 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:33,551 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:33,555 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:33,560 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,560 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,562 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,567 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,571 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,575 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,580 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,580 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:33,581 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,581 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,581 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:33,583 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:33,588 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:33,592 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,592 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,595 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,600 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,604 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,628 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,632 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,632 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:33,632 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,632 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,632 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:33,634 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:33,639 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:33,643 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,644 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,646 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,651 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,661 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,664 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,664 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:33,664 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,665 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,665 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:33,666 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:33,671 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:33,675 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,675 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,678 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,683 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,688 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,693 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,695 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,696 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:33,696 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,696 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,696 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:33,698 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:33,702 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:33,706 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,707 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,708 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,714 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,718 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,723 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,726 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,726 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:33,727 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,727 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,727 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:33,728 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:33,733 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:33,737 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,738 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,739 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,745 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,749 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,755 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,758 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,758 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:33,759 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,759 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,759 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:33,761 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:33,765 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:33,769 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,769 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,771 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,777 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,786 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,788 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,789 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:33,789 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,789 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,789 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:33,791 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:33,796 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:33,801 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,801 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,803 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,818 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,824 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,830 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,833 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,833 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:33,833 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,834 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,834 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:33,836 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:33,836 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:33,841 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,841 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,843 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,853 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,858 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,864 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 06:07:33,872 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 06:07:33,872 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:33,872 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,872 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:33,873 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:33,874 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:33,875 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:33,876 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,876 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:33,876 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:33,877 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,879 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,880 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,880 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:33,880 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:33,880 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,881 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:33,881 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:33,881 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:33,882 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:33,882 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,882 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:33,883 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:33,896 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:33,904 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:33,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:33,920 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:33,922 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:33,922 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:33,922 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:33,923 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:33,926 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:33,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:33,927 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:33,928 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:33,928 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:33,929 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,931 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,931 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:33,932 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:33,932 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 27), torch.int64', '26')
2023-10-31 06:07:33,932 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:33,932 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:33,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:33,937 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:33,937 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 27), torch.int64', '26')
2023-10-31 06:07:33,937 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:33,938 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 27), torch.int64', '26'), {})
2023-10-31 06:07:33,939 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,939 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,940 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:33,941 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:33,943 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:33,943 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,943 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,943 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:33,944 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:33,948 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:33,952 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,952 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,954 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,959 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:33,964 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:33,970 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:33,973 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:33,973 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:33,973 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:33,974 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,974 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:33,975 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:33,980 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:33,984 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:33,984 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:33,986 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:33,992 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:33,996 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,001 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,004 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,004 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:34,004 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,005 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,005 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:34,007 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:34,011 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:34,016 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,016 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,018 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,023 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,027 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,034 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,034 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:34,034 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,034 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,034 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:34,036 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:34,040 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:34,045 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,045 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,048 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,052 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,056 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,061 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,064 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,064 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:34,064 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,064 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,064 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:34,066 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:34,071 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:34,075 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,075 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,077 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,082 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,086 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,092 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,095 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,095 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:34,095 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,095 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,096 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:34,097 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:34,101 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:34,105 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,106 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,108 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,122 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,126 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,132 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,134 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,134 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:34,135 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,135 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,135 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:34,136 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:34,141 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:34,146 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,146 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,148 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,154 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,162 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,165 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,165 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:34,165 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,165 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,165 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:34,167 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:34,171 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:34,174 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,174 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,176 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,181 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,185 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,191 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,192 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:34,192 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,192 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,192 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:34,193 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:34,197 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:34,201 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,202 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,203 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,208 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,212 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,216 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,219 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,219 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:34,219 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,219 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,219 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:34,221 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:34,225 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:34,229 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,229 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,231 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,236 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,240 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,244 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,246 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,247 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:34,247 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,247 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,247 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:34,248 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:34,253 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:34,257 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,258 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,259 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,264 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,268 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,272 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,274 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,274 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:34,274 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,274 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,275 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:34,276 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:34,277 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:34,281 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,281 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,283 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,288 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,292 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,300 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 06:07:34,318 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 06:07:34,318 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:34,318 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,319 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:34,319 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:34,320 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:34,321 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:34,322 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,322 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:34,323 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:34,323 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,324 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,325 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,325 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:34,325 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:34,326 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,326 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:34,326 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:34,326 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:34,327 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:34,327 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,327 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:34,328 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:34,336 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:34,343 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:34,353 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:34,360 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:34,362 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:34,362 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:34,362 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:34,362 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:34,366 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:34,366 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:34,367 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:34,367 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:34,367 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:34,368 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,368 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,370 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:34,370 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:34,370 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 28), torch.int64', '27')
2023-10-31 06:07:34,370 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:34,371 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:34,371 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:34,375 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:34,375 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 28), torch.int64', '27')
2023-10-31 06:07:34,376 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:34,376 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 28), torch.int64', '27'), {})
2023-10-31 06:07:34,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,377 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,378 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,379 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:34,380 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:34,381 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,381 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,381 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:34,381 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:34,385 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:34,389 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,389 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,391 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,397 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,401 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,406 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,409 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,409 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:34,409 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,409 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,409 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:34,411 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:34,415 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:34,419 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,419 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,421 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,431 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,437 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,437 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:34,438 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,438 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,438 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:34,440 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:34,444 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:34,448 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,448 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,450 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,455 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,459 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,463 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,467 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,467 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:34,467 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,467 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,467 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:34,469 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:34,473 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:34,477 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,477 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,479 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,484 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,492 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,497 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,497 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:34,497 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,497 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,497 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:34,499 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:34,503 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:34,507 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,508 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,510 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,515 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,519 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,523 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,526 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,526 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:34,526 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,526 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,526 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:34,528 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:34,532 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:34,536 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,536 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,538 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,542 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,546 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,550 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,552 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,553 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:34,553 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,553 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,553 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:34,554 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:34,559 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:34,563 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,563 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,565 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,570 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,574 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,577 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,580 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,580 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:34,580 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,580 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,581 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:34,582 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:34,586 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:34,590 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,591 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,592 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,597 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,601 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,620 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,620 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:34,621 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,621 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,621 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:34,622 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:34,627 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:34,631 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,631 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,633 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,643 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,647 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,650 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,650 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:34,650 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,650 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,651 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:34,652 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:34,656 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:34,661 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,661 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,663 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,668 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,676 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,679 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,679 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:34,679 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,680 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,680 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:34,681 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:34,685 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:34,689 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,690 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,692 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,697 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,700 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,705 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,707 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,707 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:34,708 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,708 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,708 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:34,709 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:34,710 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:34,714 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,715 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,717 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,722 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,726 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,732 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 06:07:34,735 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 06:07:34,736 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:34,736 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,736 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:34,736 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:34,738 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:34,738 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:34,739 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,739 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:34,740 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:34,740 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,741 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,742 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:34,743 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:34,743 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,743 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:34,743 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:34,743 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:34,744 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:34,744 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,744 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:34,745 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:34,753 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:34,760 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:34,767 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:34,775 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:34,776 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:34,776 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:34,777 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:34,777 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:34,780 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:34,781 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:34,781 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:34,782 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:34,782 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:34,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,784 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,784 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,785 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:34,785 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:34,785 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 29), torch.int64', '28')
2023-10-31 06:07:34,786 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:34,786 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:34,786 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:34,790 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:34,790 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 29), torch.int64', '28')
2023-10-31 06:07:34,791 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:34,791 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 29), torch.int64', '28'), {})
2023-10-31 06:07:34,792 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:34,794 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:34,795 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:34,796 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,796 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,796 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:34,796 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:34,800 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:34,804 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,804 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,806 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,811 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,820 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,822 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:34,822 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:34,822 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,823 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,823 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:34,824 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:34,828 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:34,832 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,833 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,835 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,840 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,845 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,852 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,854 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:34,854 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:34,855 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,855 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,855 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:34,857 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:34,861 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:34,865 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,865 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,867 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,893 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,898 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,902 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,905 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:34,905 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:34,906 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,906 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,906 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:34,908 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:34,912 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:34,916 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,916 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,918 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,922 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,927 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,931 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,933 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:34,933 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:34,933 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,934 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,934 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:34,935 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:34,939 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:34,943 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,943 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,945 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,950 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,954 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,958 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,961 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:34,961 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:34,961 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,961 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,962 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:34,963 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:34,967 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:34,971 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,971 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,973 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:34,978 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,982 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:34,988 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:34,989 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:34,989 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:34,989 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:34,989 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:34,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:34,995 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:34,998 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:34,999 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,001 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,005 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,010 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,014 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,017 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:35,018 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:35,018 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,018 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,018 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:35,020 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:35,024 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:35,028 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,028 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,030 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,040 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,044 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,046 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:35,046 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:35,047 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,047 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,047 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:35,048 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:35,053 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:35,057 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,057 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,059 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,064 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,068 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,072 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,075 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:35,075 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:35,075 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,075 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,075 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:35,077 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:35,081 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:35,085 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,085 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,087 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,096 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,116 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,119 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:35,119 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:35,119 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,119 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,119 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:35,121 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:35,125 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:35,129 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,129 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,131 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,136 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,140 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,144 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,147 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:35,147 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:35,147 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,147 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,147 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:35,149 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:35,149 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:35,153 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,154 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,156 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,164 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,168 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,172 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 06:07:35,175 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 06:07:35,175 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:35,175 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,175 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:35,176 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:35,177 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:35,177 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:35,178 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,178 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:35,179 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:35,179 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,180 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,181 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,181 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:35,182 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:35,182 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,182 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:35,182 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:35,182 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:35,183 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:35,183 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,183 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:35,184 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:35,192 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:35,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:35,207 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:35,214 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:35,215 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:35,216 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:35,216 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:35,216 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:35,219 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:35,220 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:35,220 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:35,220 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:35,221 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:35,222 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,222 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,223 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,224 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:35,224 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:35,224 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 30), torch.int64', '29')
2023-10-31 06:07:35,224 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:35,224 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:35,225 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:35,229 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:35,229 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 30), torch.int64', '29')
2023-10-31 06:07:35,230 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:35,230 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 30), torch.int64', '29'), {})
2023-10-31 06:07:35,231 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,232 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,232 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,233 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:35,235 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:35,235 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,235 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,235 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:35,235 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:35,239 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:35,243 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,243 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,245 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,251 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,255 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,260 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,262 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,262 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:35,262 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,263 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,263 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:35,264 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:35,269 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:35,273 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,273 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,275 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,280 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,285 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,289 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,291 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,292 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:35,292 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,292 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,292 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:35,294 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:35,298 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:35,302 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,302 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,304 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,309 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,313 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,317 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,322 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,322 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:35,322 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,322 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,322 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:35,324 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:35,328 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:35,332 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,332 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,334 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,340 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,344 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,385 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,396 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,396 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:35,397 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,397 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,397 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:35,398 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:35,403 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:35,406 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,407 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,409 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,413 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,417 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,422 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,425 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,425 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:35,425 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,425 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,426 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:35,427 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:35,431 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:35,435 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,436 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,438 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,443 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,452 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,454 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,454 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:35,455 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,455 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,455 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:35,456 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:35,460 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:35,464 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,464 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,466 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,472 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,476 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,481 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,483 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,483 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:35,484 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,484 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,484 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:35,485 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:35,489 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:35,493 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,494 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,496 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,500 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,505 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,510 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,513 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,513 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:35,513 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,513 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,513 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:35,515 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:35,519 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:35,523 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,523 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,525 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,530 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,534 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,538 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,541 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,541 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:35,541 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,542 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,542 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:35,543 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:35,547 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:35,551 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,552 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,553 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,558 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,563 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,567 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,570 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,570 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:35,570 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,570 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,570 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:35,572 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:35,576 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:35,580 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,580 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,582 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,586 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,591 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,595 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,598 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,598 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:35,598 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,598 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,598 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:35,600 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:35,600 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:35,604 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,605 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,607 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,612 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,616 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,621 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 06:07:35,623 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 06:07:35,624 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:35,624 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,624 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:35,624 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:35,625 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:35,626 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:35,626 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,627 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:35,627 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:35,628 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,629 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,629 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,630 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:35,630 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:35,630 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,630 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:35,630 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:35,631 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:35,631 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:35,631 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,632 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:35,632 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:35,640 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:35,648 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:35,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:35,664 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:35,665 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:35,665 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:35,665 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:35,665 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:35,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:35,669 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:35,670 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:35,670 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:35,670 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:35,671 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,673 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:35,673 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:35,673 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 31), torch.int64', '30')
2023-10-31 06:07:35,673 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:35,674 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:35,674 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:35,678 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:35,678 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 31), torch.int64', '30')
2023-10-31 06:07:35,678 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:35,679 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 31), torch.int64', '30'), {})
2023-10-31 06:07:35,680 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,680 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,681 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:35,682 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:35,683 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:35,683 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,683 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,684 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:35,684 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:35,688 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:35,692 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,692 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,694 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,699 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,703 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,707 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,710 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,710 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:35,710 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,710 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,710 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:35,712 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:35,716 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:35,720 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,720 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,722 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,727 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,731 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,735 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,738 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,738 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:35,738 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,738 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,738 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:35,740 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:35,744 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:35,748 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,748 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,750 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,755 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,759 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,763 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,766 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,766 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:35,766 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,766 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,767 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:35,768 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:35,772 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:35,776 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,776 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,778 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,783 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,787 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,793 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,794 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:35,794 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,794 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,794 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:35,796 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:35,800 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:35,804 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,804 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,806 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,811 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,819 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,821 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,821 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:35,821 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,822 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,822 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:35,823 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:35,827 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:35,831 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,831 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,833 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,838 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,842 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,848 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,856 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,857 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:35,857 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,857 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,857 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:35,858 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:35,863 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:35,867 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,867 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,869 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,874 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,878 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,882 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,885 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,885 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:35,885 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,886 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,886 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:35,887 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:35,891 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:35,895 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,896 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,898 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,902 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,907 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,915 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,915 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:35,915 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,915 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,915 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:35,917 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:35,921 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:35,925 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,925 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,927 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,938 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,947 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,948 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:35,948 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,948 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,948 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:35,950 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:35,953 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:35,957 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,958 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,960 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,964 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,968 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,972 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,975 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:35,975 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:35,976 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:35,976 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,976 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:35,977 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:35,982 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:35,985 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:35,986 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:35,988 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:35,992 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:35,997 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:36,001 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:36,003 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:36,003 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:36,004 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,004 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,004 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:36,005 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:36,006 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:36,010 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,010 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,012 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:36,021 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:36,026 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 06:07:36,029 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 06:07:36,029 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:36,029 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,029 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:36,029 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:36,031 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:36,031 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:36,032 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,032 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:36,033 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:36,033 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,034 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,035 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,035 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:36,035 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:36,036 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,036 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:36,036 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:36,036 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:36,037 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:36,037 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,037 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:36,038 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:36,048 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:36,055 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:36,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:36,071 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:36,072 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:36,072 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:36,073 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:36,073 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:36,076 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:36,077 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:36,077 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:36,077 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:36,078 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:36,079 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,079 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,080 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,080 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:36,081 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:36,081 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 32), torch.int64', '31')
2023-10-31 06:07:36,081 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:36,081 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:36,081 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:36,086 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:36,086 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 32), torch.int64', '31')
2023-10-31 06:07:36,086 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:36,087 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 32), torch.int64', '31'), {})
2023-10-31 06:07:36,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,088 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,089 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,090 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:36,091 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:36,092 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,092 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,092 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:36,092 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:36,096 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:36,100 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,100 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,102 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,107 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,128 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,133 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,135 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,136 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:36,136 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,136 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,137 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:36,138 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:36,142 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:36,146 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,146 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,148 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,153 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,162 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,165 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,165 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:36,165 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,165 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,166 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:36,167 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:36,171 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:36,175 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,175 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,177 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,183 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,187 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,192 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,194 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,195 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:36,195 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,195 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,195 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:36,196 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:36,201 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:36,205 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,205 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,207 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,212 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,219 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,223 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,225 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,226 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:36,226 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,226 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,226 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:36,228 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:36,232 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:36,236 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,236 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,238 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,243 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,247 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,251 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,254 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,254 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:36,254 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,254 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,254 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:36,256 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:36,260 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:36,264 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,264 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,266 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,271 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,275 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,279 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,282 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,282 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:36,282 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,283 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,283 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:36,284 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:36,288 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:36,292 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,292 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,294 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,303 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,307 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,310 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,310 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:36,310 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,310 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,310 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:36,312 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:36,316 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:36,319 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,320 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,322 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,326 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,331 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,335 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,338 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,338 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:36,338 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,338 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,339 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:36,340 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:36,344 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:36,349 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,349 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,351 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,360 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,364 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,371 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,372 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:36,372 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,372 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,372 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:36,374 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:36,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:36,382 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,383 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,385 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,415 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,422 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,429 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,429 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:36,429 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,429 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,429 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:36,431 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:36,436 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:36,439 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,440 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,442 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,451 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,456 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,459 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,459 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:36,459 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,460 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,460 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:36,461 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:36,462 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:36,466 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,467 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,469 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,473 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,478 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,483 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 06:07:36,486 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 06:07:36,486 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:36,486 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,486 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:36,486 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:36,488 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:36,488 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:36,489 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,489 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:36,490 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:36,491 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,492 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,492 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,493 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:36,493 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:36,493 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,494 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:36,494 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:36,494 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:36,494 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:36,495 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,495 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:36,496 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:36,504 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:36,513 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:36,521 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:36,545 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:36,547 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:36,547 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:36,548 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:36,548 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:36,551 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:36,552 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:36,553 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:36,553 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:36,554 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:36,554 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,555 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,556 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,556 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:36,557 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:36,557 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 33), torch.int64', '32')
2023-10-31 06:07:36,557 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:36,558 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:36,558 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:36,562 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:36,564 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 33), torch.int64', '32')
2023-10-31 06:07:36,564 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:36,565 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 33), torch.int64', '32'), {})
2023-10-31 06:07:36,566 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,566 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,567 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:36,568 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:36,569 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:36,569 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,570 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,570 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:36,570 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:36,574 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:36,578 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,578 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,580 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,615 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,630 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,635 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,638 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,639 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:36,639 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,639 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,639 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:36,641 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:36,645 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:36,649 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,649 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,651 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,656 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,677 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,680 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,680 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:36,681 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,681 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,681 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:36,682 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:36,687 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:36,692 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,692 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,694 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,708 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,712 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,717 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,719 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,720 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:36,720 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,720 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,720 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:36,722 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:36,726 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:36,730 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,730 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,732 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,738 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,742 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,751 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,754 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,754 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:36,754 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,754 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,754 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:36,756 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:36,760 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:36,764 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,764 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,766 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,771 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,776 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,783 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,783 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:36,783 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,783 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,783 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:36,785 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:36,789 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:36,793 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,793 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,795 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,801 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,805 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,816 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,818 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,818 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:36,819 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,819 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,819 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:36,820 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:36,825 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:36,829 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,829 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,831 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,836 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,841 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,845 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,848 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,848 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:36,848 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,848 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,848 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:36,850 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:36,854 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:36,858 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,859 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,861 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,871 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,896 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,899 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,899 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:36,899 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,899 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,899 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:36,901 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:36,905 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:36,909 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,909 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,911 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,917 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,921 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,926 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,929 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,929 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:36,929 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,929 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,929 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:36,931 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:36,935 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:36,939 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,939 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,941 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,946 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,951 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,955 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,958 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,958 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:36,958 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,959 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,959 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:36,960 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:36,965 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:36,969 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,969 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,971 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:36,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,980 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,984 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:36,987 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:36,987 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:36,988 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:36,988 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,988 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:36,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:36,990 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:36,994 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:36,994 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:36,996 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,001 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:37,005 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:37,010 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 06:07:37,012 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 06:07:37,013 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:37,013 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,013 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,013 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:37,014 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:37,015 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:37,016 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,016 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,016 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:37,017 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,018 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,018 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,019 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:37,019 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:37,019 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,019 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,020 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:37,020 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:37,020 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:37,021 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,021 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,021 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:37,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:37,037 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:37,045 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:37,054 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:37,055 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:37,056 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:37,056 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,056 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:37,059 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:37,060 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:37,060 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:37,061 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,061 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:37,062 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,062 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,063 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,064 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:37,064 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:37,064 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 34), torch.int64', '33')
2023-10-31 06:07:37,064 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,064 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:37,065 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:37,069 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:37,069 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 34), torch.int64', '33')
2023-10-31 06:07:37,069 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,070 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 34), torch.int64', '33'), {})
2023-10-31 06:07:37,071 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,071 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,072 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,073 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:37,075 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:37,075 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,075 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,075 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:37,075 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:37,080 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:37,084 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,084 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,086 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,091 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,095 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,099 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,102 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,102 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:37,102 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,102 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,103 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:37,104 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:37,108 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:37,112 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,113 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,115 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,120 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,124 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,128 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,131 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,131 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:37,131 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,132 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,132 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:37,133 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:37,137 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:37,142 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,142 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,144 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,149 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,153 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,158 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,160 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,160 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:37,160 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,161 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,161 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:37,162 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:37,167 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:37,171 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,171 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,173 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,178 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,182 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,186 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,189 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,190 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:37,190 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,190 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,190 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:37,192 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:37,196 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:37,200 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,201 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,203 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,208 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,212 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,216 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,219 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,219 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:37,219 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,220 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,220 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:37,221 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:37,225 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:37,230 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,230 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,232 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,237 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,242 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,247 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,249 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,250 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:37,250 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,250 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,250 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:37,251 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:37,255 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:37,259 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,259 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,261 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,267 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,271 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,275 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,278 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,279 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:37,279 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,279 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,279 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:37,281 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:37,285 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:37,289 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,289 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,291 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,296 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,301 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,305 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,308 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,309 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:37,309 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,309 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,309 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:37,310 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:37,315 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:37,320 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,320 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,322 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,327 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,332 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,336 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,339 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,340 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:37,340 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,340 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,340 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:37,342 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:37,346 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:37,350 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,351 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,353 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,359 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,363 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,371 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,371 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:37,372 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,372 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,372 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:37,373 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:37,378 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:37,382 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,382 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,384 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,390 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,395 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,400 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,403 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,403 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:37,403 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,403 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,404 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:37,405 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:37,406 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:37,410 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,410 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,412 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,418 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,423 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,427 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 06:07:37,430 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 06:07:37,430 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:37,431 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,431 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,431 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:37,432 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:37,433 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:37,433 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,434 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,434 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:37,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,436 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,437 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:37,437 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:37,437 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,438 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,438 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:37,438 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:37,438 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:37,439 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,439 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,440 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:37,448 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:37,455 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:37,463 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:37,471 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:37,473 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:37,473 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:37,473 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,473 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:37,477 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:37,477 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:37,478 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:37,478 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,479 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:37,479 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,480 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,481 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,481 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:37,482 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:37,482 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 35), torch.int64', '34')
2023-10-31 06:07:37,482 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,482 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:37,482 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:37,486 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:37,487 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 35), torch.int64', '34')
2023-10-31 06:07:37,487 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,488 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 35), torch.int64', '34'), {})
2023-10-31 06:07:37,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,489 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,490 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,491 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:37,492 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:37,492 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,493 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,493 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:37,493 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:37,497 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:37,500 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,501 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,503 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,508 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,512 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,516 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,520 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,521 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:37,521 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,521 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,521 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:37,523 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:37,527 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:37,531 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,531 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,533 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,538 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,543 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,547 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,550 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,550 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:37,550 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,550 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,551 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:37,552 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:37,556 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:37,560 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,561 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,563 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,568 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,572 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,576 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,579 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,579 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:37,579 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,579 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,579 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:37,581 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:37,585 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:37,589 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,589 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,591 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,596 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,601 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,605 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,608 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,608 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:37,608 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,608 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,608 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:37,610 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:37,614 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:37,618 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,618 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,620 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,633 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,665 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,698 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,717 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,717 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:37,718 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,718 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,718 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:37,720 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:37,724 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:37,728 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,729 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,731 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,735 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,740 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,744 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,747 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,747 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:37,747 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,747 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,747 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:37,748 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:37,752 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:37,756 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,756 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,758 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,780 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,786 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,793 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,797 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,797 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:37,798 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,798 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,798 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:37,802 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:37,809 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:37,813 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,813 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,816 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,821 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,826 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,834 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,837 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,838 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:37,838 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,838 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,838 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:37,839 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:37,844 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:37,850 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,850 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,854 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,866 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,872 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,878 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,881 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,881 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:37,881 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,881 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,881 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:37,883 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:37,887 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:37,894 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,895 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,897 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,903 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,908 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,913 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,916 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,917 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:37,917 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,917 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,917 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:37,919 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:37,923 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:37,927 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,928 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,930 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,934 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,939 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,944 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,946 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,947 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:37,947 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,947 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,947 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:37,949 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:37,949 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:37,953 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,953 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:37,955 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:37,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,964 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,970 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 06:07:37,972 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 06:07:37,972 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:37,973 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,973 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,973 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:37,974 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:37,974 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:37,975 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,975 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,976 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:37,977 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,977 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,978 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:37,978 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:37,979 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:37,979 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:37,979 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:37,979 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:37,979 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:37,980 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:37,980 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:37,980 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:37,981 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:37,989 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:37,996 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:38,004 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:38,012 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:38,013 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:38,014 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:38,014 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,014 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:38,017 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:38,018 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:38,018 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:38,018 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,019 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:38,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,020 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,021 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,021 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:38,022 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:38,022 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 36), torch.int64', '35')
2023-10-31 06:07:38,022 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,022 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:38,022 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:38,027 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:38,028 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 36), torch.int64', '35')
2023-10-31 06:07:38,028 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,029 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 36), torch.int64', '35'), {})
2023-10-31 06:07:38,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,030 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,031 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,032 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:38,034 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:38,034 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,034 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,034 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:38,035 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:38,039 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:38,043 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,044 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,045 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,073 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,078 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,082 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,085 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,085 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:38,086 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,086 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,086 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:38,088 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:38,092 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:38,096 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,096 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,098 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,104 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,108 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,113 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,131 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,131 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:38,131 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,132 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,132 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:38,133 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:38,138 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:38,142 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,142 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,144 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,153 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,157 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,162 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,165 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,165 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:38,165 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,165 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,166 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:38,167 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:38,173 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:38,180 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,180 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,183 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,189 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,195 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,201 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,205 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,205 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:38,205 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,206 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,206 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:38,209 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:38,216 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:38,222 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,222 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,226 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,232 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,238 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,244 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,247 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,248 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:38,248 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,248 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,248 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:38,251 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:38,256 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:38,260 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,260 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,262 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,268 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,272 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,277 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,280 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,280 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:38,280 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,280 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,280 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:38,281 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:38,286 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:38,291 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,291 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,294 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,299 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,304 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,310 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,312 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,313 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:38,313 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,313 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,313 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:38,315 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:38,320 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:38,324 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,325 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,327 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,332 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,336 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,344 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,347 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,348 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:38,348 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,348 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,348 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:38,349 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:38,354 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:38,358 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,359 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,361 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,366 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,370 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,375 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,377 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,378 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:38,378 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,378 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,378 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:38,380 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:38,384 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:38,389 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,389 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,391 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,401 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,405 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,411 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,415 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,415 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:38,416 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,416 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,416 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:38,418 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:38,424 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:38,428 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,428 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,430 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,440 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,447 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,451 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,452 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:38,452 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,452 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,452 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:38,455 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:38,457 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:38,462 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,462 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,464 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,469 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,474 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,479 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 06:07:38,481 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 06:07:38,482 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:38,482 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,482 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,482 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:38,484 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:38,484 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:38,485 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,485 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,486 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:38,486 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,487 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,488 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,488 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:38,489 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:38,489 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,489 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,489 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:38,489 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:38,490 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:38,490 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,491 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,491 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:38,499 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:38,506 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:38,513 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:38,520 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:38,522 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:38,522 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:38,522 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,522 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:38,525 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:38,526 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:38,526 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:38,527 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,527 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:38,528 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,528 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,529 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,530 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:38,530 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:38,530 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 37), torch.int64', '36')
2023-10-31 06:07:38,531 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,531 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:38,531 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:38,535 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:38,536 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 37), torch.int64', '36')
2023-10-31 06:07:38,536 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,537 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 37), torch.int64', '36'), {})
2023-10-31 06:07:38,538 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,538 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,539 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,540 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:38,542 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:38,542 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,542 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,542 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:38,543 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:38,547 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:38,551 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,552 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,554 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,559 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,564 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,568 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,572 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,572 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:38,572 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,572 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,572 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:38,574 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:38,578 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:38,582 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,582 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,585 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,589 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,594 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,613 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,616 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,616 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:38,617 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,617 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,617 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:38,618 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:38,623 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:38,627 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,627 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,629 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,634 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,638 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,644 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,646 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,647 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:38,647 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,647 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,647 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:38,649 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:38,653 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:38,657 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,657 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,659 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,664 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,668 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,672 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,675 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,675 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:38,675 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,676 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,676 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:38,677 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:38,682 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:38,685 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,686 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,688 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,693 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,697 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,702 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,704 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,705 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:38,705 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,705 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,705 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:38,706 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:38,711 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:38,717 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,717 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,719 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,724 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,728 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,732 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,735 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,735 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:38,735 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,736 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,736 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:38,737 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:38,741 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:38,745 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,745 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,747 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,752 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,756 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,761 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,764 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,764 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:38,764 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,764 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,764 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:38,766 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:38,770 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:38,774 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,774 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,776 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,781 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,785 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,791 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,794 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,794 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:38,794 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,794 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,794 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:38,796 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:38,800 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:38,804 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,804 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,806 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,811 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,815 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,820 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,822 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,823 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:38,823 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,823 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,823 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:38,824 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:38,828 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:38,833 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,833 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,835 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,840 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,844 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,848 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,851 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,851 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:38,851 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,851 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,852 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:38,853 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:38,857 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:38,861 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,861 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,863 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,877 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,890 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,895 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,898 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,898 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:38,898 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,898 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,898 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:38,900 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:38,901 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:38,905 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,905 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,907 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:38,912 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,917 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,922 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 06:07:38,925 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 06:07:38,925 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:38,925 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,925 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,925 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:38,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:38,927 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:38,928 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,928 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,929 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:38,929 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,930 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,931 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,931 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:38,932 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:38,932 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,932 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,932 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:38,932 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:38,933 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:38,933 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,934 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,934 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:38,945 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:38,952 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:38,960 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:38,968 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:38,969 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 06:07:38,970 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 06:07:38,970 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,970 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 06:07:38,973 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:38,974 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:38,974 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 06:07:38,975 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,975 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 06:07:38,976 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,977 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,977 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,978 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:38,978 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 06:07:38,978 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 38), torch.int64', '37')
2023-10-31 06:07:38,979 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:38,979 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 06:07:38,979 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:38,983 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 06:07:38,983 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<Tensor>: (1, 38), torch.int64', '37')
2023-10-31 06:07:38,984 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:38,984 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 38), torch.int64', '37'), {})
2023-10-31 06:07:38,985 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,986 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,987 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:38,987 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:38,989 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 06:07:38,989 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:38,989 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:38,989 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 06:07:38,989 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:38,994 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 06:07:38,998 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:38,998 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,000 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,006 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,012 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,039 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,044 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,044 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 06:07:39,044 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,044 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,045 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 06:07:39,046 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:39,050 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 06:07:39,054 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,054 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,056 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,062 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,067 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,072 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,075 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,075 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 06:07:39,075 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,076 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,076 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 06:07:39,077 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:39,081 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 06:07:39,086 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,086 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,088 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,094 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,120 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,125 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,128 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,128 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 06:07:39,128 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,128 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,129 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 06:07:39,130 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:39,133 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 06:07:39,137 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,137 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,139 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,144 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,148 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,152 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,155 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,155 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 06:07:39,155 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,155 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,156 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 06:07:39,157 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:39,161 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 06:07:39,164 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,164 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,166 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,171 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,175 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,179 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,183 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,183 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 06:07:39,183 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,184 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,184 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 06:07:39,185 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:39,189 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 06:07:39,192 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,192 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,194 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,199 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,203 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,207 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,210 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,210 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 06:07:39,210 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,210 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,210 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 06:07:39,212 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:39,215 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 06:07:39,219 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,219 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,221 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,226 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,230 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,235 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,237 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,237 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 06:07:39,237 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,238 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,238 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 06:07:39,239 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:39,243 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 06:07:39,247 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,247 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,249 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,254 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,258 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,262 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,264 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,265 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 06:07:39,265 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,266 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,266 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 06:07:39,267 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:39,271 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 06:07:39,274 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,274 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,276 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,281 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,285 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,289 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,291 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,291 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 06:07:39,291 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,291 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,292 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 06:07:39,293 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:39,297 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 06:07:39,300 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,300 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,302 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,321 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,326 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,337 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,339 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,340 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 06:07:39,340 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,340 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,340 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 06:07:39,342 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:39,347 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 06:07:39,351 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,352 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,354 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,364 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,369 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,397 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,400 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,401 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 06:07:39,401 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,401 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,401 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 06:07:39,403 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:39,405 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 06:07:39,409 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,409 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 06:07:39,411 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 06:07:39,416 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,420 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,426 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 06:07:39,428 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 06:07:39,429 [wrapper.py:214 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 06:07:39,429 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,429 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:39,429 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 06:07:39,430 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:39,431 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 06:07:39,431 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,432 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:39,432 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:39,433 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:39,434 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:39,435 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 06:07:39,435 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 06:07:39,435 [wrapper.py:214 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 06:07:39,435 [wrapper.py:215 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 06:07:39,436 [wrapper.py:216 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 06:07:39,436 [model.py:505 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 06:07:39,436 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 06:07:39,436 [model.py:487 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 06:07:39,437 [wrapper.py:118 in _load_curr_layer] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 06:07:39,437 [wrapper.py:119 in _load_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 06:07:39,438 [wrapper.py:140 in _store_prev_batch] DEBUG - output of last layer batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 06:07:39,446 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:39,453 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:39,460 [wrapper.py:135 in _store_prev_batch] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 06:07:39,468 [wrapper.py:239 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 06:07:39,470 [test.py:45 in test_hf_gen] INFO - for i in range(10):                               
2023-10-31 06:07:39,470 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:07:39,470 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-31 06:07:39,470 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:07:39,470 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 06:07:39,470 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:07:39,470 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-31 06:07:39,470 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 06:07:39,480 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-31 06:07:39,480 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-31 06:07:39,481 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-31 06:07:39,481 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-31 06:07:39,481 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-31 06:07:39,481 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-31 06:07:39,481 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-31 06:07:39,481 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-31 06:07:39,481 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-31 06:07:39,482 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-31 06:07:39,482 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-31 06:07:39,482 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-31 06:07:39,482 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-31 06:07:39,482 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-31 06:07:39,482 [wrapper.py:88 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-31 06:07:39,482 [wrapper.py:88 in layer_reset] DEBUG - lm_head from flexgen to old.
