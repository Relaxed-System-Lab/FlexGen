2023-10-31 05:26:11,480 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpsjm1uhl7
2023-10-31 05:26:11,481 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpsjm1uhl7/_remote_module_non_scriptable.py
2023-10-31 05:26:11,606 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-31 05:26:11,662 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 05:26:13,240 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-31 05:26:13,580 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-31 05:26:13,581 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-31 05:26:13,581 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-31 05:26:13,581 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-31 05:26:14,402 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 05:26:14,489 [model.py:130 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-31 05:26:14,489 [model.py:69 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-31 05:26:14,524 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 05:26:14,611 [model.py:79 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-31 05:26:14,615 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-31 05:26:14,615 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-31 05:26:14,616 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-31 05:26:14,617 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-31 05:26:14,618 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-31 05:26:14,619 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-31 05:26:14,619 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-31 05:26:14,620 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-31 05:26:14,621 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-31 05:26:14,622 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-31 05:26:14,623 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-31 05:26:14,624 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-31 05:26:14,625 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-31 05:26:14,626 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-31 05:26:14,627 [model.py:261 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 05:26:14,627 [model.py:261 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-31 05:26:14,627 [model.py:267 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-31 05:26:14,629 [model.py:303 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-31 05:26:14,763 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 05:26:14,930 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-31 05:26:14,931 [model.py:393 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-31 05:26:14,931 [model.py:393 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-31 05:26:14,931 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-31 05:26:14,931 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-31 05:26:14,931 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-31 05:26:14,931 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-31 05:26:14,932 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-31 05:26:14,932 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-31 05:26:14,932 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-31 05:26:14,932 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-31 05:26:14,932 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-31 05:26:14,932 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-31 05:26:14,932 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-31 05:26:14,932 [model.py:393 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-31 05:26:14,933 [model.py:393 in to_test_forward] DEBUG - lm_head to test forward
2023-10-31 05:26:14,936 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:14,938 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-31 05:26:14,939 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:14,939 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-31 05:26:14,940 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:14,952 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-31 05:26:14,955 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:14,963 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-31 05:26:14,966 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:14,973 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-31 05:26:14,975 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:14,981 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-31 05:26:14,984 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:14,990 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-31 05:26:14,993 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:14,999 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-31 05:26:15,002 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:15,008 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-31 05:26:15,011 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:15,018 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-31 05:26:15,020 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:15,043 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-31 05:26:15,045 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:15,072 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-31 05:26:15,075 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:15,083 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-31 05:26:15,085 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:15,091 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-31 05:26:15,093 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:15,094 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-31 05:26:15,095 [model.py:348 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:15,104 [model.py:366 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-31 05:26:15,110 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-31 05:26:15,110 [model.py:401 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-31 05:26:15,110 [model.py:401 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-31 05:26:15,110 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-31 05:26:15,111 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-31 05:26:15,111 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-31 05:26:15,111 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-31 05:26:15,111 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-31 05:26:15,111 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-31 05:26:15,111 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-31 05:26:15,112 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-31 05:26:15,112 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-31 05:26:15,112 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-31 05:26:15,112 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-31 05:26:15,112 [model.py:401 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-31 05:26:15,113 [model.py:401 in reset_forward] DEBUG - lm_head from test to old.
2023-10-31 05:26:15,123 [model.py:518 in init_all_weights] DEBUG - init all weights...
2023-10-31 05:26:15,155 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-31 05:26:15,155 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-31 05:26:15,155 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-31 05:26:15,155 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-31 05:26:15,155 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-31 05:26:15,155 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-31 05:26:15,156 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-31 05:26:15,156 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-31 05:26:15,156 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-31 05:26:15,156 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-31 05:26:15,156 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-31 05:26:15,156 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-31 05:26:15,156 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-31 05:26:15,157 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-31 05:26:15,157 [wrapper.py:185 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-31 05:26:15,157 [wrapper.py:185 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-31 05:26:15,200 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 05:26:15,349 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:15,349 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:15,350 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:15,350 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:15,351 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64',)
2023-10-31 05:26:15,351 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:15,351 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64',)
2023-10-31 05:26:15,351 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:15,352 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64',), {})
2023-10-31 05:26:15,352 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,353 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,354 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,354 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 05:26:15,355 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:15,355 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:15,358 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:15,359 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:15,359 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 9), torch.int64', '0')
2023-10-31 05:26:15,359 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:15,359 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 9), torch.int64', '0')
2023-10-31 05:26:15,360 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:15,360 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9), torch.int64', '0'), {})
2023-10-31 05:26:15,361 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,361 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,362 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,363 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 05:26:15,364 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:15,364 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:15,368 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:15,371 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:15,372 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,372 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,372 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,372 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,373 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,380 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,384 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,388 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,391 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,391 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:15,394 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:15,399 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:15,405 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:15,405 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,406 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,406 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,406 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,408 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,428 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,433 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,439 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,442 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,442 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:15,444 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:15,447 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:15,451 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:15,451 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,451 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,452 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,452 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,453 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,459 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,464 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,470 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,473 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,473 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:15,475 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:15,478 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:15,482 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:15,482 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,482 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,483 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,483 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,484 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,489 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,493 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,498 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,502 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,502 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:15,504 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:15,508 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:15,511 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:15,511 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,512 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,512 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,512 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,513 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,524 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,527 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,531 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,534 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,535 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:15,536 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:15,540 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:15,544 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:15,544 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,544 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,544 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,544 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,545 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,553 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,558 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,562 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,565 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,565 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:15,567 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:15,570 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:15,574 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:15,574 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,575 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,575 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,575 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,576 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,582 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,586 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,590 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,592 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,593 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:15,594 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:15,598 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:15,601 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:15,601 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,602 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,602 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,602 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,603 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,618 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,622 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,627 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,630 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,630 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:15,631 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:15,635 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:15,639 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:15,639 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,639 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,639 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,639 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,640 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,645 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,648 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,652 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,655 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,655 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:15,656 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:15,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:15,664 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:15,664 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,664 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,664 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,665 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,666 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,670 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,674 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,677 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,681 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,681 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:15,682 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:15,686 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:15,690 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:15,693 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,694 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,694 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,694 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,695 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,700 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,703 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,707 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,709 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,710 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:15,711 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:15,712 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:15,716 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:15,716 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,716 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,716 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,717 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,717 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 9, 9), torch.float32', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,722 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,726 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,731 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 9, 768), torch.float32', ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'))
2023-10-31 05:26:15,734 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 9, 768), torch.float32', ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'))


2023-10-31 05:26:15,734 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:15,735 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:15,735 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:15,736 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:15,736 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,736 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:15,737 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,737 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:15,737 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 05:26:15,738 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,739 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,740 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 768), torch.float32
2023-10-31 05:26:15,740 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 9, 768), torch.float32


2023-10-31 05:26:15,740 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:15,741 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:15,741 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:15,742 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:15,742 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 9, 768), torch.float32',)
2023-10-31 05:26:15,742 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:15,742 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 9, 768), torch.float32',)
2023-10-31 05:26:15,742 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:15,743 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 9, 768), torch.float32',), {})
2023-10-31 05:26:15,758 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 05:26:15,771 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 05:26:15,781 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 9, 50272), torch.float32
2023-10-31 05:26:15,791 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 9, 50272), torch.float32


2023-10-31 05:26:15,793 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:15,796 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:15,797 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:15,797 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:15,798 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:15,798 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:15,798 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:15,798 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:15,799 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:15,800 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:15,800 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:15,801 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:15,802 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:15,802 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:15,802 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:15,806 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:15,807 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:15,807 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 10), torch.int64', '9')
2023-10-31 05:26:15,807 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:15,807 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 10), torch.int64', '9')
2023-10-31 05:26:15,808 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:15,808 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 10), torch.int64', '9'), {})
2023-10-31 05:26:15,809 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:15,810 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:15,810 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:15,811 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:15,812 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:15,813 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:15,817 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:15,820 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:15,820 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:15,821 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,821 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:15,821 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,823 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,827 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,831 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,839 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,843 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:15,843 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:15,845 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:15,849 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:15,853 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:15,853 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:15,853 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,854 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:15,854 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,856 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,860 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,864 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,868 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,870 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:15,870 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:15,872 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:15,876 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:15,879 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:15,880 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:15,880 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,881 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:15,881 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,883 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,887 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,890 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,894 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,896 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:15,897 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:15,898 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:15,902 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:15,906 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:15,906 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:15,906 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,906 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:15,907 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,908 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,913 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,917 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,920 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,922 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:15,922 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:15,924 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:15,928 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:15,932 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:15,932 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:15,932 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,933 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:15,933 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,934 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,939 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,942 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,946 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,948 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:15,948 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:15,950 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:15,954 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:15,958 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:15,958 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:15,958 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,958 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:15,958 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,960 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,964 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,973 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,977 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:15,980 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:15,980 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:15,981 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:15,985 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:15,989 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:15,989 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:15,989 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,989 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:15,990 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:15,991 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:15,997 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,001 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,004 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,006 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:16,007 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:16,008 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:16,012 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:16,016 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:16,016 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,016 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,016 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,017 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,018 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,022 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,026 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,030 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,032 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:16,032 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:16,034 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:16,038 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:16,042 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:16,042 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,042 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,042 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,043 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,044 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,051 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,055 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,059 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,061 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:16,062 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:16,063 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:16,068 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:16,071 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:16,071 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,071 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,072 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,072 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,074 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,079 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,083 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,086 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,088 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:16,088 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:16,089 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:16,093 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:16,097 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:16,097 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,097 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,098 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,098 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,099 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,128 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,136 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,140 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,144 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:16,144 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:16,146 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:16,147 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:16,151 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:16,151 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,151 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 9, 64), torch.float32', '<BlockTensor>: (4, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,152 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,152 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,154 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 10), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 9, 64), torch.float32', '<MixTensor>: (1, 12, 9, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,158 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,162 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,165 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'))
2023-10-31 05:26:16,168 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'))


2023-10-31 05:26:16,168 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:16,169 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:16,169 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:16,170 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:16,170 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,170 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:16,171 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,171 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:16,171 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:16,172 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,173 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,174 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,174 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:16,174 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:16,175 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:16,175 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:16,176 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:16,176 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,176 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:16,176 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,176 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:16,177 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:16,185 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:16,192 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:16,199 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:16,207 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:16,208 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:16,212 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:16,212 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:16,213 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:16,213 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:16,213 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:16,213 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:16,214 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:16,214 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:16,215 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,216 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,217 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,217 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:16,217 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:16,218 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:16,222 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:16,223 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:16,223 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 11), torch.int64', '10')
2023-10-31 05:26:16,223 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:16,223 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 11), torch.int64', '10')
2023-10-31 05:26:16,223 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:16,224 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 11), torch.int64', '10'), {})
2023-10-31 05:26:16,224 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,225 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,226 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,226 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:16,228 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:16,228 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:16,232 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:16,236 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:16,236 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,236 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,237 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,237 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,239 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,243 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,247 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,251 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,253 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,254 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:16,255 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:16,259 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:16,263 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:16,263 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,263 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,263 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,264 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,265 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,270 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,274 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,277 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,280 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,280 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:16,282 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:16,286 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:16,290 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:16,290 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,290 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,290 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,291 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,292 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,297 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,300 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,304 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,306 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,306 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:16,308 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:16,312 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:16,316 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:16,316 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,316 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,316 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,316 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,318 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,322 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,326 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,330 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,332 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,332 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:16,334 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:16,338 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:16,341 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:16,341 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,342 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,342 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,342 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,344 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,348 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,352 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,356 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,358 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,358 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:16,360 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:16,364 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:16,367 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:16,367 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,368 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,368 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,368 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,370 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,374 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,403 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,406 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,409 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,409 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:16,410 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:16,415 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:16,419 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:16,419 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,419 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,419 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,420 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,422 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,426 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,429 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,433 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,435 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,435 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:16,437 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:16,441 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:16,444 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:16,444 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,445 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,445 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,445 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,447 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,452 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,455 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,459 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,461 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,461 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:16,463 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:16,467 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:16,470 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:16,471 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,471 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,471 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,471 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,473 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,483 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,486 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,490 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,492 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,492 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:16,494 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:16,498 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:16,502 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:16,502 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,502 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,502 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,503 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,504 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,509 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,512 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,516 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,518 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,518 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:16,519 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:16,523 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:16,527 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:16,527 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,527 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,527 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,528 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,529 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,534 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,537 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,541 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,543 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,544 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:16,546 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:16,547 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:16,550 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:16,550 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,551 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 10, 64), torch.float32', '<BlockTensor>: (4, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,551 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,551 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,553 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 11), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 10, 64), torch.float32', '<MixTensor>: (1, 12, 10, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,562 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,566 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,572 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'))
2023-10-31 05:26:16,574 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'))


2023-10-31 05:26:16,575 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:16,576 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:16,577 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:16,577 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:16,577 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,577 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:16,578 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,578 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:16,578 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:16,579 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,580 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,581 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,581 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:16,581 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:16,582 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:16,582 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:16,582 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:16,583 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,583 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:16,583 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,583 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:16,584 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:16,599 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:16,613 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:16,620 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:16,627 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:16,628 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:16,633 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:16,633 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:16,634 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:16,634 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:16,634 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:16,635 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:16,635 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:16,636 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:16,637 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,637 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,638 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,639 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:16,639 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:16,639 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:16,643 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:16,644 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:16,644 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 12), torch.int64', '11')
2023-10-31 05:26:16,644 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:16,644 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 12), torch.int64', '11')
2023-10-31 05:26:16,644 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:16,645 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 12), torch.int64', '11'), {})
2023-10-31 05:26:16,646 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,646 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,647 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:16,647 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:16,649 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:16,649 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:16,653 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:16,657 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:16,657 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,657 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,658 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,658 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,660 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,664 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,668 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,671 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,673 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:16,674 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:16,675 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:16,679 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:16,683 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:16,683 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,684 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,684 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,684 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,686 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,691 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,694 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,698 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,700 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:16,700 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:16,702 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:16,706 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:16,709 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:16,709 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,710 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,710 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,710 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,712 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,717 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,722 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,726 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,729 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:16,729 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:16,731 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:16,735 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:16,739 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:16,739 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,739 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,739 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,740 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,741 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,746 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,750 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,755 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,757 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:16,757 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:16,759 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:16,763 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:16,767 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:16,767 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,767 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,768 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,768 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,770 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,775 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,779 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,784 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,786 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:16,787 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:16,789 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:16,793 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:16,797 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:16,797 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,797 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,797 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,798 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,800 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,816 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,819 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,823 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,826 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:16,826 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:16,827 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:16,831 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:16,835 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:16,835 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,835 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,835 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,835 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,837 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,847 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,854 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,858 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,860 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:16,861 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:16,862 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:16,866 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:16,870 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:16,870 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,870 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,871 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,871 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,873 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,899 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,903 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,907 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,909 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:16,909 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:16,911 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:16,915 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:16,918 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:16,919 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:16,919 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,919 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:16,919 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:16,921 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:16,927 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,959 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:16,976 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,006 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:17,007 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:17,009 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:17,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:17,016 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:17,017 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,017 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,017 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,017 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,019 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,024 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,028 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,031 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,034 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:17,034 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:17,035 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:17,041 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:17,045 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:17,045 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,045 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,045 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,045 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,047 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,051 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,055 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,059 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,061 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:17,061 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:17,063 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:17,063 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:17,067 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:17,068 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,068 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 11, 64), torch.float32', '<BlockTensor>: (4, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,068 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,068 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,070 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 12), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 11, 64), torch.float32', '<MixTensor>: (1, 12, 11, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,074 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,078 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,081 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'))
2023-10-31 05:26:17,083 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'))


2023-10-31 05:26:17,083 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:17,085 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:17,085 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:17,086 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:17,086 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,086 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,086 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,086 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,087 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:17,088 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,088 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,089 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,090 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:17,090 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:17,090 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:17,090 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:17,091 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:17,091 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,091 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,091 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,091 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,092 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:17,101 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:17,112 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:17,126 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:17,144 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:17,146 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:17,150 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:17,151 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:17,151 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:17,151 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:17,151 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,152 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:17,152 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,152 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:17,153 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,153 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,154 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,155 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:17,155 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:17,155 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:17,159 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:17,160 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:17,160 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 13), torch.int64', '12')
2023-10-31 05:26:17,160 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,160 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 13), torch.int64', '12')
2023-10-31 05:26:17,160 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,161 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 13), torch.int64', '12'), {})
2023-10-31 05:26:17,162 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,162 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,163 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,163 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:17,165 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:17,165 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:17,169 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:17,172 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:17,173 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,173 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,173 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,173 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,175 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,179 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,183 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,187 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,189 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,189 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:17,191 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:17,195 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:17,199 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:17,199 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,199 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,199 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,200 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,201 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,206 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,209 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,213 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,215 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,215 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:17,217 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:17,221 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:17,225 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:17,225 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,225 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,225 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,226 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,227 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,232 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,236 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,241 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,243 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,243 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:17,244 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:17,248 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:17,252 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:17,252 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,252 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,253 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,253 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,255 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,259 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,263 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,267 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,270 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,270 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:17,272 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:17,276 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:17,279 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:17,279 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,280 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,280 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,280 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,282 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,287 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,290 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,295 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,298 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,298 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:17,299 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:17,303 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:17,307 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:17,307 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,307 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,307 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,308 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,309 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,314 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,318 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,321 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,324 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,324 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:17,325 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:17,329 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:17,333 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:17,333 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,333 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,334 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,334 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,336 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,340 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,349 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,353 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,355 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,355 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:17,357 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:17,361 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:17,364 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:17,365 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,365 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,365 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,365 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,367 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,372 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,375 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,379 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,402 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,402 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:17,404 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:17,408 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:17,413 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:17,413 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,413 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,413 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,413 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,415 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,421 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,425 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,429 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,431 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,431 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:17,433 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:17,437 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:17,441 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:17,442 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,442 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,442 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,442 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,444 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,449 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,452 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,456 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,459 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,459 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:17,460 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:17,465 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:17,468 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:17,468 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,469 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,469 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,469 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,471 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,475 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,479 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,483 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,485 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,485 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:17,487 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:17,487 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:17,491 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:17,491 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,492 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 12, 64), torch.float32', '<BlockTensor>: (4, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,492 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,492 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,494 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 13), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 12, 64), torch.float32', '<MixTensor>: (1, 12, 12, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,498 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,502 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,506 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'))
2023-10-31 05:26:17,508 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'))


2023-10-31 05:26:17,508 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:17,510 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:17,510 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:17,511 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:17,511 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,511 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,511 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,512 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,512 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:17,513 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,514 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,515 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,515 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:17,515 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:17,516 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:17,516 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:17,516 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:17,517 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,517 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,517 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,517 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,518 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:17,526 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:17,533 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:17,540 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:17,547 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:17,548 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:17,552 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:17,552 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:17,553 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:17,553 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:17,553 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,553 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:17,553 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,554 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:17,555 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,555 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,556 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,557 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:17,557 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:17,557 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:17,561 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:17,562 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:17,562 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 14), torch.int64', '13')
2023-10-31 05:26:17,562 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,562 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 14), torch.int64', '13')
2023-10-31 05:26:17,562 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,563 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 14), torch.int64', '13'), {})
2023-10-31 05:26:17,564 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,564 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,565 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,566 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:17,568 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:17,568 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:17,572 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:17,576 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:17,576 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,576 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,577 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,577 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,579 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,584 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,604 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,608 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,611 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,611 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:17,613 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:17,617 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:17,621 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:17,621 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,621 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,622 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,622 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,624 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,629 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,632 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,636 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,638 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,639 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:17,640 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:17,645 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:17,649 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:17,649 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,649 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,650 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,650 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,652 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,656 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,660 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,664 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,667 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,667 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:17,669 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:17,673 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:17,677 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:17,677 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,678 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,678 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,678 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,680 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,685 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,688 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,693 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,696 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,697 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:17,699 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:17,704 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:17,708 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:17,708 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,708 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,709 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,709 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,711 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,716 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,727 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,732 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,736 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,737 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:17,739 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:17,745 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:17,751 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:17,751 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,751 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,752 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,752 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,755 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,761 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,766 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,771 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,774 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,774 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:17,776 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:17,780 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:17,784 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:17,784 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,785 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,785 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,785 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,787 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,792 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,796 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,799 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,802 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,803 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:17,804 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:17,808 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:17,812 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:17,812 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,813 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,813 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,813 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,815 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,820 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,824 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,828 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,830 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,831 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:17,832 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:17,836 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:17,840 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:17,840 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,840 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,841 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,841 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,843 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,853 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,884 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,890 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,893 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,893 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:17,895 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:17,899 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:17,903 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:17,903 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,903 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,904 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,904 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,906 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,910 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,914 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,918 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,920 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,921 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:17,922 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:17,926 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:17,930 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:17,930 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,930 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,931 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,931 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,933 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,937 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,941 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,945 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,947 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,947 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:17,949 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:17,950 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:17,953 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:17,954 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,954 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 13, 64), torch.float32', '<BlockTensor>: (4, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,954 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,954 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:17,956 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 14), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 13, 64), torch.float32', '<MixTensor>: (1, 12, 13, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:17,961 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,964 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,969 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'))
2023-10-31 05:26:17,971 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'))


2023-10-31 05:26:17,971 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:17,973 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:17,973 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:17,974 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:17,974 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,974 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,974 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,974 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,975 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:17,976 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,976 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,977 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:17,977 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:17,978 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:17,978 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:17,978 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:17,979 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:17,979 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:17,979 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:17,979 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:17,979 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:17,980 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:17,988 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:17,995 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:18,001 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:18,008 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:18,010 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:18,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:18,014 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:18,014 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:18,014 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:18,015 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,015 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:18,015 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,015 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:18,016 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,017 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,018 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,018 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:18,018 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:18,019 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:18,023 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:18,023 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:18,024 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 15), torch.int64', '14')
2023-10-31 05:26:18,024 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,024 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 15), torch.int64', '14')
2023-10-31 05:26:18,024 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,024 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 15), torch.int64', '14'), {})
2023-10-31 05:26:18,025 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,026 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,027 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,027 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:18,029 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:18,029 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:18,033 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:18,037 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:18,037 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,037 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,038 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,038 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,040 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,045 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,049 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,055 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,057 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,057 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:18,059 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:18,063 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:18,067 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:18,067 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,068 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,068 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,068 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,070 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,075 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,079 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,083 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,085 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,086 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:18,087 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:18,091 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:18,095 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:18,096 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,096 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,096 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,096 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,098 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,103 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,107 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,112 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,115 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,115 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:18,116 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:18,121 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:18,125 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:18,125 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,125 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,126 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,126 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,128 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,132 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,136 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,141 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,144 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,144 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:18,145 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:18,150 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:18,153 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:18,154 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,154 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,154 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,154 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,156 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,161 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,165 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,170 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,173 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,173 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:18,174 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:18,178 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:18,182 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:18,182 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,183 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,183 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,183 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,185 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,197 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,208 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,212 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,216 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,216 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:18,217 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:18,222 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:18,225 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:18,226 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,226 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,226 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,226 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,228 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,233 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,241 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,246 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,249 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,249 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:18,250 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:18,254 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:18,258 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:18,258 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,258 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,259 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,259 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,261 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,266 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,270 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,276 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,278 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,278 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:18,280 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:18,284 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:18,288 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:18,289 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,289 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,289 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,289 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,291 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,295 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,299 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,303 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,306 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,306 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:18,307 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:18,311 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:18,315 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:18,315 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,315 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,315 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,315 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,317 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,322 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,326 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,330 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,335 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,335 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:18,337 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:18,341 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:18,344 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:18,345 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,345 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,345 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,345 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,347 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,351 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,355 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,359 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,361 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,362 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:18,363 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:18,364 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:18,367 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:18,367 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,368 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 14, 64), torch.float32', '<BlockTensor>: (4, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,368 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,368 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,370 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 15), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 14, 64), torch.float32', '<MixTensor>: (1, 12, 14, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,374 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,379 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,383 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'))
2023-10-31 05:26:18,386 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'))


2023-10-31 05:26:18,386 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:18,387 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:18,387 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:18,388 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:18,388 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,388 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,389 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,389 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,389 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:18,390 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,391 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,391 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,392 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:18,392 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:18,392 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:18,393 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:18,393 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:18,393 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,393 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,394 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,394 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,394 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:18,405 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:18,413 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:18,422 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:18,430 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:18,431 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:18,434 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:18,435 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:18,435 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:18,435 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:18,435 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,436 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:18,436 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,436 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:18,437 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,438 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,438 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,439 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:18,439 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:18,440 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:18,443 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:18,444 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:18,444 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 16), torch.int64', '15')
2023-10-31 05:26:18,444 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,444 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 16), torch.int64', '15')
2023-10-31 05:26:18,444 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,445 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 16), torch.int64', '15'), {})
2023-10-31 05:26:18,446 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,446 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,447 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,448 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:18,449 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:18,449 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:18,453 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:18,457 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:18,457 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,457 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,457 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,457 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,459 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,488 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,497 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,502 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,505 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,505 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:18,507 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:18,512 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:18,516 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:18,516 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,517 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,517 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,517 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,519 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,524 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,529 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,533 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,536 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,536 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:18,538 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:18,542 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:18,546 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:18,547 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,547 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,547 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,547 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,549 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,554 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,558 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,564 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,567 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,567 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:18,569 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:18,573 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:18,578 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:18,578 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,578 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,578 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,579 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,581 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,586 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,590 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,594 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,597 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,597 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:18,599 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:18,604 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:18,608 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:18,609 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,609 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,609 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,609 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,612 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,617 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,621 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,627 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,629 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,629 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:18,631 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:18,636 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:18,641 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:18,641 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,641 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,641 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,642 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,644 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,649 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,653 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,657 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,660 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,660 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:18,662 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:18,667 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:18,671 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:18,671 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,671 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,672 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,672 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,674 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,679 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,684 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,688 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,718 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,718 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:18,720 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:18,724 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:18,728 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:18,728 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,728 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,728 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,728 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,730 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,735 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,740 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,749 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,751 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,751 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:18,753 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:18,757 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:18,761 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:18,761 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,761 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,761 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,761 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,763 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,767 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,771 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,775 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,777 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,777 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:18,778 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:18,782 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:18,786 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:18,786 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,786 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,787 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,787 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,788 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,793 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,796 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,800 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,802 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,803 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:18,804 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:18,808 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:18,812 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:18,812 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,812 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,813 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,813 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,815 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,819 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,822 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,826 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,828 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,828 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:18,830 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:18,830 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:18,834 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:18,834 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,834 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 15, 64), torch.float32', '<BlockTensor>: (4, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,835 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,835 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,836 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 16), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 15, 64), torch.float32', '<MixTensor>: (1, 12, 15, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,841 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,852 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,857 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'))
2023-10-31 05:26:18,859 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'))


2023-10-31 05:26:18,859 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:18,861 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:18,862 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:18,862 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:18,863 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,863 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,863 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,863 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,864 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:18,865 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,865 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,866 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,867 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:18,867 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:18,867 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:18,868 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:18,868 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:18,869 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,869 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,869 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,869 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,869 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:18,878 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:18,885 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:18,893 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:18,901 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:18,903 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:18,907 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:18,908 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:18,908 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:18,909 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:18,909 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,909 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:18,909 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,910 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:18,911 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,911 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,912 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,912 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:18,913 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:18,913 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:18,918 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:18,919 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:18,919 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 17), torch.int64', '16')
2023-10-31 05:26:18,919 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:18,919 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 17), torch.int64', '16')
2023-10-31 05:26:18,919 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:18,920 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 17), torch.int64', '16'), {})
2023-10-31 05:26:18,920 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,921 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,922 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:18,923 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:18,925 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:18,925 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:18,930 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:18,934 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:18,935 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,935 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,935 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,935 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,937 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,942 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:18,946 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:18,952 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:18,955 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:18,955 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:18,957 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:18,961 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:18,966 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:18,966 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:18,966 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,967 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:18,967 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:18,969 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:18,986 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:18,990 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:18,994 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:18,996 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:18,996 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:18,998 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:19,002 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:19,005 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:19,006 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,006 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,006 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,006 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,008 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,012 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,016 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,020 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,023 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,023 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:19,024 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:19,028 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:19,032 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:19,032 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,032 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,033 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,033 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,035 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,041 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,045 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,048 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,051 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,051 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:19,052 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:19,056 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:19,060 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:19,060 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,061 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,061 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,061 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,063 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,069 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,073 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,076 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,078 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,079 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:19,080 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:19,084 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:19,088 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:19,088 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,088 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,089 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,089 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,091 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,095 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,099 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,102 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,104 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,105 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:19,106 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:19,110 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:19,113 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:19,114 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,114 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,114 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,114 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,116 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,121 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,124 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,128 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,131 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,131 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:19,132 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:19,136 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:19,140 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:19,140 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,141 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,141 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,141 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,143 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,148 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,152 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,155 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,158 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,158 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:19,159 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:19,164 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:19,168 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:19,168 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,168 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,168 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,168 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,170 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,175 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,178 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,182 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,184 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,184 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:19,186 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:19,190 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:19,194 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:19,194 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,194 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,194 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,195 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,196 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,200 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,204 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,208 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,210 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,210 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:19,212 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:19,216 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:19,220 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:19,220 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,220 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,220 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,220 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,222 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,244 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,248 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,253 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,255 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,255 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:19,257 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:19,257 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:19,261 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:19,261 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,261 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 16, 64), torch.float32', '<BlockTensor>: (4, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,262 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,262 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,264 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 17), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 16, 64), torch.float32', '<MixTensor>: (1, 12, 16, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,268 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,273 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,280 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'))
2023-10-31 05:26:19,282 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'))


2023-10-31 05:26:19,282 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:19,283 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:19,284 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:19,284 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:19,285 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,285 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:19,285 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,285 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:19,286 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:19,287 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,287 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,288 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,289 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:19,289 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:19,289 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:19,289 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:19,290 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:19,290 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,290 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:19,290 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,291 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:19,291 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:19,299 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:19,307 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:19,314 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:19,323 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:19,324 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:19,327 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:19,328 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:19,328 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:19,328 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:19,329 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:19,329 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:19,329 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:19,330 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:19,330 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,331 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,332 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,332 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:19,333 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:19,333 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:19,337 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:19,337 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:19,338 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 18), torch.int64', '17')
2023-10-31 05:26:19,338 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:19,338 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 18), torch.int64', '17')
2023-10-31 05:26:19,338 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:19,339 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 18), torch.int64', '17'), {})
2023-10-31 05:26:19,339 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,340 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,341 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,341 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:19,343 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:19,343 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:19,347 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:19,351 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:19,351 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,351 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,351 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,351 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,353 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,358 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,362 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,367 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,369 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,369 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:19,371 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:19,375 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:19,379 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:19,379 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,379 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,379 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,380 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,381 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,386 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,389 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,393 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,396 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,396 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:19,398 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:19,401 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:19,405 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:19,405 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,405 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,406 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,406 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,407 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,412 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,418 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,421 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,424 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,424 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:19,426 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:19,430 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:19,434 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:19,434 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,434 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,434 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,434 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,436 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,441 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,444 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,448 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,451 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,451 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:19,452 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:19,457 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:19,460 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:19,461 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,461 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,461 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,461 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,463 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,467 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,471 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,475 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,477 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,477 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:19,479 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:19,483 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:19,486 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:19,487 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,487 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,487 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,487 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,489 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,505 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,522 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,526 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,528 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,529 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:19,530 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:19,534 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:19,540 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:19,540 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,541 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,541 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,541 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,544 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,551 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,558 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,565 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,569 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,569 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:19,571 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:19,575 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:19,578 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:19,579 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,579 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,579 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,579 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,581 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,586 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,590 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,595 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,598 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,598 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:19,599 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:19,603 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:19,607 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:19,607 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,607 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,608 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,608 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,610 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,621 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,626 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,632 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,635 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,635 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:19,637 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:19,641 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:19,645 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:19,645 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,645 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,646 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,646 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,648 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,653 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,658 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,663 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,667 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,667 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:19,669 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:19,674 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:19,678 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:19,679 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,679 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,679 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,680 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,682 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,700 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,704 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,709 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,712 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,712 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:19,713 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:19,714 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:19,718 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:19,719 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,719 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 17, 64), torch.float32', '<BlockTensor>: (4, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,719 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,719 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,721 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 18), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 17, 64), torch.float32', '<MixTensor>: (1, 12, 17, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,726 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,730 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,734 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'))
2023-10-31 05:26:19,736 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'))


2023-10-31 05:26:19,736 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:19,737 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:19,738 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:19,738 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:19,739 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,739 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:19,739 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,739 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:19,740 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:19,740 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,741 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,742 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,742 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:19,742 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:19,743 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:19,743 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:19,743 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:19,744 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,744 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:19,744 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,744 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:19,744 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:19,753 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:19,760 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:19,768 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:19,776 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:19,778 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:19,781 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:19,782 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:19,782 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:19,782 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:19,783 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:19,783 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:19,783 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:19,783 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:19,784 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,785 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,785 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,786 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:19,786 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:19,786 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:19,791 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:19,791 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:19,791 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 19), torch.int64', '18')
2023-10-31 05:26:19,791 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:19,792 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 19), torch.int64', '18')
2023-10-31 05:26:19,792 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:19,792 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 19), torch.int64', '18'), {})
2023-10-31 05:26:19,793 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,794 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,795 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:19,795 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:19,797 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:19,797 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:19,801 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:19,804 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:19,805 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,805 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,805 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,805 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,807 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,811 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,815 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,820 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,822 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:19,822 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:19,824 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:19,828 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:19,831 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:19,832 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,832 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,832 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,832 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,834 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,839 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,842 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,857 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,860 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:19,860 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:19,861 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:19,866 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:19,870 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:19,870 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,870 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,870 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,871 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,872 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,877 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,886 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,892 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,895 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:19,895 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:19,897 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:19,901 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:19,904 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:19,905 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,905 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,905 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,905 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,907 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,911 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,915 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,919 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,921 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:19,921 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:19,923 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:19,927 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:19,931 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:19,931 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,931 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,931 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,932 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,933 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,938 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,941 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,946 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,976 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:19,977 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:19,979 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:19,983 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:19,987 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:19,987 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:19,987 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,987 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:19,987 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:19,989 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:19,994 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:19,998 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,002 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,004 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:20,004 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:20,006 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:20,010 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:20,014 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:20,014 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,014 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,014 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,015 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,016 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,021 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,024 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,028 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,031 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:20,031 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:20,032 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:20,037 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:20,041 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:20,041 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,041 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,041 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,041 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,043 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,048 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,052 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,056 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,058 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:20,058 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:20,059 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:20,064 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:20,067 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:20,068 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,068 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,068 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,068 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,070 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,075 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,079 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,082 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,085 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:20,085 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:20,086 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:20,091 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:20,094 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:20,094 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,094 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,095 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,095 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,097 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,101 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,105 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,109 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,111 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:20,112 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:20,113 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:20,117 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:20,121 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:20,121 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,121 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,122 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,122 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,124 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,128 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,132 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,136 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,139 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:20,139 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:20,140 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:20,141 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:20,145 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:20,145 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,145 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 18, 64), torch.float32', '<BlockTensor>: (4, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,145 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,145 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,147 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 19), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 18, 64), torch.float32', '<MixTensor>: (1, 12, 18, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,152 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,156 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,160 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'))
2023-10-31 05:26:20,162 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'))


2023-10-31 05:26:20,163 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:20,164 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:20,164 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:20,165 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:20,165 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,165 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:20,165 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,165 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:20,166 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:20,167 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,168 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,168 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,169 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:20,169 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:20,169 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:20,170 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:20,170 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:20,170 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,170 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:20,171 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,171 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:20,171 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:20,179 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:20,187 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:20,194 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:20,202 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:20,203 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:20,207 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:20,207 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:20,208 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:20,208 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:20,208 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:20,208 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:20,209 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:20,209 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:20,210 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,210 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,211 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,212 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:20,212 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:20,212 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:20,216 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:20,217 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:20,217 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 20), torch.int64', '19')
2023-10-31 05:26:20,217 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:20,217 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 20), torch.int64', '19')
2023-10-31 05:26:20,217 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:20,218 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 20), torch.int64', '19'), {})
2023-10-31 05:26:20,219 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,219 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,220 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,221 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:20,222 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:20,222 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:20,227 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:20,231 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:20,231 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,231 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,231 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,231 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,233 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,238 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,243 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,247 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,250 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,250 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:20,251 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:20,255 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:20,259 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:20,259 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,259 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,260 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,260 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,262 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,286 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,293 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,297 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,299 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,300 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:20,301 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:20,305 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:20,309 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:20,309 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,309 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,310 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,310 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,312 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,316 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,320 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,324 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,326 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,326 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:20,328 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:20,332 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:20,336 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:20,336 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,337 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,337 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,337 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,339 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,343 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,347 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,351 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,353 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,353 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:20,355 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:20,359 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:20,363 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:20,363 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,364 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,364 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,364 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,366 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,370 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,374 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,378 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,380 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,380 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:20,382 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:20,386 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:20,390 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:20,390 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,390 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,390 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,391 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,393 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,398 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,401 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,406 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,408 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,408 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:20,410 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:20,414 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:20,418 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:20,418 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,418 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,418 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,418 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,420 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,425 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,428 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,432 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,435 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,435 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:20,436 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:20,441 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:20,444 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:20,445 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,445 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,445 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,445 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,447 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,452 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,456 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,460 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,462 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,462 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:20,463 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:20,468 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:20,472 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:20,472 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,472 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,473 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,473 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,475 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,479 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,483 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,488 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,491 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,491 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:20,492 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:20,496 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:20,500 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:20,500 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,501 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,501 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,501 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,503 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,508 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,512 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,516 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,518 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,518 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:20,519 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:20,524 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:20,528 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:20,529 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,529 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,529 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,529 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,531 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,536 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,564 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,581 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,584 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,585 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:20,586 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:20,587 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:20,591 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:20,591 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,591 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 19, 64), torch.float32', '<BlockTensor>: (4, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,592 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,592 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,594 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 20), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 19, 64), torch.float32', '<MixTensor>: (1, 12, 19, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,601 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,605 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,610 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'))
2023-10-31 05:26:20,612 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'))


2023-10-31 05:26:20,612 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:20,614 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:20,614 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:20,615 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:20,615 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,615 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:20,615 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,615 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:20,616 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:20,616 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,617 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,618 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,618 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:20,619 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:20,619 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:20,619 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:20,620 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:20,620 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,620 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:20,620 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,620 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:20,621 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:20,631 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:20,639 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:20,647 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:20,655 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:20,656 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:20,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:20,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:20,661 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:20,661 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:20,661 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:20,661 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:20,661 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:20,662 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:20,663 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,663 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,664 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,664 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:20,665 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:20,665 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:20,669 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:20,670 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:20,670 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 21), torch.int64', '20')
2023-10-31 05:26:20,670 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:20,670 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 21), torch.int64', '20')
2023-10-31 05:26:20,670 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:20,671 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 21), torch.int64', '20'), {})
2023-10-31 05:26:20,672 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,672 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,673 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:20,674 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:20,675 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:20,676 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:20,680 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:20,684 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:20,684 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,684 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,685 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,685 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,687 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,693 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,697 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,701 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,704 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,705 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:20,706 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:20,711 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:20,715 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:20,716 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,716 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,716 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,716 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,718 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,723 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,727 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,732 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,735 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,735 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:20,737 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:20,741 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:20,745 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:20,745 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,746 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,746 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,746 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,748 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,753 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,757 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,763 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,765 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,765 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:20,767 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:20,772 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:20,776 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:20,777 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,777 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,777 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,777 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,780 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,786 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,791 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,815 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,818 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,818 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:20,819 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:20,824 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:20,827 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:20,828 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,828 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,828 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,828 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,830 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,835 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,838 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,842 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,850 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,850 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:20,851 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:20,855 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:20,859 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:20,859 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,860 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,860 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,860 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,862 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,881 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,887 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,891 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,894 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,895 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:20,896 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:20,900 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:20,904 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:20,904 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,904 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,904 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,905 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,906 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,911 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,916 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,921 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,924 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,924 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:20,925 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:20,929 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:20,932 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:20,933 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,933 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,933 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,933 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,935 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,940 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,944 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,948 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,951 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,951 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:20,952 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:20,956 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:20,960 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:20,960 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,960 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,960 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,960 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,962 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,967 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,971 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,974 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,977 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:20,977 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:20,978 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:20,982 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:20,986 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:20,986 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:20,986 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,986 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:20,986 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:20,988 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:20,993 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:20,996 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:21,000 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:21,002 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:21,003 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:21,004 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:21,008 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:21,011 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:21,011 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,011 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,012 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,012 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,014 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,018 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:21,022 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:21,026 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:21,028 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:21,028 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:21,030 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:21,030 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:21,034 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:21,034 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,034 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 20, 64), torch.float32', '<BlockTensor>: (4, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,034 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,035 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,036 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 21), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 20, 64), torch.float32', '<MixTensor>: (1, 12, 20, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,041 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:21,044 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:21,048 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'))
2023-10-31 05:26:21,050 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'))


2023-10-31 05:26:21,051 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:21,052 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:21,052 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:21,053 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:21,053 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,053 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,053 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,053 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,054 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:21,055 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,055 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,056 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,056 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:21,057 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:21,057 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:21,057 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:21,058 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:21,058 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,058 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,058 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,058 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,059 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:21,072 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:21,083 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:21,094 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:21,102 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:21,104 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:21,107 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:21,108 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:21,108 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:21,108 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:21,108 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,108 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:21,109 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,109 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:21,110 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,110 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,111 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,112 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:21,112 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:21,112 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:21,116 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:21,116 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:21,117 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 22), torch.int64', '21')
2023-10-31 05:26:21,117 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,117 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 22), torch.int64', '21')
2023-10-31 05:26:21,117 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,118 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 22), torch.int64', '21'), {})
2023-10-31 05:26:21,118 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,119 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,120 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,120 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:21,122 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:21,122 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:21,126 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:21,129 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:21,130 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,130 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,130 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,130 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,132 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,137 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,141 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,146 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,149 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,149 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:21,150 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:21,154 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:21,158 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:21,158 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,158 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,159 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,159 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,160 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,165 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,170 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,174 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,176 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,176 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:21,178 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:21,182 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:21,186 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:21,186 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,186 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,186 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,186 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,188 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,193 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,197 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,201 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,203 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,203 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:21,205 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:21,209 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:21,212 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:21,213 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,213 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,213 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,213 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,215 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,220 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,224 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,227 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,230 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,230 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:21,232 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:21,236 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:21,240 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:21,240 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,241 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,241 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,241 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,243 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,248 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,252 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,256 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,259 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,259 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:21,261 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:21,265 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:21,268 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:21,269 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,269 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,269 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,269 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,271 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,276 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,280 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,284 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,287 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,287 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:21,288 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:21,293 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:21,297 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:21,297 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,297 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,297 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,298 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,299 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,306 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,310 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,343 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,346 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,346 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:21,347 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:21,351 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:21,355 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:21,355 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,355 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,356 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,356 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,358 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,363 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,380 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,385 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,388 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,388 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:21,389 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:21,394 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:21,397 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:21,397 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,398 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,398 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,398 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,400 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,405 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,409 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,414 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,416 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,416 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:21,418 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:21,422 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:21,425 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:21,426 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,426 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,426 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,426 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,428 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,433 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,437 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,442 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,444 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,445 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:21,446 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:21,450 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:21,454 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:21,454 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,454 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,454 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,455 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,456 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,462 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,466 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,470 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,472 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,473 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:21,474 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:21,475 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:21,479 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:21,479 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,479 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 21, 64), torch.float32', '<BlockTensor>: (4, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,479 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,479 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,481 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 22), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 21, 64), torch.float32', '<MixTensor>: (1, 12, 21, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,486 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,490 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,494 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'))
2023-10-31 05:26:21,496 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'))


2023-10-31 05:26:21,496 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:21,498 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:21,498 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:21,499 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:21,499 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,499 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,499 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,499 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,500 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:21,501 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,501 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,502 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,502 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:21,503 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:21,503 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:21,503 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:21,504 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:21,504 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,504 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,504 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,504 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,505 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:21,516 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:21,524 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:21,537 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:21,555 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:21,557 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:21,561 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:21,561 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:21,562 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:21,562 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:21,562 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,562 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:21,563 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,563 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:21,564 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,564 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,565 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,566 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:21,566 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:21,566 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:21,570 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:21,571 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:21,571 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 23), torch.int64', '22')
2023-10-31 05:26:21,571 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,571 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 23), torch.int64', '22')
2023-10-31 05:26:21,571 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,572 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 23), torch.int64', '22'), {})
2023-10-31 05:26:21,573 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,573 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,574 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,575 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:21,576 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:21,576 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:21,580 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:21,583 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:21,584 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,584 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,584 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,584 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,586 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,618 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,637 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,641 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,644 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,644 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:21,646 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:21,650 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:21,654 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:21,654 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,654 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,654 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,655 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,656 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,661 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,665 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,669 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,671 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,671 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:21,673 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:21,677 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:21,680 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:21,681 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,681 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,681 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,681 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,683 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,688 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,692 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,696 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,698 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,698 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:21,700 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:21,704 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:21,707 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:21,708 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,708 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,708 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,708 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,710 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,715 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,720 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,724 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,726 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,727 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:21,728 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:21,732 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:21,736 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:21,736 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,737 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,737 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,737 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,739 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,744 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,748 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,752 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,754 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,754 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:21,756 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:21,760 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:21,764 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:21,764 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,764 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,764 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,764 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,766 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,771 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,774 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,778 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,781 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,781 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:21,782 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:21,786 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:21,790 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:21,790 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,790 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,791 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,791 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,793 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,797 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,801 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,804 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,807 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,807 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:21,809 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:21,813 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:21,816 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:21,816 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,817 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,817 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,817 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,819 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,824 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,828 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,849 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,851 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,852 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:21,853 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:21,858 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:21,861 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:21,862 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,862 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,862 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,862 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,864 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,875 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,879 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,884 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,887 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,887 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:21,888 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:21,892 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:21,896 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:21,896 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,897 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,897 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,897 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,899 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,904 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,908 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,912 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,915 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,915 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:21,916 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:21,920 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:21,924 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:21,924 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,924 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,925 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,925 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,927 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,931 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,935 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,939 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,942 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,942 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:21,943 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:21,944 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:21,948 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:21,948 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,948 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 22, 64), torch.float32', '<BlockTensor>: (4, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,948 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,949 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:21,950 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 23), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 22, 64), torch.float32', '<MixTensor>: (1, 12, 22, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:21,955 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,959 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,963 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'))
2023-10-31 05:26:21,966 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'))


2023-10-31 05:26:21,966 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:21,967 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:21,968 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:21,968 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:21,969 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,969 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,969 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,969 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,970 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:21,970 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,971 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,972 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:21,972 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:21,972 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:21,973 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:21,973 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:21,973 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:21,974 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:21,974 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:21,974 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:21,974 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:21,974 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:21,985 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:21,993 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:22,000 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:22,008 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:22,010 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:22,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:22,014 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:22,014 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:22,014 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:22,014 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,014 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:22,015 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,015 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:22,016 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,016 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,017 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,018 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:22,018 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:22,018 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:22,022 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:22,023 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:22,023 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 24), torch.int64', '23')
2023-10-31 05:26:22,023 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,023 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 24), torch.int64', '23')
2023-10-31 05:26:22,023 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,024 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 24), torch.int64', '23'), {})
2023-10-31 05:26:22,024 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,025 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,026 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,026 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:22,028 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:22,028 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:22,032 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:22,036 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:22,036 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,036 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,037 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,037 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,038 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,043 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,047 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,051 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,053 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,054 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:22,055 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:22,059 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:22,063 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:22,063 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,063 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,064 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,064 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,066 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,070 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,074 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,078 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,080 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,081 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:22,082 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:22,086 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:22,090 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:22,090 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,090 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,090 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,090 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,093 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,101 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,128 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,132 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,135 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,136 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:22,137 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:22,141 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:22,145 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:22,145 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,146 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,146 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,146 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,148 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,153 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,157 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,161 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,164 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,164 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:22,166 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:22,171 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:22,174 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:22,175 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,175 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,175 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,175 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,177 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,182 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,186 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,190 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,193 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,193 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:22,194 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:22,198 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:22,202 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:22,202 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,203 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,203 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,203 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,205 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,210 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,215 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,219 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,221 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,221 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:22,223 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:22,227 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:22,231 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:22,231 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,231 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,232 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,232 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,234 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,239 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,242 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,246 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,249 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,249 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:22,250 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:22,255 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:22,259 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:22,259 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,260 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,260 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,260 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,262 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,267 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,270 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,275 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,277 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,278 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:22,279 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:22,283 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:22,287 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:22,288 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,288 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,288 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,288 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,290 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,308 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,312 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,316 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,318 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,319 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:22,320 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:22,324 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:22,328 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:22,328 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,328 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,329 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,329 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,331 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,335 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,344 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,347 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,350 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,350 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:22,351 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:22,356 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:22,359 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:22,359 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,360 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,360 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,360 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,362 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,366 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,380 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,384 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,387 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,387 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:22,389 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:22,390 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:22,394 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:22,394 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,394 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 23, 64), torch.float32', '<BlockTensor>: (4, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,395 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,395 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,397 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 24), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 23, 64), torch.float32', '<MixTensor>: (1, 12, 23, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,427 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,431 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,436 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'))
2023-10-31 05:26:22,438 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'))


2023-10-31 05:26:22,438 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:22,440 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:22,440 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:22,441 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:22,441 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,441 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,442 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,442 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,442 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:22,443 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,444 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,445 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,445 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:22,446 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:22,446 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:22,446 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:22,447 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:22,447 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,447 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,447 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,447 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,448 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:22,456 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:22,464 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:22,472 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:22,480 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:22,481 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:22,485 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:22,485 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:22,485 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:22,486 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:22,486 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,486 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:22,486 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,487 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:22,487 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,488 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,489 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,489 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:22,489 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:22,490 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:22,494 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:22,494 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:22,494 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 25), torch.int64', '24')
2023-10-31 05:26:22,494 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,495 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 25), torch.int64', '24')
2023-10-31 05:26:22,495 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,495 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 25), torch.int64', '24'), {})
2023-10-31 05:26:22,496 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,497 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,497 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,498 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:22,499 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:22,500 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:22,504 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:22,507 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:22,507 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,508 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,508 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,508 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,510 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,515 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,518 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,524 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,527 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,527 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:22,528 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:22,532 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:22,536 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:22,536 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,536 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,537 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,537 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,539 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,543 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,547 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,551 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,553 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,553 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:22,555 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:22,559 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:22,563 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:22,563 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,563 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,563 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,563 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,565 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,570 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,574 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,578 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,580 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,580 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:22,582 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:22,586 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:22,589 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:22,589 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,590 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,590 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,590 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,592 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,597 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,600 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,604 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,606 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,606 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:22,608 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:22,612 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:22,616 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:22,616 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,616 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,616 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,616 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,618 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,633 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,637 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,642 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,644 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,645 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:22,646 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:22,650 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:22,654 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:22,654 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,654 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,654 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,655 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,656 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,661 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,664 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,669 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,672 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,672 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:22,673 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:22,677 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:22,681 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:22,681 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,681 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,682 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,682 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,684 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,688 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,692 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,697 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,700 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,700 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:22,702 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:22,706 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:22,709 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:22,710 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,710 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,710 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,710 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,712 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,717 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,721 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,725 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,728 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,728 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:22,729 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:22,733 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:22,737 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:22,738 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,738 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,738 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,738 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,740 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,745 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,749 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,753 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,755 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,756 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:22,757 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:22,761 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:22,765 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:22,765 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,765 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,765 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,766 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,768 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,772 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,775 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,779 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,782 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,782 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:22,783 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:22,787 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:22,791 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:22,791 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,792 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,792 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,792 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,794 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,799 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,803 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,807 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,809 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,810 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:22,811 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:22,812 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:22,816 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:22,816 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,816 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 24, 64), torch.float32', '<BlockTensor>: (4, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,816 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,817 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,818 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 25), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 24, 64), torch.float32', '<MixTensor>: (1, 12, 24, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,823 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,827 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,831 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'))
2023-10-31 05:26:22,833 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'))


2023-10-31 05:26:22,834 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:22,835 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:22,835 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:22,836 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:22,836 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,836 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,836 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,837 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,837 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:22,838 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,839 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,840 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,840 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:22,840 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:22,841 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:22,841 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:22,841 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:22,842 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,842 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,842 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,842 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,842 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:22,851 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:22,858 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:22,866 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:22,890 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:22,895 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:22,899 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:22,899 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:22,900 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:22,900 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:22,900 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,900 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:22,900 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,901 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:22,901 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,902 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,903 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,903 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:22,903 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:22,904 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:22,908 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:22,908 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:22,908 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 26), torch.int64', '25')
2023-10-31 05:26:22,908 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:22,909 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 26), torch.int64', '25')
2023-10-31 05:26:22,909 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:22,909 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 26), torch.int64', '25'), {})
2023-10-31 05:26:22,910 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,911 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,911 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:22,912 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:22,913 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:22,914 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:22,918 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:22,921 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:22,921 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,921 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,922 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,922 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,924 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,944 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:22,949 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:22,964 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:22,967 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:22,967 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:22,969 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:22,973 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:22,976 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:22,976 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:22,976 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,977 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:22,977 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:22,979 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:22,983 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:22,988 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:22,997 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,000 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,000 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:23,002 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:23,006 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:23,009 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:23,009 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,009 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,010 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,010 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,012 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,017 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,020 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,025 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,028 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,028 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:23,029 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:23,033 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:23,037 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:23,037 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,037 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,037 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,037 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,039 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,044 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,048 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,052 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,055 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,055 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:23,057 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:23,061 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:23,064 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:23,064 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,064 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,064 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,065 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,066 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,071 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,081 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,086 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,089 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,089 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:23,091 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:23,095 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:23,098 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:23,099 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,099 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,099 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,099 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,101 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,106 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,111 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,115 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,118 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,118 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:23,119 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:23,124 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:23,127 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:23,127 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,128 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,128 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,128 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,130 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,135 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,139 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,143 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,145 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,145 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:23,147 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:23,151 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:23,154 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:23,154 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,154 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,155 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,155 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,157 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,162 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,167 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,172 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,175 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,175 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:23,176 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:23,181 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:23,184 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:23,185 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,185 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,185 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,185 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,187 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,192 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,196 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,200 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,202 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,202 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:23,204 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:23,208 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:23,212 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:23,212 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,212 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,212 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,213 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,214 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,221 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,228 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,249 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,252 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,253 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:23,255 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:23,261 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:23,265 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:23,266 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,267 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,268 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,268 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,270 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,276 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,280 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,285 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,287 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,288 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:23,290 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:23,291 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:23,295 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:23,295 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,295 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 25, 64), torch.float32', '<BlockTensor>: (4, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,296 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,296 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,298 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 26), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 25, 64), torch.float32', '<MixTensor>: (1, 12, 25, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,311 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,315 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,319 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'))
2023-10-31 05:26:23,322 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'))


2023-10-31 05:26:23,322 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:23,323 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:23,324 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:23,324 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:23,324 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,325 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:23,325 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,325 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:23,325 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:23,326 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,327 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,328 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,328 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:23,328 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:23,329 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:23,329 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:23,329 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:23,330 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,330 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:23,330 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,330 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:23,331 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:23,339 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:23,347 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:23,354 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:23,362 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:23,363 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:23,367 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:23,367 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:23,367 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:23,368 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:23,368 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:23,368 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:23,368 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:23,369 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:23,369 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,370 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,371 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,371 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:23,371 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:23,372 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:23,376 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:23,376 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:23,376 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 27), torch.int64', '26')
2023-10-31 05:26:23,376 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:23,376 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 27), torch.int64', '26')
2023-10-31 05:26:23,377 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:23,377 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 27), torch.int64', '26'), {})
2023-10-31 05:26:23,378 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,378 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,379 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,380 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:23,381 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:23,382 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:23,385 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:23,389 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:23,389 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,389 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,390 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,390 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,392 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,396 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,400 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,406 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,421 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,421 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:23,423 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:23,427 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:23,431 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:23,431 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,431 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,431 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,431 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,433 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,438 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,448 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,452 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,455 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,455 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:23,457 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:23,461 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:23,465 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:23,466 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,466 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,466 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,466 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,468 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,473 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,477 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,481 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,484 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,484 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:23,486 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:23,491 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:23,494 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:23,495 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,495 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,495 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,495 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,497 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,502 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,506 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,511 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,513 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,513 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:23,515 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:23,519 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:23,523 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:23,523 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,524 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,524 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,524 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,526 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,531 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,535 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,541 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,543 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,543 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:23,545 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:23,549 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:23,553 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:23,553 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,554 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,554 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,554 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,556 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,561 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,565 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,569 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,572 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,572 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:23,573 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:23,578 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:23,581 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:23,582 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,582 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,582 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,582 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,584 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,589 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,593 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,597 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,600 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,600 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:23,602 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:23,606 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:23,609 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:23,610 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,610 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,610 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,610 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,612 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,617 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,639 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,644 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,647 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,647 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:23,649 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:23,654 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:23,657 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:23,658 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,658 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,658 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,658 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,661 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,666 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,670 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,675 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,678 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,678 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:23,680 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:23,684 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:23,688 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:23,689 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,689 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,689 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,689 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,691 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,696 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,700 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,704 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,707 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,707 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:23,708 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:23,713 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:23,717 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:23,717 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,717 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,718 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,718 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,720 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,725 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,729 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,734 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,736 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,736 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:23,738 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:23,738 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:23,742 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:23,742 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,742 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 26, 64), torch.float32', '<BlockTensor>: (4, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,743 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,743 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,745 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 27), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 26, 64), torch.float32', '<MixTensor>: (1, 12, 26, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,750 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,754 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,758 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'))
2023-10-31 05:26:23,761 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'))


2023-10-31 05:26:23,761 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:23,762 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:23,763 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:23,763 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:23,763 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,763 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:23,764 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,764 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:23,764 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:23,765 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,766 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,766 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,767 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:23,767 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:23,767 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:23,768 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:23,768 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:23,768 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,768 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:23,769 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,769 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:23,769 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:23,778 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:23,786 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:23,794 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:23,801 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:23,803 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:23,807 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:23,808 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:23,808 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:23,808 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:23,808 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:23,808 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:23,809 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:23,809 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:23,810 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,811 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,811 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,812 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:23,812 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:23,813 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:23,817 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:23,817 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:23,818 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 28), torch.int64', '27')
2023-10-31 05:26:23,818 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:23,818 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 28), torch.int64', '27')
2023-10-31 05:26:23,818 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:23,819 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 28), torch.int64', '27'), {})
2023-10-31 05:26:23,819 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,820 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,821 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:23,822 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:23,823 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:23,824 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:23,828 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:23,832 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:23,832 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,832 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,832 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,832 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,834 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,839 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,843 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,847 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,850 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:23,850 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:23,851 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:23,855 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:23,860 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:23,860 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,860 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,861 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,861 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,863 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,867 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,871 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,876 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,882 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:23,882 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:23,883 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:23,888 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:23,892 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:23,892 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,892 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,892 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,893 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,895 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,923 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,944 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,948 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,951 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:23,951 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:23,953 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:23,957 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:23,961 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:23,962 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,962 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,962 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,962 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,964 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:23,969 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,974 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,979 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:23,981 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:23,982 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:23,984 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:23,988 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:23,992 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:23,992 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:23,993 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,993 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:23,993 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:23,995 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,000 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,004 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,008 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,011 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:24,011 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:24,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:24,017 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:24,020 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:24,020 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,021 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,021 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,021 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,023 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,028 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,032 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,036 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,039 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:24,039 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:24,040 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:24,045 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:24,049 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:24,049 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,049 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,049 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,049 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,051 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,056 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,060 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,064 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,066 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:24,067 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:24,068 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:24,072 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:24,076 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:24,076 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,077 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,077 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,077 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,079 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,084 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,088 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,092 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,094 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:24,094 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:24,096 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:24,100 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:24,104 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:24,105 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,105 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,105 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,105 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,107 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,112 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,116 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,122 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,129 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:24,129 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:24,131 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:24,135 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:24,138 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:24,139 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,139 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,139 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,139 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,141 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,148 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,152 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,155 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,158 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:24,158 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:24,159 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:24,163 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:24,167 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:24,167 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,167 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,168 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,168 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,170 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,174 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,178 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,182 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,184 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:24,184 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:24,186 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:24,186 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:24,190 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:24,190 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,190 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 27, 64), torch.float32', '<BlockTensor>: (4, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,191 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,191 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,193 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 28), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 27, 64), torch.float32', '<MixTensor>: (1, 12, 27, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,197 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,201 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,205 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'))
2023-10-31 05:26:24,207 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'))


2023-10-31 05:26:24,207 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:24,208 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:24,209 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:24,209 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:24,209 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,210 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:24,210 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,210 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:24,210 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:24,211 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,212 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,213 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,213 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:24,213 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:24,214 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:24,214 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:24,214 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:24,214 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,214 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:24,215 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,215 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:24,215 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:24,223 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:24,231 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:24,239 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:24,246 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:24,248 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:24,251 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:24,252 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:24,252 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:24,252 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:24,252 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:24,252 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:24,253 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:24,253 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:24,254 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,254 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,255 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,255 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:24,256 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:24,256 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:24,260 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:24,260 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:24,260 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 29), torch.int64', '28')
2023-10-31 05:26:24,261 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:24,261 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 29), torch.int64', '28')
2023-10-31 05:26:24,261 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:24,261 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 29), torch.int64', '28'), {})
2023-10-31 05:26:24,262 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,263 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,263 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,264 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:24,265 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:24,266 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:24,269 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:24,273 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:24,273 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,273 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,273 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,273 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,275 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,280 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,284 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,288 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,290 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,290 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:24,292 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:24,296 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:24,299 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:24,299 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,300 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,300 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,300 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,302 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,306 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,310 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,314 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,316 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,317 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:24,318 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:24,322 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:24,326 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:24,326 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,326 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,326 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,327 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,329 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,333 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,337 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,341 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,343 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,343 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:24,345 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:24,349 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:24,353 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:24,353 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,353 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,353 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,353 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,355 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,360 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,364 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,368 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,370 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,370 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:24,372 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:24,376 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:24,379 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:24,380 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,380 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,380 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,380 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,382 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,389 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,393 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,421 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,424 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,424 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:24,426 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:24,430 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:24,435 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:24,436 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,436 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,436 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,437 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,439 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,444 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,449 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,453 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,456 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,456 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:24,458 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:24,462 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:24,467 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:24,467 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,467 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,467 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,468 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,470 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,475 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,479 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,484 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,486 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,487 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:24,488 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:24,493 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:24,497 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:24,497 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,497 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,498 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,498 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,500 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,505 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,509 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,515 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,517 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,518 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:24,519 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:24,524 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:24,528 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:24,528 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,529 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,529 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,529 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,531 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,536 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,540 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,545 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,548 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,548 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:24,549 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:24,554 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:24,558 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:24,558 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,558 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,559 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,559 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,561 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,566 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,570 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,575 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,578 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,578 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:24,580 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:24,585 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:24,589 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:24,589 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,589 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,590 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,590 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,592 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,597 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,601 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,607 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,609 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,610 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:24,611 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:24,612 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:24,616 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:24,617 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,617 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 28, 64), torch.float32', '<BlockTensor>: (4, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,617 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,617 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,619 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 29), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 28, 64), torch.float32', '<MixTensor>: (1, 12, 28, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,652 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,656 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,661 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'))
2023-10-31 05:26:24,664 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'))


2023-10-31 05:26:24,664 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:24,665 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:24,666 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:24,667 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:24,667 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,667 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:24,667 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,667 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:24,668 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:24,669 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,670 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,670 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,671 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:24,671 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:24,672 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:24,672 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:24,673 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:24,673 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,673 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:24,673 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,673 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:24,674 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:24,682 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:24,690 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:24,698 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:24,706 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:24,707 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:24,711 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:24,712 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:24,712 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:24,713 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:24,713 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:24,713 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:24,713 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:24,714 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:24,714 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,715 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,716 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,716 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:24,717 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:24,717 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:24,722 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:24,722 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:24,723 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 30), torch.int64', '29')
2023-10-31 05:26:24,723 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:24,723 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 30), torch.int64', '29')
2023-10-31 05:26:24,723 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:24,724 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 30), torch.int64', '29'), {})
2023-10-31 05:26:24,725 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,725 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,726 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:24,727 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:24,729 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:24,729 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:24,733 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:24,737 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:24,737 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,737 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,737 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,738 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,740 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,745 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,750 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,754 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,757 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:24,757 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:24,759 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:24,763 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:24,767 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:24,767 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,767 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,768 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,768 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,770 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,775 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,780 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,784 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,787 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:24,788 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:24,790 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:24,794 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:24,798 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:24,798 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,798 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,799 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,799 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,801 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,806 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,810 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,817 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,821 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:24,822 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:24,824 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:24,829 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:24,832 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:24,833 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,833 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,833 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,833 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,835 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,840 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,845 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,849 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,852 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:24,852 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:24,854 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:24,859 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:24,863 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:24,864 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,864 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,864 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,864 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,866 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,872 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,876 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,882 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,910 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:24,910 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:24,912 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:24,916 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:24,919 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:24,919 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,920 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,920 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,920 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,922 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,927 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,931 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,944 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,946 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:24,946 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:24,947 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:24,952 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:24,955 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:24,955 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,955 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,956 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,956 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,958 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,963 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,968 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,972 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,975 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:24,975 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:24,977 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:24,981 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:24,984 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:24,984 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:24,985 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,985 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:24,985 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:24,987 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:24,992 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:24,996 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,000 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,002 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:25,003 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:25,004 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:25,008 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:25,012 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:25,012 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,012 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,012 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,012 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,014 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,019 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,023 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,027 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,029 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:25,029 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:25,031 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:25,035 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:25,039 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:25,039 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,039 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,039 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,039 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,041 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,046 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,050 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,055 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,057 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:25,057 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:25,058 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:25,063 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:25,066 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:25,067 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,067 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,067 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,067 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,069 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,074 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,078 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,082 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,084 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:25,085 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:25,086 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:25,087 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:25,091 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:25,091 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,091 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 29, 64), torch.float32', '<BlockTensor>: (4, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,091 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,092 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,093 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 30), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 29, 64), torch.float32', '<MixTensor>: (1, 12, 29, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,098 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,102 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,110 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'))
2023-10-31 05:26:25,112 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'))


2023-10-31 05:26:25,112 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:25,113 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:25,114 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:25,114 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:25,115 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,115 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:25,115 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,115 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:25,116 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:25,116 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,117 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,118 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,118 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:25,119 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:25,119 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:25,119 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:25,120 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:25,120 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,120 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:25,120 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,120 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:25,121 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:25,132 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:25,143 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:25,153 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:25,161 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:25,163 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:25,166 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:25,167 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:25,167 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:25,167 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:25,167 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:25,168 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:25,168 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:25,168 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:25,169 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,170 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,170 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,171 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:25,171 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:25,171 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:25,175 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:25,176 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:25,176 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 31), torch.int64', '30')
2023-10-31 05:26:25,176 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:25,176 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 31), torch.int64', '30')
2023-10-31 05:26:25,176 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:25,177 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 31), torch.int64', '30'), {})
2023-10-31 05:26:25,177 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,178 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,179 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,179 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:25,181 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:25,181 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:25,185 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:25,188 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:25,189 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,189 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,189 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,189 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,191 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,197 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,201 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,207 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,209 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,209 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:25,211 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:25,216 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:25,219 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:25,220 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,220 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,220 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,220 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,222 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,227 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,231 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,236 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,239 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,239 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:25,241 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:25,245 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:25,248 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:25,249 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,249 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,249 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,249 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,251 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,256 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,261 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,265 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,268 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,268 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:25,269 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:25,273 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:25,277 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:25,278 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,278 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,278 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,278 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,280 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,285 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,289 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,294 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,296 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,296 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:25,298 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:25,302 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:25,306 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:25,306 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,306 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,307 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,307 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,309 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,316 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,347 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,351 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,355 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,355 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:25,356 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:25,361 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:25,364 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:25,365 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,365 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,365 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,365 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,367 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,372 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,376 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,381 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,390 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,391 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:25,392 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:25,396 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:25,400 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:25,400 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,400 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,400 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,401 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,403 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,434 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,438 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,443 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,445 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,446 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:25,447 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:25,451 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:25,455 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:25,455 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,455 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,455 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,456 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,457 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,462 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,466 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,471 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,473 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,473 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:25,475 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:25,479 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:25,483 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:25,483 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,483 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,483 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,484 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,485 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,490 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,494 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,498 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,501 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,501 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:25,503 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:25,507 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:25,510 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:25,510 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,511 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,511 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,511 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,513 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,519 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,523 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,527 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,530 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,530 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:25,531 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:25,536 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:25,540 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:25,540 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,540 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,540 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,540 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,542 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,547 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,551 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,556 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,559 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,559 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:25,560 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:25,561 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:25,565 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:25,565 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,565 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 30, 64), torch.float32', '<BlockTensor>: (4, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,565 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,565 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,567 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 31), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 30, 64), torch.float32', '<MixTensor>: (1, 12, 30, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,572 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,576 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,581 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'))
2023-10-31 05:26:25,584 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'))


2023-10-31 05:26:25,584 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:25,585 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:25,586 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:25,586 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:25,586 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,586 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:25,587 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,587 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:25,587 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:25,588 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,589 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,589 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,590 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:25,590 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:25,590 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:25,591 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:25,591 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:25,591 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,591 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:25,592 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,592 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:25,592 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:25,601 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:25,608 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:25,616 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:25,637 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:25,641 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:25,645 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:25,646 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:25,646 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:25,646 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:25,646 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:25,647 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:25,647 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:25,647 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:25,648 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,648 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,649 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,650 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:25,650 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:25,650 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:25,654 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:25,655 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:25,655 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 32), torch.int64', '31')
2023-10-31 05:26:25,655 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:25,655 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 32), torch.int64', '31')
2023-10-31 05:26:25,655 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:25,656 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 32), torch.int64', '31'), {})
2023-10-31 05:26:25,657 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,657 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,658 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:25,659 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:25,660 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:25,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:25,664 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:25,668 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:25,668 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,668 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,669 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,669 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,671 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,676 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,680 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,685 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,687 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,687 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:25,689 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:25,693 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:25,697 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:25,697 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,697 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,697 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,697 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,699 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,704 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,708 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,712 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,715 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,715 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:25,717 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:25,721 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:25,724 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:25,725 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,725 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,725 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,725 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,727 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,732 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,736 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,740 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,743 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,743 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:25,745 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:25,749 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:25,752 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:25,753 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,753 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,753 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,753 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,755 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,760 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,764 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,768 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,771 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,771 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:25,772 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:25,777 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:25,781 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:25,781 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,781 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,781 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,781 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,784 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,788 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,792 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,797 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,801 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,801 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:25,804 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:25,808 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:25,811 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:25,812 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,812 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,812 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,812 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,814 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,819 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,823 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,827 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,830 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,830 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:25,832 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:25,835 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:25,839 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:25,839 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,839 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,840 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,840 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,842 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,847 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,851 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,855 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,858 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,858 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:25,859 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:25,863 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:25,867 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:25,867 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,867 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,868 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,868 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,870 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,875 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,879 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,883 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,910 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,911 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:25,912 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:25,917 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:25,920 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:25,921 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,921 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,921 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,921 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,923 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,929 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,934 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,939 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,943 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,943 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:25,945 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:25,949 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:25,952 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:25,953 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,953 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,953 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,954 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,956 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,962 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,968 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,975 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:25,977 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:25,978 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:25,979 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:25,983 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:25,987 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:25,987 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:25,987 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,988 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:25,988 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:25,991 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:25,996 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:26,001 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:26,008 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:26,011 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:26,011 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:26,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:26,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:26,017 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:26,017 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,018 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 31, 64), torch.float32', '<BlockTensor>: (4, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,018 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,018 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,020 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 32), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 31, 64), torch.float32', '<MixTensor>: (1, 12, 31, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,026 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:26,031 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:26,038 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'))
2023-10-31 05:26:26,041 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'))


2023-10-31 05:26:26,041 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:26,042 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:26,042 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:26,043 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:26,043 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,043 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:26,044 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,044 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:26,044 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:26,045 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,046 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,047 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,047 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:26,047 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:26,048 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:26,048 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:26,048 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:26,048 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,049 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:26,049 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,049 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:26,049 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:26,058 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:26,066 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:26,073 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:26,081 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:26,082 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:26,086 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:26,086 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:26,087 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:26,087 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:26,087 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:26,087 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:26,087 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:26,088 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:26,088 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,089 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,090 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,090 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:26,091 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:26,091 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:26,095 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:26,096 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:26,096 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 33), torch.int64', '32')
2023-10-31 05:26:26,096 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:26,096 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 33), torch.int64', '32')
2023-10-31 05:26:26,096 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:26,097 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 33), torch.int64', '32'), {})
2023-10-31 05:26:26,097 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,098 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,099 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,099 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:26,101 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:26,101 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:26,105 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:26,109 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:26,109 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,109 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,110 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,110 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,112 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,117 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,121 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,126 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,128 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,129 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:26,130 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:26,134 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:26,138 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:26,138 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,138 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,139 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,139 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,141 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,157 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,161 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,166 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,168 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,169 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:26,170 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:26,174 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:26,177 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:26,178 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,178 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,178 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,178 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,180 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,185 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,189 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,193 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,195 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,196 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:26,197 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:26,201 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:26,205 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:26,205 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,205 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,205 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,205 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,207 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,212 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,216 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,220 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,222 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,223 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:26,224 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:26,228 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:26,232 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:26,232 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,232 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,233 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,233 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,235 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,239 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,243 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,247 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,250 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,250 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:26,251 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:26,256 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:26,259 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:26,259 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,259 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,260 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,260 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,262 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,267 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,271 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,274 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,277 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,277 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:26,278 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:26,282 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:26,286 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:26,286 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,286 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,286 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,287 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,288 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,293 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,297 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,301 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,304 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,304 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:26,306 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:26,310 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:26,313 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:26,314 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,314 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,314 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,314 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,316 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,321 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,325 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,329 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,331 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,331 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:26,332 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:26,337 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:26,340 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:26,340 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,341 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,341 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,341 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,343 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,348 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,352 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,356 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,358 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,358 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:26,360 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:26,364 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:26,368 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:26,368 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,368 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,368 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,368 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,370 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,375 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,379 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,384 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,393 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,393 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:26,395 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:26,399 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:26,402 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:26,402 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,403 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,403 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,403 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,405 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,428 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,433 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,437 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,440 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,440 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:26,441 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:26,442 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:26,445 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:26,446 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,446 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 32, 64), torch.float32', '<BlockTensor>: (4, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,446 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,446 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,448 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 33), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 32, 64), torch.float32', '<MixTensor>: (1, 12, 32, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,460 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,464 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,469 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'))
2023-10-31 05:26:26,471 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'))


2023-10-31 05:26:26,471 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:26,473 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:26,473 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:26,474 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:26,474 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,474 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:26,474 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,474 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:26,475 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:26,476 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,476 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,477 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,477 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:26,478 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:26,478 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:26,478 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:26,479 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:26,479 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,479 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:26,479 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,479 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:26,480 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:26,490 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:26,498 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:26,506 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:26,515 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:26,516 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:26,519 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:26,520 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:26,521 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:26,521 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:26,521 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:26,521 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:26,521 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:26,522 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:26,523 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,524 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,525 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,525 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:26,526 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:26,526 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:26,531 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:26,532 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:26,532 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 34), torch.int64', '33')
2023-10-31 05:26:26,532 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:26,532 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 34), torch.int64', '33')
2023-10-31 05:26:26,533 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:26,533 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 34), torch.int64', '33'), {})
2023-10-31 05:26:26,534 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,535 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,536 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:26,536 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:26,539 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:26,539 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:26,544 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:26,550 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:26,550 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,550 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,551 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,551 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,553 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,561 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,566 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,575 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,578 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,578 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:26,581 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:26,586 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:26,591 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:26,591 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,591 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,592 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,592 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,595 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,602 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,607 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,613 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,616 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,616 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:26,618 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:26,624 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:26,629 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:26,629 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,629 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,629 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,630 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,631 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,650 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,654 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,659 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,662 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,662 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:26,664 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:26,667 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:26,671 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:26,671 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,671 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,672 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,672 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,674 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,679 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,683 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,695 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,697 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,697 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:26,699 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:26,703 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:26,707 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:26,707 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,707 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,707 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,708 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,709 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,714 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,718 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,723 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,726 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,726 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:26,727 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:26,731 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:26,735 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:26,735 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,735 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,735 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,736 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,738 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,742 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,748 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,752 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,755 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,755 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:26,756 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:26,760 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:26,764 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:26,764 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,764 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,765 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,765 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,767 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,776 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,781 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,785 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,787 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,787 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:26,789 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:26,793 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:26,796 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:26,796 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,797 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,797 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,797 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,799 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,806 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,810 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,814 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,816 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,817 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:26,818 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:26,822 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:26,826 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:26,826 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,826 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,826 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,826 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,828 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,833 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,838 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,842 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,844 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,844 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:26,846 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:26,850 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:26,854 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:26,854 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,855 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,856 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,856 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,859 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,913 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,921 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,925 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,928 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,928 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:26,930 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:26,934 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:26,937 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:26,938 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,938 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,938 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,938 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,940 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,946 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,954 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,958 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,961 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:26,961 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:26,963 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:26,963 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:26,967 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:26,967 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:26,967 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 33, 64), torch.float32', '<BlockTensor>: (4, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,968 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:26,968 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:26,970 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 34), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 33, 64), torch.float32', '<MixTensor>: (1, 12, 33, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:26,979 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:26,983 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:27,003 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'))
2023-10-31 05:26:27,006 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'))


2023-10-31 05:26:27,006 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:27,007 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:27,008 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:27,008 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:27,008 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,009 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,009 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,009 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,010 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:27,010 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,011 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,012 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,012 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,013 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:27,013 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:27,014 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:27,014 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:27,014 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,014 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,015 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,015 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,015 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:27,023 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,031 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,039 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,046 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:27,048 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:27,052 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:27,052 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:27,052 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:27,053 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:27,053 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,053 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:27,053 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,054 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:27,055 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,055 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,056 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,057 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,057 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:27,058 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:27,063 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:27,063 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:27,063 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 35), torch.int64', '34')
2023-10-31 05:26:27,063 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,064 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 35), torch.int64', '34')
2023-10-31 05:26:27,064 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,064 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 35), torch.int64', '34'), {})
2023-10-31 05:26:27,065 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,066 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,067 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,067 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,069 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:27,070 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:27,074 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:27,079 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:27,079 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,079 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,079 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,079 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,082 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,088 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,092 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,097 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,100 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,100 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:27,102 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:27,106 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:27,110 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:27,110 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,110 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,111 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,111 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,113 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,118 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,123 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,128 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,130 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,131 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:27,132 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:27,137 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:27,142 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:27,142 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,142 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,143 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,143 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,145 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,150 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,155 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,160 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,178 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,179 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:27,180 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:27,184 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:27,188 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:27,188 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,188 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,189 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,189 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,191 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,195 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,199 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,203 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,206 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,206 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:27,207 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:27,211 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:27,215 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:27,215 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,215 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,216 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,216 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,218 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,222 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,226 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,230 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,232 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,232 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:27,234 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:27,238 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:27,241 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:27,242 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,242 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,242 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,242 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,244 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,249 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,253 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,257 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,259 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,259 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:27,261 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:27,265 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:27,268 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:27,268 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,269 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,269 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,269 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,271 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,277 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,281 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,288 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,290 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,290 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:27,292 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:27,296 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:27,299 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:27,299 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,299 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,300 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,300 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,302 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,307 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,325 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,329 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,333 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,333 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:27,334 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:27,339 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:27,342 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:27,343 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,343 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,343 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,343 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,345 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,350 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,355 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,359 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,361 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,362 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:27,363 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:27,367 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:27,371 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:27,371 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,371 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,372 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,372 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,374 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,378 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,383 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,387 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,390 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,390 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:27,391 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:27,395 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:27,399 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:27,399 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,400 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,401 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,401 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,403 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,408 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,412 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,417 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,419 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,420 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:27,421 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:27,422 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:27,425 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:27,425 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,425 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 34, 64), torch.float32', '<BlockTensor>: (4, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,426 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,426 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,428 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 35), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 34, 64), torch.float32', '<MixTensor>: (1, 12, 34, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,433 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,438 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,442 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'))
2023-10-31 05:26:27,445 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'))


2023-10-31 05:26:27,445 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:27,446 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:27,447 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:27,447 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:27,447 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,447 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,448 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,448 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,448 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:27,449 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,450 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,451 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,451 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,451 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:27,451 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:27,452 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:27,452 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:27,453 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,453 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,453 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,453 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,454 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:27,462 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,469 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,477 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,485 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:27,487 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:27,490 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:27,491 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:27,491 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:27,491 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:27,491 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,492 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:27,492 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,492 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:27,493 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,493 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,494 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,495 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,495 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:27,495 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:27,500 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:27,500 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:27,500 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 36), torch.int64', '35')
2023-10-31 05:26:27,500 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,501 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 36), torch.int64', '35')
2023-10-31 05:26:27,501 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,501 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 36), torch.int64', '35'), {})
2023-10-31 05:26:27,502 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,503 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,503 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,504 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,505 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:27,506 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:27,510 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:27,513 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:27,513 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,514 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,514 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,514 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,516 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,521 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,526 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,531 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,533 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,533 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:27,535 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:27,539 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:27,543 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:27,543 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,543 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,544 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,544 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,546 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,551 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,555 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,560 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,563 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,563 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:27,564 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:27,569 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:27,572 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:27,572 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,572 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,573 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,573 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,575 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,580 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,584 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,588 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,592 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,592 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:27,594 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:27,598 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:27,602 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:27,602 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,602 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,603 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,603 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,605 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,610 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,614 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,618 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,620 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,621 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:27,622 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:27,627 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:27,631 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:27,631 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,631 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,631 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,632 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,634 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,638 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,642 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,647 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,649 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,649 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:27,651 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:27,655 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:27,659 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:27,659 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,659 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,659 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,660 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,662 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,666 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,670 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,675 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,677 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,678 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:27,679 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:27,683 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:27,687 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:27,687 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,687 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,688 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,688 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,690 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,695 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,699 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,703 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,706 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,706 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:27,708 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:27,712 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:27,716 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:27,716 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,716 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,716 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,716 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,718 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,723 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,727 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,732 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,735 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,735 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:27,736 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:27,741 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:27,744 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:27,745 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,745 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,745 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,745 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,747 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,775 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,780 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,785 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,788 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,788 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:27,790 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:27,794 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:27,798 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:27,798 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,798 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,799 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,799 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,801 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,806 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,811 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,816 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,819 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,819 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:27,821 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:27,825 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:27,829 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:27,829 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,829 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,829 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,829 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,832 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,837 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,841 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,846 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,849 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,850 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:27,851 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:27,852 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:27,856 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:27,856 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,856 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 35, 64), torch.float32', '<BlockTensor>: (4, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,856 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,857 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,859 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 36), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 35, 64), torch.float32', '<MixTensor>: (1, 12, 35, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,864 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,868 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,872 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'))
2023-10-31 05:26:27,875 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'))


2023-10-31 05:26:27,875 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:27,876 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:27,877 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:27,877 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:27,877 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,878 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,878 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,878 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,878 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:27,879 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,880 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,881 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,881 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,881 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:27,882 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:27,882 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:27,882 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:27,883 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,883 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,883 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,883 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,883 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:27,893 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,901 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,909 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:27,917 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:27,919 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:27,923 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:27,923 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:27,923 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:27,924 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:27,924 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,924 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:27,924 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,925 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:27,925 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,926 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,927 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,927 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,928 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:27,928 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:27,932 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:27,932 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:27,933 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 37), torch.int64', '36')
2023-10-31 05:26:27,933 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:27,933 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 37), torch.int64', '36')
2023-10-31 05:26:27,933 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:27,934 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 37), torch.int64', '36'), {})
2023-10-31 05:26:27,934 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,935 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,936 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:27,936 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:27,938 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:27,938 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:27,942 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:27,946 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:27,946 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,946 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,947 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,947 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,949 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,957 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:27,961 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:27,967 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:27,970 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:27,970 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:27,971 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:27,977 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:27,981 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:27,981 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:27,981 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,981 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:27,982 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:27,983 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:27,989 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:27,993 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,001 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,004 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,004 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:28,006 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:28,010 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:28,013 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:28,014 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,014 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,014 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,014 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,016 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,021 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,046 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,052 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,055 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,055 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:28,057 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:28,061 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:28,065 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:28,065 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,065 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,066 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,066 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,068 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,075 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,079 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,084 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,087 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,087 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:28,089 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:28,093 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:28,098 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:28,098 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,098 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,099 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,099 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,101 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,107 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,111 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,116 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,120 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,121 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:28,122 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:28,127 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:28,130 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:28,130 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,131 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,131 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,131 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,133 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,138 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,143 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,147 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,150 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,151 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:28,152 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:28,157 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:28,163 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:28,163 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,163 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,164 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,164 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,166 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,171 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,176 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,181 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,183 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,184 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:28,185 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:28,191 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:28,195 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:28,195 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,195 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,196 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,196 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,198 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,204 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,208 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,239 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,267 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,267 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:28,269 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:28,276 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:28,280 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:28,281 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,281 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,282 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,282 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,285 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,291 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,296 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,301 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,303 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,303 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:28,305 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:28,309 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:28,313 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:28,314 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,314 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,314 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,314 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,316 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,321 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,326 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,331 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,333 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,334 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:28,335 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:28,340 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:28,344 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:28,344 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,344 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,344 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,345 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,347 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,352 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,356 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,365 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,367 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,367 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:28,369 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:28,370 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:28,373 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:28,373 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,373 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 36, 64), torch.float32', '<BlockTensor>: (4, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,374 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,374 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,376 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 37), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 36, 64), torch.float32', '<MixTensor>: (1, 12, 36, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,381 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,385 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,392 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'))
2023-10-31 05:26:28,394 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'))


2023-10-31 05:26:28,395 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:28,396 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:28,396 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:28,397 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:28,397 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,397 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:28,397 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,397 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:28,398 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:28,399 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,399 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,400 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,400 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:28,401 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:28,401 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:28,401 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:28,402 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:28,402 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,402 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:28,402 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,402 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:28,403 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:28,414 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:28,421 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:28,429 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:28,438 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:28,439 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 05:26:28,443 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:28,443 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:28,444 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 05:26:28,444 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 1), torch.int64',)
2023-10-31 05:26:28,444 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:28,444 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 1), torch.int64',)
2023-10-31 05:26:28,444 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:28,445 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1), torch.int64',), {})
2023-10-31 05:26:28,445 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,446 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,447 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,447 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:28,447 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 05:26:28,448 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:28,452 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 05:26:28,452 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 05:26:28,452 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<Tensor>: (4, 38), torch.int64', '37')
2023-10-31 05:26:28,452 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:28,452 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<Tensor>: (1, 38), torch.int64', '37')
2023-10-31 05:26:28,453 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:28,453 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 38), torch.int64', '37'), {})
2023-10-31 05:26:28,454 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,454 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,455 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,456 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:28,457 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 05:26:28,457 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:28,461 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 05:26:28,465 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 05:26:28,465 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,465 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,465 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,465 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,467 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,473 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,477 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,482 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,485 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,485 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 05:26:28,487 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:28,490 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 05:26:28,494 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 05:26:28,494 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,494 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,495 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,495 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,497 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,515 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,527 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,531 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,534 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,534 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 05:26:28,536 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:28,540 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 05:26:28,543 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 05:26:28,543 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,544 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,544 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,544 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,546 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,551 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,556 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,568 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,571 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,571 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 05:26:28,573 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:28,577 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 05:26:28,581 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 05:26:28,581 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,581 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,581 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,581 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,583 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,588 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,592 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,598 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,601 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,601 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 05:26:28,602 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:28,607 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 05:26:28,610 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 05:26:28,611 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,611 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,611 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,611 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,613 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,618 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,623 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,627 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,630 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,630 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 05:26:28,631 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:28,635 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 05:26:28,639 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 05:26:28,639 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,639 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,640 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,640 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,642 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,647 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,652 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,656 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,659 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,659 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 05:26:28,660 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:28,665 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 05:26:28,669 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 05:26:28,669 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,669 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,669 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,669 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,671 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,676 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,684 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,688 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,691 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,691 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 05:26:28,693 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:28,697 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 05:26:28,701 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 05:26:28,701 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,701 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,701 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,702 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,704 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,708 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,712 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,717 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,719 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,719 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 05:26:28,720 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:28,725 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 05:26:28,728 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 05:26:28,728 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,729 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,729 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,729 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,731 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,736 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,740 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,744 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,764 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,765 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 05:26:28,766 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:28,770 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 05:26:28,774 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 05:26:28,774 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,774 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,774 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,774 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,777 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,781 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,785 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,790 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,792 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,792 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 05:26:28,794 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:28,798 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 05:26:28,802 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 05:26:28,802 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,802 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,803 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,803 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,805 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,810 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,814 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,818 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,821 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,821 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 05:26:28,823 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:28,823 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 05:26:28,827 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 05:26:28,827 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,827 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {'attention_mask': '<Tensor>: (4, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<BlockTensor>: (4, 12, 37, 64), torch.float32', '<BlockTensor>: (4, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,828 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,828 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {'attention_mask': '<Tensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 05:26:28,830 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {'attention_mask': '<MixTensor>: (1, 1, 1, 38), torch.float32', 'layer_head_mask': 'None', 'past_key_value': ('<MixTensor>: (1, 12, 37, 64), torch.float32', '<MixTensor>: (1, 12, 37, 64), torch.float32'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 05:26:28,835 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,839 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,845 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: ('<MixTensor>: (1, 1, 768), torch.float32', ('<MixTensor>: (1, 12, 38, 64), torch.float32', '<MixTensor>: (1, 12, 38, 64), torch.float32'))
2023-10-31 05:26:28,847 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: ('<BlockTensor>: (4, 1, 768), torch.float32', ('<BlockTensor>: (4, 12, 38, 64), torch.float32', '<BlockTensor>: (4, 12, 38, 64), torch.float32'))


2023-10-31 05:26:28,847 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 05:26:28,848 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:28,849 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 05:26:28,850 [wrapper.py:113 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 05:26:28,850 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,850 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:28,850 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,850 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:28,851 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:28,852 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,852 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,853 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 768), torch.float32
2023-10-31 05:26:28,853 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <BlockTensor>: (4, 1, 768), torch.float32


2023-10-31 05:26:28,854 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 05:26:28,854 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 05:26:28,854 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 05:26:28,855 [wrapper.py:113 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 05:26:28,855 [wrapper.py:114 in flexgen_forward] DEBUG - args: ('<BlockTensor>: (4, 1, 768), torch.float32',)
2023-10-31 05:26:28,855 [wrapper.py:115 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 05:26:28,855 [wrapper.py:119 in flexgen_forward] DEBUG - args_k: ('<MixTensor>: (1, 1, 768), torch.float32',)
2023-10-31 05:26:28,855 [wrapper.py:120 in flexgen_forward] DEBUG - kwarg_k: {}
2023-10-31 05:26:28,856 [wrapper.py:131 in flexgen_forward] DEBUG - last layer, batch: 3, as curr layer's input: (('<MixTensor>: (1, 1, 768), torch.float32',), {})
2023-10-31 05:26:28,864 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 0, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:28,872 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 1, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:28,880 [wrapper.py:126 in flexgen_forward] DEBUG - batch: 2, output: <MixTensor>: (1, 1, 50272), torch.float32
2023-10-31 05:26:28,889 [wrapper.py:160 in flexgen_forward] DEBUG - outputs after concat: <Tensor>: (4, 1, 50272), torch.float32


2023-10-31 05:26:28,891 [test.py:45 in test_hf_gen] INFO - for i in range(10):                               
2023-10-31 05:26:28,891 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 05:26:28,892 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-31 05:26:28,892 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 05:26:28,892 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 05:26:28,892 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 05:26:28,892 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-31 05:26:28,892 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 05:26:28,902 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-31 05:26:28,903 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-31 05:26:28,903 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-31 05:26:28,903 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-31 05:26:28,903 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-31 05:26:28,903 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-31 05:26:28,903 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-31 05:26:28,903 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-31 05:26:28,903 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-31 05:26:28,904 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-31 05:26:28,904 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-31 05:26:28,904 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-31 05:26:28,904 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-31 05:26:28,904 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-31 05:26:28,905 [wrapper.py:73 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-31 05:26:28,905 [wrapper.py:73 in layer_reset] DEBUG - lm_head from flexgen to old.
