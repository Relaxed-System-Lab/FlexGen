2023-10-08 04:36:39,988 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpgtn0dxsa
2023-10-08 04:36:39,988 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpgtn0dxsa/_remote_module_non_scriptable.py
2023-10-08 04:36:40,427 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-08 04:36:40,485 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-08 04:36:42,049 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-08 04:36:42,366 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-08 04:36:42,367 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-08 04:36:42,367 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-08 04:36:42,367 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-08 04:36:43,225 [flexgen_init.py:40 in policy_init] DEBUG - Got empty CausalLM: 'Salesforce/codegen-350M-mono' on meta device.
2023-10-08 04:36:43,237 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.wte, [0. 0. 1.], size_todo: 304283648
2023-10-08 04:36:43,237 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.drop, [0. 0. 1.], size_todo: 304283648
2023-10-08 04:36:43,238 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.0, [0.         0.06454052 0.93545948], size_todo: 291693568
2023-10-08 04:36:43,239 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.1, [0.         0.10814092 0.89185908], size_todo: 279103488
2023-10-08 04:36:43,239 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.2, [0.         0.13956973 0.86043027], size_todo: 266513408
2023-10-08 04:36:43,240 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.3, [0.         0.16329946 0.83670054], size_todo: 253923328
2023-10-08 04:36:43,241 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.4, [0.         0.18185045 0.81814955], size_todo: 241333248
2023-10-08 04:36:43,242 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.5, [0.         0.19675122 0.80324878], size_todo: 228743168
2023-10-08 04:36:43,242 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.6, [0.         0.20898262 0.79101738], size_todo: 216153088
2023-10-08 04:36:43,243 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.7, [0.       0.219203 0.780797], size_todo: 203563008
2023-10-08 04:36:43,244 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.8, [0.         0.22787062 0.77212938], size_todo: 190972928
2023-10-08 04:36:43,244 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.9, [0.         0.23531438 0.76468562], size_todo: 178382848
2023-10-08 04:36:43,245 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.10, [0.        0.2417764 0.7582236], size_todo: 165792768
2023-10-08 04:36:43,246 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.11, [0.         0.24743886 0.75256114], size_todo: 153202688
2023-10-08 04:36:43,247 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.12, [0.         0.25244154 0.74755846], size_todo: 140612608
2023-10-08 04:36:43,247 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.13, [0.         0.25689339 0.74310661], size_todo: 128022528
2023-10-08 04:36:43,248 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.14, [0.         0.26088064 0.73911936], size_todo: 115432448
2023-10-08 04:36:43,249 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.15, [0.         0.26447241 0.73552759], size_todo: 102842368
2023-10-08 04:36:43,249 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.16, [0.         0.26772477 0.73227523], size_todo: 90252288
2023-10-08 04:36:43,250 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.17, [0.         0.27068364 0.72931636], size_todo: 77662208
2023-10-08 04:36:43,251 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.18, [0.         0.27338705 0.72661295], size_todo: 65072128
2023-10-08 04:36:43,251 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.h.19, [0.         0.27586671 0.72413329], size_todo: 52482048
2023-10-08 04:36:43,252 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - transformer.ln_f, [0.         0.27586822 0.72413178], size_todo: 52480000
2023-10-08 04:36:43,252 [flexgen_init.py:216 in get_policy_weight_map] DEBUG - lm_head, [0.         0.23528213 0.76471787], size_todo: 0
2023-10-08 04:36:43,252 [flexgen_init.py:220 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-08 04:36:43,254 [flexgen_init.py:226 in get_policy_weight_map] INFO - CausalLM Salesforce/codegen-350M-mono is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.16 GiB (23.53%), Disk Mem 0.51 Gib (76.47%)
2023-10-08 04:36:43,293 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-08 04:36:43,475 [flexgen_init.py:123 in check_disk] INFO - [], []
2023-10-08 04:36:43,515 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/config.json HTTP/1.1" 200 0
2023-10-08 04:36:43,710 [flexgen_init.py:123 in check_disk] INFO - [], []
2023-10-08 04:36:43,724 [flexgen_init.py:73 in policy_init] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/Salesforce.codegen-350M-mono'
2023-10-08 04:36:43,776 [flexgen_init.py:85 in policy_init] INFO - model has been loaded by policy.
2023-10-08 04:36:43,776 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.wte to test forward
2023-10-08 04:36:43,776 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.drop to test forward
2023-10-08 04:36:43,776 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.0 to test forward
2023-10-08 04:36:43,777 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.1 to test forward
2023-10-08 04:36:43,777 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.2 to test forward
2023-10-08 04:36:43,777 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.3 to test forward
2023-10-08 04:36:43,777 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.4 to test forward
2023-10-08 04:36:43,778 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.5 to test forward
2023-10-08 04:36:43,778 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.6 to test forward
2023-10-08 04:36:43,778 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.7 to test forward
2023-10-08 04:36:43,778 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.8 to test forward
2023-10-08 04:36:43,779 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.9 to test forward
2023-10-08 04:36:43,779 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.10 to test forward
2023-10-08 04:36:43,779 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.11 to test forward
2023-10-08 04:36:43,779 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.12 to test forward
2023-10-08 04:36:43,779 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.13 to test forward
2023-10-08 04:36:43,780 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.14 to test forward
2023-10-08 04:36:43,780 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.15 to test forward
2023-10-08 04:36:43,780 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.16 to test forward
2023-10-08 04:36:43,780 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.17 to test forward
2023-10-08 04:36:43,781 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.18 to test forward
2023-10-08 04:36:43,781 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.h.19 to test forward
2023-10-08 04:36:43,781 [flexgen_forward.py:38 in to_test_forward] DEBUG - transformer.ln_f to test forward
2023-10-08 04:36:43,781 [flexgen_forward.py:38 in to_test_forward] DEBUG - lm_head to test forward
2023-10-08 04:36:43,949 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-08 04:36:44,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:44,053 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.wte forward pass:
2023-10-08 04:36:44,053 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:44,054 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:44,054 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.drop forward pass:
2023-10-08 04:36:44,054 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:44,054 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:44,057 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.0 forward pass:
2023-10-08 04:36:44,064 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:44,066 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:44,069 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.1 forward pass:
2023-10-08 04:36:44,074 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:44,076 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:44,078 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.2 forward pass:
2023-10-08 04:36:44,094 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:44,096 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:44,099 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.3 forward pass:
2023-10-08 04:36:44,105 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:44,106 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:44,109 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.4 forward pass:
2023-10-08 04:36:44,114 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:44,116 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:44,120 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.5 forward pass:
2023-10-08 04:36:44,124 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:44,126 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:44,129 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.6 forward pass:
2023-10-08 04:36:44,134 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:44,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:44,139 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.7 forward pass:
2023-10-08 04:36:44,144 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:44,145 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:44,148 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.8 forward pass:
2023-10-08 04:36:44,152 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:44,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:44,157 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.9 forward pass:
2023-10-08 04:36:44,162 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:44,163 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:44,167 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.10 forward pass:
2023-10-08 04:36:44,172 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:44,174 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:44,177 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.11 forward pass:
2023-10-08 04:36:44,181 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:44,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:44,186 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.12 forward pass:
2023-10-08 04:36:44,190 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:44,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:44,195 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.13 forward pass:
2023-10-08 04:36:44,199 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:44,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:44,204 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.14 forward pass:
2023-10-08 04:36:44,208 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:44,210 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:44,213 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.15 forward pass:
2023-10-08 04:36:44,218 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:44,219 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:44,222 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.16 forward pass:
2023-10-08 04:36:44,227 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:44,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:44,232 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.17 forward pass:
2023-10-08 04:36:44,236 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:44,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:44,241 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.18 forward pass:
2023-10-08 04:36:44,245 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:44,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:44,250 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.h.19 forward pass:
2023-10-08 04:36:44,254 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:44,256 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:44,257 [flexgen_forward.py:47 in new_forward] DEBUG - transformer.ln_f forward pass:
2023-10-08 04:36:44,257 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:44,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:44,259 [flexgen_forward.py:47 in new_forward] DEBUG - lm_head forward pass:
2023-10-08 04:36:44,268 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:44,274 [flexgen_test.py:33 in test_hf_gen] INFO - 0,
2023-10-08 04:36:44,274 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:36:44,284 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.wte from test to old.
2023-10-08 04:36:44,284 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.drop from test to old.
2023-10-08 04:36:44,284 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.0 from test to old.
2023-10-08 04:36:44,284 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.1 from test to old.
2023-10-08 04:36:44,284 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.2 from test to old.
2023-10-08 04:36:44,284 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.3 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.4 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.5 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.6 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.7 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.8 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.9 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.10 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.11 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.12 from test to old.
2023-10-08 04:36:44,285 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.13 from test to old.
2023-10-08 04:36:44,286 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.14 from test to old.
2023-10-08 04:36:44,286 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.15 from test to old.
2023-10-08 04:36:44,286 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.16 from test to old.
2023-10-08 04:36:44,286 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.17 from test to old.
2023-10-08 04:36:44,286 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.18 from test to old.
2023-10-08 04:36:44,286 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.h.19 from test to old.
2023-10-08 04:36:44,286 [flexgen_forward.py:27 in to_old_forward] DEBUG - transformer.ln_f from test to old.
2023-10-08 04:36:44,286 [flexgen_forward.py:27 in to_old_forward] DEBUG - lm_head from test to old.
2023-10-08 04:36:44,286 [flexgen_init.py:105 in policy_init] INFO - layer order: ['transformer.wte', 'transformer.drop', 'transformer.h.0', 'transformer.h.1', 'transformer.h.2', 'transformer.h.3', 'transformer.h.4', 'transformer.h.5', 'transformer.h.6', 'transformer.h.7', 'transformer.h.8', 'transformer.h.9', 'transformer.h.10', 'transformer.h.11', 'transformer.h.12', 'transformer.h.13', 'transformer.h.14', 'transformer.h.15', 'transformer.h.16', 'transformer.h.17', 'transformer.h.18', 'transformer.h.19', 'transformer.ln_f', 'lm_head']
2023-10-08 04:36:44,287 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.wte to flexgen forward
2023-10-08 04:36:44,287 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.drop to flexgen forward
2023-10-08 04:36:44,287 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.0 to flexgen forward
2023-10-08 04:36:44,287 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.1 to flexgen forward
2023-10-08 04:36:44,287 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.2 to flexgen forward
2023-10-08 04:36:44,287 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.3 to flexgen forward
2023-10-08 04:36:44,287 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.4 to flexgen forward
2023-10-08 04:36:44,287 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.5 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.6 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.7 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.8 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.9 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.10 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.11 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.12 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.13 to flexgen forward
2023-10-08 04:36:44,288 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.14 to flexgen forward
2023-10-08 04:36:44,289 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.15 to flexgen forward
2023-10-08 04:36:44,289 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.16 to flexgen forward
2023-10-08 04:36:44,289 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.17 to flexgen forward
2023-10-08 04:36:44,289 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.18 to flexgen forward
2023-10-08 04:36:44,289 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.h.19 to flexgen forward
2023-10-08 04:36:44,289 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - transformer.ln_f to flexgen forward
2023-10-08 04:36:44,289 [flexgen_forward.py:95 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-08 04:36:44,330 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /Salesforce/codegen-350M-mono/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-08 04:36:44,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:44,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:44,404 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 8]),)
2023-10-08 04:36:44,404 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:44,404 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 8]),)
2023-10-08 04:36:44,405 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:44,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:44,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:44,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:44,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:44,412 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 8, 1024])
2023-10-08 04:36:44,412 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 8, 1024])
2023-10-08 04:36:44,412 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:44,413 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:44,413 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:44,416 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 8, 1024]),)
2023-10-08 04:36:44,417 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:44,417 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 8, 1024]),)
2023-10-08 04:36:44,417 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:44,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:44,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:44,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:44,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:44,417 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 8, 1024])
2023-10-08 04:36:44,418 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 8, 1024])
2023-10-08 04:36:44,418 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:44,418 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:44,421 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:44,424 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,425 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,425 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,425 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:44,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:44,439 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:44,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:44,453 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,453 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,453 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:44,455 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:44,459 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:44,462 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,462 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,462 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,463 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:44,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:44,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:44,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:44,512 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,513 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,513 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:44,515 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:44,518 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:44,521 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,521 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,521 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,521 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:44,530 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:44,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:44,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:44,552 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,552 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,553 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:44,554 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:44,557 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:44,560 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,560 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,561 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,561 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:44,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:44,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:44,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:44,589 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,589 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,589 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:44,591 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:44,594 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:44,597 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,597 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,598 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,598 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,598 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:44,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:44,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:44,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:44,619 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,620 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,620 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:44,621 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:44,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:44,628 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,628 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,628 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,629 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:44,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:44,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:44,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:44,655 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,655 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,655 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:44,657 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:44,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:44,665 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,665 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,665 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,665 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:44,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:44,677 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:44,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:44,689 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,689 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,690 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:44,691 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:44,694 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:44,698 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,698 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,698 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,698 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:44,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:44,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:44,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:44,722 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,722 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,723 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:44,724 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:44,727 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:44,731 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,731 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,731 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,731 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,731 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:44,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:44,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:44,748 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:44,753 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,754 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,754 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:44,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:44,759 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:44,762 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,762 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,762 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,763 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,763 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:44,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:44,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:44,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:44,786 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,786 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,787 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:44,788 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:44,791 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:44,795 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,795 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,795 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,795 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:44,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:44,806 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:44,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:44,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:44,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:44,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:44,826 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,826 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,827 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,827 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:44,833 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:44,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:44,843 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:44,849 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,849 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:44,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:44,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:44,858 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,858 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,858 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,858 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:44,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:44,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:44,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:44,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,880 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,881 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:44,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:44,885 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:44,889 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,889 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,889 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,889 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:44,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:44,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:44,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:44,912 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,912 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,912 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:44,914 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:44,917 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:44,921 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,921 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,921 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,921 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:44,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:44,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:44,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:44,943 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,944 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,944 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:44,945 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:44,948 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:44,952 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,952 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,952 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,952 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:44,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:44,963 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:44,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:44,974 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:44,974 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:44,974 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:44,976 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:44,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:44,982 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:44,983 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,983 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:44,983 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:44,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:44,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:44,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:45,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:45,005 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:45,005 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:45,006 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:45,007 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:45,011 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:45,014 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,014 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,014 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,015 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:45,021 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:45,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:45,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:45,036 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:45,037 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:45,037 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:45,038 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:45,042 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:45,045 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,045 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,046 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,046 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:45,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:45,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:45,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:45,068 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:45,068 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:45,069 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:45,070 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:45,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:45,075 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,075 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([8, 1, 1, 8]), 'position_ids': torch.Size([8, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,075 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,075 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 8, 1024]), 'layer_past': None, 'attention_mask': torch.Size([2, 1, 1, 8]), 'position_ids': torch.Size([2, 8]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:45,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:45,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:45,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:45,098 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 8, 1024]), (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])))
2023-10-08 04:36:45,099 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 8, 1024]), (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])))
2023-10-08 04:36:45,099 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:45,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:45,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:45,102 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 8, 1024]),)
2023-10-08 04:36:45,102 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:45,103 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 8, 1024]),)
2023-10-08 04:36:45,103 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:45,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:45,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:45,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:45,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:45,104 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 8, 1024])
2023-10-08 04:36:45,104 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 8, 1024])
2023-10-08 04:36:45,104 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:45,104 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:45,105 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:45,106 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 8, 1024]),)
2023-10-08 04:36:45,106 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:45,106 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 8, 1024]),)
2023-10-08 04:36:45,106 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:45,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:45,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:45,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:45,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:45,156 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 8, 51200])
2023-10-08 04:36:45,159 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 8, 51200])
2023-10-08 04:36:45,159 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:45,165 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:45,166 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:45,166 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:45,167 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:45,167 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:45,167 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:45,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:45,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:45,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:45,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:45,169 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:45,169 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:45,169 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:45,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:45,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:45,174 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:45,174 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:45,174 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:45,174 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:45,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:45,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:45,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:45,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:45,176 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:45,176 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:45,176 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:45,176 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:45,180 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:45,184 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,184 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,184 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,185 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:45,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:45,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:45,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:45,204 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,205 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,205 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:45,206 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:45,209 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:45,213 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,213 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,213 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,213 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:45,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:45,222 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:45,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:45,231 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,232 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,232 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:45,234 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:45,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:45,240 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,240 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,240 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,240 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:45,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:45,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:45,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:45,257 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,258 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,258 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:45,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:45,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:45,266 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,266 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,266 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,266 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:45,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:45,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:45,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:45,283 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,284 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,284 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:45,286 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:45,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:45,292 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,292 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,292 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,292 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:45,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:45,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:45,306 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:45,310 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,310 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:45,312 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:45,315 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:45,319 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,319 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,319 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,319 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:45,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:45,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:45,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:45,337 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,337 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,338 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:45,339 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:45,343 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:45,346 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,346 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,346 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,346 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:45,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:45,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:45,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:45,363 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,364 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,364 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:45,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:45,369 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:45,372 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,372 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,373 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,373 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:45,378 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:45,382 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:45,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:45,391 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,391 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,391 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:45,393 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:45,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:45,400 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,400 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,400 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,400 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:45,405 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:45,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:45,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:45,418 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,418 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,418 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:45,420 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:45,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:45,427 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,427 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,427 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,427 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,427 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:45,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:45,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:45,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:45,445 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,445 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,446 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:45,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:45,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:45,454 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,454 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,454 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,454 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:45,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:45,464 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:45,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:45,472 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,472 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,472 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:45,474 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:45,477 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:45,481 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,481 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,481 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,481 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:45,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:45,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:45,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:45,498 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,499 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,499 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:45,501 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:45,504 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:45,508 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,508 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,508 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,508 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:45,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:45,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:45,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:45,526 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,527 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,527 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:45,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:45,532 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:45,535 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,536 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,536 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,536 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:45,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:45,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:45,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:45,553 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,554 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,554 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:45,556 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:45,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:45,562 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,562 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,563 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,563 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:45,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:45,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:45,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:45,644 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,645 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,645 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:45,647 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:45,650 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:45,653 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,654 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,654 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,654 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:45,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:45,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:45,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:45,674 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,675 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,675 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:45,677 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:45,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:45,684 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,684 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,684 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,684 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:45,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:45,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:45,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:45,703 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,704 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,704 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:45,705 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:45,709 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:45,712 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,712 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,713 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,713 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:45,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:45,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:45,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:45,734 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,734 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,735 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:45,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:45,739 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:45,743 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,743 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,743 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,743 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:45,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:45,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:45,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:45,761 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,762 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,762 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:45,764 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:45,768 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:45,769 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,769 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 8, 64]), torch.Size([8, 16, 8, 64])), 'attention_mask': torch.Size([8, 1, 1, 9]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,770 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 8, 64]), torch.Size([2, 16, 8, 64])), 'attention_mask': torch.Size([2, 1, 1, 9]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:45,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:45,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:45,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:45,789 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])))
2023-10-08 04:36:45,790 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])))
2023-10-08 04:36:45,791 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:45,792 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:45,794 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:45,795 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:45,796 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:45,796 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:45,796 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:45,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:45,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:45,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:45,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:45,797 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:45,798 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:45,798 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:45,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:45,799 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:45,800 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:45,800 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:45,800 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:45,800 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:45,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:45,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:45,820 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:45,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:45,839 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:45,840 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:45,840 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:45,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:45,847 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:45,847 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:45,847 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:45,847 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:45,847 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:45,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:45,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:45,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:45,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:45,848 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:45,848 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:45,848 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:45,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:45,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:45,852 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:45,853 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:45,853 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:45,853 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:45,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:45,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:45,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:45,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:45,854 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:45,854 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:45,854 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:45,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:45,857 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:45,860 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,860 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,860 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,860 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:45,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:45,870 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:45,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:45,878 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:45,879 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:45,879 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:45,881 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:45,884 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:45,888 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,888 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,888 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,888 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:45,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:45,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:45,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:45,907 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:45,907 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:45,907 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:45,909 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:45,913 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:45,916 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,916 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,917 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,917 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:45,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:45,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:45,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:45,937 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:45,937 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:45,937 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:45,939 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:45,942 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:45,945 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,946 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,946 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,946 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:45,951 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:45,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:45,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:45,964 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:45,964 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:45,965 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:45,966 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:45,970 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:45,973 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:45,973 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,974 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:45,974 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:45,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:45,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:45,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:45,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:45,992 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:45,993 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:45,993 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:45,995 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:45,999 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:46,003 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,003 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,003 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,003 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:46,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:46,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:46,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:46,023 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,023 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,024 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:46,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:46,029 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:46,033 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,033 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,033 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,033 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:46,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:46,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:46,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:46,052 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,052 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,052 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:46,054 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:46,057 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:46,060 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,061 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,061 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,061 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:46,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:46,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:46,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:46,080 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,080 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,081 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:46,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:46,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:46,089 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,089 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,090 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,090 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:46,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:46,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:46,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:46,108 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,109 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,109 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:46,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:46,114 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:46,118 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,118 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,118 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,118 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,118 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:46,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:46,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:46,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:46,137 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,138 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,138 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:46,140 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:46,143 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:46,147 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,147 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,147 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,147 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:46,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:46,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:46,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:46,166 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,167 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,167 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:46,169 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:46,172 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:46,176 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,176 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,176 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,176 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:46,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:46,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:46,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:46,196 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,196 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,196 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:46,198 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:46,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:46,205 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,205 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,205 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,205 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,206 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:46,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:46,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:46,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:46,223 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,224 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,224 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:46,226 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:46,229 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:46,233 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,233 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,233 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,233 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:46,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:46,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:46,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:46,251 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,252 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,252 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:46,253 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:46,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:46,260 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,260 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,261 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,261 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:46,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:46,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:46,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:46,279 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,280 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,280 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:46,282 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:46,285 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:46,289 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,289 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,289 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,289 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:46,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:46,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:46,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:46,307 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,307 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,308 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:46,310 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:46,313 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:46,317 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,317 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,317 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,317 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:46,326 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:46,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:46,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:46,349 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,350 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,350 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:46,353 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:46,357 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:46,360 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,361 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,361 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,361 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,361 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:46,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:46,370 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:46,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:46,379 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,379 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,380 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:46,381 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:46,385 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:46,388 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,388 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,389 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,389 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:46,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:46,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:46,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:46,408 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,409 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,409 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:46,411 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:46,416 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:46,417 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,417 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 9, 64]), torch.Size([8, 16, 9, 64])), 'attention_mask': torch.Size([8, 1, 1, 10]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,417 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,418 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 9, 64]), torch.Size([2, 16, 9, 64])), 'attention_mask': torch.Size([2, 1, 1, 10]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:46,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:46,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:46,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:46,437 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])))
2023-10-08 04:36:46,437 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])))
2023-10-08 04:36:46,437 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:46,439 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:46,441 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:46,442 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:46,442 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:46,442 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:46,442 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:46,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:46,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:46,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:46,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:46,443 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:46,443 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:46,444 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:46,444 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:46,446 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:46,446 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:46,447 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:46,447 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:46,447 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:46,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:46,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:46,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:46,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:46,486 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:46,487 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:46,487 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:46,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:46,494 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:46,495 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:46,495 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:46,495 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:46,495 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:46,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:46,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:46,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:46,496 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:46,497 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:46,497 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:46,497 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:46,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:46,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:46,502 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:46,502 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:46,502 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:46,502 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:46,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:46,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:46,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:46,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:46,504 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:46,504 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:46,504 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:46,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:46,508 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:46,511 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,512 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,512 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,512 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:46,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:46,521 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:46,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:46,530 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,531 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,531 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:46,533 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:46,536 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:46,539 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,539 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,540 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,540 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:46,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:46,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:46,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:46,557 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,558 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,558 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:46,560 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:46,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:46,567 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,567 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,567 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,568 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:46,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:46,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:46,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:46,587 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,587 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,588 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:46,589 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:46,592 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:46,596 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,596 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,596 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,596 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:46,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:46,606 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:46,611 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:46,615 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,615 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,616 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:46,617 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:46,621 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:46,624 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,624 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,625 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,625 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:46,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:46,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:46,639 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:46,644 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,644 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,644 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:46,646 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:46,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:46,653 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,653 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,653 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,654 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:46,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:46,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:46,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:46,674 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,675 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,675 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:46,677 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:46,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:46,684 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,684 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,684 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,685 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:46,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:46,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:46,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:46,703 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,704 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,704 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:46,706 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:46,709 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:46,713 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,713 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,714 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,714 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:46,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:46,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:46,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:46,735 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,736 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,736 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:46,738 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:46,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:46,746 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,746 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,746 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:46,752 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:46,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:46,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:46,765 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,766 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,766 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:46,768 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:46,771 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:46,775 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,775 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,775 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,775 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:46,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:46,784 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:46,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:46,793 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,793 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,793 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:46,795 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:46,799 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:46,802 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,803 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,803 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,803 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:46,808 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:46,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:46,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:46,820 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,821 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,821 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:46,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:46,826 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:46,830 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,830 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,830 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,830 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:46,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:46,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:46,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:46,848 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,849 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:46,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:46,854 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:46,858 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,858 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,858 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,858 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:46,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:46,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:46,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:46,875 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,876 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,876 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:46,878 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:46,881 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:46,885 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,885 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,885 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,885 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:46,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:46,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:46,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:46,902 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,903 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,903 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:46,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:46,908 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:46,911 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,911 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,912 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,912 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:46,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:46,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:46,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:46,931 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,931 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,931 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:46,933 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:46,936 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:46,940 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,940 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,940 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,941 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:46,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:46,951 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:46,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:46,960 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,961 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,961 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:46,963 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:46,966 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:46,969 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,969 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,969 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,970 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,970 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:46,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:46,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:46,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:46,988 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:46,989 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:46,989 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:46,991 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:46,994 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:46,998 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:46,998 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,998 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:46,998 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:46,999 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:47,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:47,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:47,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:47,019 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:47,020 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:47,020 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:47,022 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:47,025 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:47,029 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,029 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,029 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,029 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:47,034 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:47,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:47,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:47,046 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:47,047 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:47,047 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:47,049 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:47,053 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:47,054 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,054 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 10, 64]), torch.Size([8, 16, 10, 64])), 'attention_mask': torch.Size([8, 1, 1, 11]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,054 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,054 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 10, 64]), torch.Size([2, 16, 10, 64])), 'attention_mask': torch.Size([2, 1, 1, 11]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:47,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:47,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:47,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:47,072 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])))
2023-10-08 04:36:47,073 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])))
2023-10-08 04:36:47,073 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:47,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:47,076 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:47,077 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:47,077 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:47,078 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:47,078 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:47,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:47,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:47,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:47,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:47,079 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:47,079 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:47,079 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:47,079 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:47,080 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:47,081 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:47,081 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:47,081 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:47,081 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:47,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:47,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:47,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:47,115 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:47,124 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:47,125 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:47,125 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:47,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:47,133 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:47,133 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:47,133 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:47,133 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:47,133 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:47,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:47,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:47,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:47,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:47,135 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:47,135 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:47,135 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:47,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:47,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:47,140 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:47,140 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:47,140 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:47,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:47,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:47,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:47,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:47,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:47,141 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:47,141 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:47,141 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:47,141 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:47,144 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:47,148 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,148 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,148 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,148 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:47,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:47,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:47,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:47,166 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,166 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,166 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:47,168 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:47,171 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:47,175 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,175 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,175 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,176 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:47,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:47,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:47,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:47,194 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,194 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,195 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:47,196 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:47,199 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:47,203 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,203 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,203 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,203 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:47,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:47,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:47,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:47,223 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,223 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,223 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:47,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:47,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:47,232 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,232 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,232 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,232 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:47,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:47,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:47,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:47,251 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,252 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,252 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:47,253 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:47,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:47,260 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,260 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,260 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,261 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:47,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:47,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:47,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:47,280 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,281 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,281 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:47,283 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:47,286 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:47,289 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,289 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,290 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,290 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:47,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:47,300 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:47,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:47,309 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,310 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,310 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:47,311 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:47,315 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:47,318 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,318 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,319 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,319 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:47,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:47,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:47,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:47,338 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,338 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,339 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:47,340 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:47,344 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:47,350 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,351 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,351 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,352 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:47,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:47,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:47,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:47,372 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,372 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,372 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:47,374 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:47,377 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:47,381 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,381 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,381 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,381 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:47,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:47,391 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:47,395 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:47,399 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,400 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,400 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:47,402 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:47,405 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:47,408 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,408 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,409 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,409 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:47,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:47,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:47,422 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:47,426 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,427 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,427 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:47,429 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:47,432 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:47,435 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,435 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,436 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,436 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:47,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:47,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:47,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:47,453 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,454 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,454 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:47,456 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:47,459 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:47,462 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,463 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,463 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,463 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:47,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:47,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:47,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:47,481 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,482 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,482 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:47,483 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:47,486 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:47,489 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,490 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,490 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,490 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,490 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:47,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:47,500 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:47,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:47,508 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,509 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,509 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:47,511 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:47,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:47,517 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,517 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,517 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,517 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:47,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:47,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:47,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:47,540 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,540 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,540 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:47,542 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:47,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:47,548 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,548 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,549 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,549 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:47,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:47,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:47,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:47,576 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,576 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,576 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:47,578 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:47,581 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:47,585 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,585 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,585 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,585 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:47,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:47,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:47,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:47,603 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,603 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,603 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:47,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:47,608 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:47,611 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,612 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,612 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,612 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:47,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:47,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:47,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:47,631 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,631 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,631 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:47,633 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:47,636 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:47,639 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,639 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,640 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,640 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:47,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:47,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:47,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:47,657 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,658 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,658 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:47,660 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:47,663 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:47,666 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,666 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,667 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,667 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,667 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:47,672 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:47,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:47,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:47,685 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,686 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,686 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:47,688 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:47,691 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:47,691 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,692 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 11, 64]), torch.Size([8, 16, 11, 64])), 'attention_mask': torch.Size([8, 1, 1, 12]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,692 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,692 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 11, 64]), torch.Size([2, 16, 11, 64])), 'attention_mask': torch.Size([2, 1, 1, 12]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:47,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:47,701 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:47,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:47,709 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])))
2023-10-08 04:36:47,710 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])))
2023-10-08 04:36:47,710 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:47,712 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:47,713 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:47,714 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:47,714 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:47,714 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:47,714 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:47,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:47,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:47,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:47,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:47,715 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:47,715 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:47,715 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:47,716 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:47,717 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:47,718 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:47,718 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:47,718 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:47,718 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:47,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:47,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:47,739 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:47,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:47,759 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:47,760 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:47,760 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:47,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:47,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:47,768 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:47,768 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:47,768 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:47,768 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:47,768 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:47,768 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:47,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:47,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:47,769 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:47,769 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:47,770 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:47,770 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:47,770 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:47,774 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:47,775 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:47,775 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:47,775 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:47,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:47,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:47,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:47,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:47,776 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:47,776 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:47,776 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:47,776 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:47,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:47,782 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,782 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,783 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,783 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:47,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:47,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:47,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:47,801 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:47,801 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:47,801 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:47,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:47,806 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:47,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,810 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,810 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,810 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,810 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:47,815 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:47,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:47,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:47,827 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:47,828 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:47,828 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:47,830 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:47,833 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:47,836 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,837 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,837 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,837 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:47,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:47,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:47,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:47,854 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:47,855 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:47,855 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:47,856 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:47,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:47,863 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,863 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,864 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,864 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:47,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:47,872 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:47,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:47,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:47,881 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:47,881 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:47,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:47,886 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:47,889 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,889 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,890 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,890 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:47,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:47,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:47,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:47,908 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:47,909 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:47,909 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:47,910 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:47,914 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:47,917 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,917 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,918 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,918 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:47,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:47,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:47,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:47,935 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:47,935 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:47,935 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:47,937 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:47,940 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:47,943 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,944 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,944 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,944 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:47,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:47,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:47,957 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:47,961 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:47,962 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:47,962 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:47,964 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:47,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:47,971 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,971 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,971 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,971 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:47,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:47,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:47,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:47,988 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:47,988 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:47,989 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:47,990 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:47,993 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:47,997 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:47,997 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,997 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:47,997 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:47,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:48,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:48,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:48,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:48,014 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,015 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,015 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:48,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:48,020 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:48,023 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,023 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,023 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,023 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:48,028 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:48,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:48,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:48,043 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,043 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,044 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:48,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:48,048 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:48,052 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,052 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,052 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,052 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:48,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:48,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:48,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:48,069 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,070 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,070 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:48,072 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:48,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:48,079 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,079 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,079 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,079 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:48,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:48,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:48,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:48,097 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,098 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,098 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:48,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:48,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:48,107 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,107 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,107 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,107 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:48,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:48,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:48,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:48,125 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,125 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,126 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:48,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:48,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:48,133 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,134 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,134 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,134 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:48,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:48,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:48,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:48,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,153 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:48,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:48,157 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:48,161 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,161 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,161 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,161 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:48,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:48,170 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:48,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:48,179 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,180 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,180 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:48,181 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:48,185 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:48,188 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,188 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,188 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,188 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:48,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:48,197 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:48,202 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:48,206 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,206 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,206 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:48,208 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:48,211 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:48,215 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,215 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,215 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,215 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:48,220 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:48,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:48,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:48,234 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,234 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,235 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:48,236 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:48,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:48,243 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,243 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,243 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,243 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:48,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:48,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:48,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:48,260 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,260 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,261 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:48,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:48,265 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:48,269 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,269 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,269 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,269 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:48,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:48,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:48,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:48,287 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,287 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,288 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:48,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:48,292 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:48,293 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,294 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 12, 64]), torch.Size([8, 16, 12, 64])), 'attention_mask': torch.Size([8, 1, 1, 13]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,294 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,294 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 12, 64]), torch.Size([2, 16, 12, 64])), 'attention_mask': torch.Size([2, 1, 1, 13]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:48,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:48,303 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:48,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:48,312 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])))
2023-10-08 04:36:48,313 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])))
2023-10-08 04:36:48,313 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:48,315 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:48,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:48,317 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:48,317 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:48,317 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:48,317 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:48,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:48,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:48,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:48,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:48,318 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:48,318 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:48,318 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:48,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:48,319 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:48,320 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:48,320 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:48,320 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:48,320 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:48,320 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:48,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:48,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:48,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:48,358 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:48,359 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:48,359 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:48,365 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:48,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:48,366 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:48,366 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:48,366 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:48,366 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:48,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:48,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:48,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:48,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:48,367 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:48,367 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:48,367 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:48,368 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:48,368 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:48,372 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:48,372 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:48,372 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:48,372 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:48,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:48,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:48,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:48,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:48,373 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:48,373 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:48,373 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:48,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:48,376 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:48,379 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,380 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,380 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,380 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:48,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:48,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:48,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:48,431 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,432 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,433 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:48,434 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:48,438 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:48,441 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,442 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,442 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,442 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:48,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:48,453 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:48,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:48,461 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,462 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,463 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:48,465 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:48,468 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:48,472 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,472 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,472 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,473 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:48,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:48,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:48,488 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:48,493 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,494 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,494 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:48,496 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:48,499 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:48,503 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,503 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,503 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,504 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:48,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:48,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:48,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:48,522 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,522 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,522 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:48,524 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:48,527 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:48,531 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,531 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,531 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,531 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:48,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:48,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:48,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:48,549 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,550 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,550 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:48,552 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:48,555 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:48,558 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,559 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,559 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,559 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:48,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:48,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:48,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:48,578 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,578 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,578 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:48,580 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:48,583 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:48,586 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,587 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,587 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,587 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:48,592 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:48,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:48,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:48,605 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,606 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,606 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:48,608 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:48,611 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:48,614 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,614 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,615 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,615 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,615 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:48,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:48,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:48,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:48,645 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,646 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,646 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:48,647 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:48,651 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:48,654 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,654 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,654 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,655 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:48,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:48,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:48,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:48,686 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,687 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,687 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:48,690 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:48,694 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:48,697 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,697 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,697 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,698 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:48,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:48,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:48,712 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:48,717 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,718 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,718 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:48,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:48,723 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:48,726 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,727 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,727 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,727 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:48,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:48,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:48,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:48,747 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,748 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,748 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:48,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:48,753 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:48,756 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,756 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,756 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,756 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:48,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:48,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:48,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:48,774 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,775 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,775 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:48,777 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:48,780 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:48,783 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,784 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,784 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,784 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,784 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:48,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:48,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:48,798 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:48,802 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,803 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,803 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:48,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:48,808 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:48,811 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,811 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,812 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,812 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:48,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:48,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:48,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:48,830 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,830 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,830 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:48,832 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:48,835 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:48,839 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,839 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,839 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,839 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:48,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:48,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:48,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:48,857 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,857 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,857 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:48,859 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:48,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:48,865 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,865 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,866 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,866 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:48,870 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:48,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:48,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:48,884 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,885 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,885 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:48,887 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:48,890 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:48,893 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,894 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,894 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,894 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:48,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:48,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:48,907 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:48,911 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,912 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,912 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:48,913 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:48,917 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:48,920 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,920 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,920 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,920 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,921 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:48,926 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:48,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:48,934 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:48,938 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,938 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,938 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:48,940 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:48,943 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:48,947 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,947 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,947 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,947 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:48,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:48,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:48,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:48,965 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,966 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,966 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:48,968 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:48,971 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:48,972 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:48,972 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 13, 64]), torch.Size([8, 16, 13, 64])), 'attention_mask': torch.Size([8, 1, 1, 14]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,972 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:48,972 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 13, 64]), torch.Size([2, 16, 13, 64])), 'attention_mask': torch.Size([2, 1, 1, 14]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:48,972 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:48,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:48,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:48,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:48,995 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])))
2023-10-08 04:36:48,995 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])))
2023-10-08 04:36:48,996 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:48,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:48,998 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:48,999 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:48,999 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:48,999 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:49,000 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:49,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:49,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:49,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:49,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:49,001 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:49,001 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:49,001 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:49,001 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:49,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:49,003 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:49,003 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:49,003 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:49,003 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:49,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:49,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:49,022 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:49,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:49,040 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:49,041 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:49,041 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:49,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:49,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:49,048 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:49,048 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:49,048 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:49,048 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:49,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:49,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:49,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:49,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:49,050 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:49,050 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:49,050 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:49,050 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:49,051 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:49,054 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:49,055 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:49,055 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:49,055 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:49,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:49,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:49,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:49,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:49,055 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:49,056 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:49,056 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:49,056 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:49,058 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:49,061 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,062 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,062 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,062 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:49,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:49,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:49,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:49,080 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,080 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,080 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:49,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:49,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:49,089 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,089 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,089 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,089 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:49,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:49,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:49,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:49,107 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,108 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,108 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:49,109 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:49,112 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:49,116 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,116 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,116 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,116 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:49,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:49,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:49,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:49,133 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,134 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,134 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:49,135 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:49,138 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:49,141 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,142 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,142 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,142 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,142 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:49,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:49,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:49,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:49,160 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,160 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,160 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:49,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:49,165 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:49,169 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,169 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,169 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,169 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,169 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:49,174 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:49,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:49,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:49,187 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,187 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,187 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:49,189 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:49,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:49,196 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,196 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,196 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,196 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:49,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:49,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:49,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:49,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,215 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:49,217 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:49,220 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:49,223 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,223 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,223 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,224 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:49,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:49,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:49,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:49,242 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,242 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,243 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:49,244 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:49,248 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:49,251 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,251 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,252 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,252 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:49,257 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:49,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:49,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:49,270 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,271 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,271 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:49,273 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:49,276 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:49,280 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,280 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,280 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,280 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:49,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:49,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:49,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:49,299 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,300 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,300 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:49,301 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:49,305 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:49,308 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,308 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,308 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,308 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:49,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:49,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:49,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:49,326 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,327 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,327 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:49,328 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:49,332 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:49,335 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,335 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,336 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,336 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:49,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:49,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:49,349 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:49,353 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,354 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,354 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:49,356 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:49,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:49,363 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,363 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,363 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,363 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:49,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:49,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:49,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:49,381 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,381 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,381 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:49,383 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:49,386 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:49,389 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,390 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,390 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,390 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,390 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:49,395 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:49,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:49,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:49,408 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,408 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,408 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:49,410 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:49,413 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:49,417 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,417 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,417 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,418 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:49,422 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:49,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:49,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:49,436 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,437 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,437 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:49,438 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:49,442 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:49,445 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,446 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,446 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,446 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,446 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:49,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:49,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:49,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:49,489 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,490 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,490 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:49,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:49,495 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:49,498 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,499 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,499 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,499 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:49,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:49,508 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:49,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:49,516 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,517 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,517 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:49,519 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:49,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:49,526 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,526 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,526 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,527 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:49,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:49,536 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:49,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:49,544 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,544 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,545 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:49,546 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:49,549 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:49,553 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,553 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,553 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,553 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:49,559 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:49,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:49,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:49,572 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,573 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,573 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:49,575 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:49,578 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:49,581 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,581 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,582 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,582 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:49,587 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:49,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:49,596 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:49,600 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,601 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,601 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:49,603 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:49,606 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:49,607 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,607 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 14, 64]), torch.Size([8, 16, 14, 64])), 'attention_mask': torch.Size([8, 1, 1, 15]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,607 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,607 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 14, 64]), torch.Size([2, 16, 14, 64])), 'attention_mask': torch.Size([2, 1, 1, 15]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:49,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:49,617 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:49,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:49,626 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])))
2023-10-08 04:36:49,627 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])))
2023-10-08 04:36:49,627 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:49,628 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:49,629 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:49,630 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:49,630 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:49,631 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:49,631 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:49,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:49,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:49,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:49,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:49,632 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:49,632 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:49,632 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:49,632 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:49,633 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:49,634 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:49,634 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:49,634 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:49,634 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:49,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:49,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:49,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:49,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:49,674 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:49,674 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:49,674 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:49,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:49,681 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:49,681 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:49,681 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:49,682 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:49,682 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:49,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:49,682 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:49,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:49,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:49,683 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:49,683 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:49,683 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:49,684 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:49,684 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:49,688 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:49,688 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:49,688 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:49,688 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:49,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:49,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:49,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:49,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:49,689 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:49,689 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:49,689 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:49,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:49,692 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:49,696 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,696 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,696 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,696 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:49,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:49,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:49,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:49,724 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,725 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,725 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:49,726 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:49,729 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:49,733 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,733 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,733 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,733 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:49,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:49,743 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:49,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:49,752 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,752 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,752 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:49,754 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:49,757 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:49,761 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,761 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,761 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,761 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:49,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:49,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:49,775 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:49,779 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,779 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,780 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:49,781 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:49,784 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:49,788 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,788 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,788 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,788 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:49,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:49,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:49,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:49,806 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,807 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,807 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:49,809 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:49,812 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:49,815 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,816 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,816 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,816 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:49,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:49,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:49,829 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:49,834 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,835 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,835 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:49,836 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:49,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:49,843 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,843 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,844 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,844 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:49,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:49,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:49,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:49,861 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,862 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,862 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:49,864 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:49,867 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:49,870 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,870 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,870 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,870 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,870 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:49,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:49,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:49,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:49,888 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,889 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,889 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:49,890 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:49,893 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:49,897 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,897 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,897 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,897 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:49,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:49,907 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:49,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:49,916 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,916 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,916 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:49,918 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:49,921 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:49,925 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,925 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,925 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,925 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:49,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:49,934 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:49,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:49,945 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,945 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,945 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:49,947 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:49,950 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:49,953 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,953 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,954 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,954 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,954 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:49,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:49,963 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:49,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:49,972 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:49,972 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:49,973 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:49,974 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:49,977 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:49,981 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:49,981 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,981 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:49,981 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:49,981 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:49,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:49,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:49,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:50,000 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,001 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,001 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:50,002 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:50,006 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:50,009 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,009 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,009 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,010 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:50,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:50,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:50,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:50,027 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,028 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,028 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:50,029 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:50,033 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:50,036 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,036 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,036 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,037 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:50,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:50,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:50,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:50,055 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,055 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,055 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:50,057 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:50,060 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:50,064 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,064 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,064 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,064 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:50,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:50,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:50,078 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:50,083 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,083 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,084 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:50,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:50,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:50,092 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,092 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,092 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,092 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:50,097 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:50,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:50,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:50,112 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,112 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,112 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:50,114 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:50,117 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:50,121 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,121 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,121 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,121 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:50,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:50,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:50,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:50,149 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,149 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,150 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:50,151 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:50,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:50,158 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,158 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,158 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,159 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:50,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:50,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:50,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:50,177 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,178 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,178 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:50,180 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:50,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:50,186 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,186 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,186 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,186 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:50,191 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:50,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:50,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:50,205 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,205 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,205 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:50,207 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:50,210 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:50,213 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,214 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,214 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,214 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:50,220 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:50,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:50,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:50,235 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,236 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,236 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:50,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:50,241 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:50,242 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,242 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 15, 64]), torch.Size([8, 16, 15, 64])), 'attention_mask': torch.Size([8, 1, 1, 16]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,242 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,242 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 15, 64]), torch.Size([2, 16, 15, 64])), 'attention_mask': torch.Size([2, 1, 1, 16]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:50,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:50,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:50,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:50,264 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])))
2023-10-08 04:36:50,265 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])))
2023-10-08 04:36:50,265 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:50,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:50,268 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:50,269 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:50,269 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:50,269 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:50,269 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:50,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:50,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:50,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:50,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:50,270 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:50,270 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:50,271 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:50,271 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:50,272 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:50,272 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:50,273 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:50,273 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:50,273 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:50,273 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:50,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:50,292 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:50,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:50,310 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:50,310 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:50,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:50,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:50,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:50,317 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:50,317 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:50,317 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:50,318 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:50,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:50,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:50,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:50,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:50,319 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:50,319 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:50,319 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:50,319 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:50,319 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:50,323 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:50,323 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:50,323 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:50,323 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:50,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:50,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:50,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:50,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:50,324 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:50,324 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:50,324 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:50,324 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:50,327 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:50,331 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,331 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:50,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:50,340 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:50,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:50,349 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,349 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,349 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:50,351 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:50,354 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:50,357 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,357 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,357 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,357 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,357 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:50,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:50,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:50,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:50,375 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,376 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,376 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:50,377 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:50,381 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:50,384 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,384 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,384 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,384 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:50,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:50,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:50,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:50,401 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,402 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,402 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:50,404 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:50,407 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:50,410 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,411 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,411 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,411 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,411 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:50,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:50,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:50,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:50,429 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,429 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,429 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:50,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:50,434 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:50,437 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,437 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,438 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,438 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:50,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:50,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:50,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:50,455 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,456 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,456 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:50,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:50,461 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:50,464 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,464 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,464 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,464 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,465 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:50,469 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:50,473 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:50,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:50,482 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,484 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,484 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:50,485 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:50,488 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:50,491 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,492 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,492 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,492 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:50,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:50,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:50,506 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:50,518 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,519 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,519 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:50,520 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:50,523 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:50,527 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,527 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,527 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,527 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:50,532 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:50,537 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:50,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:50,545 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,546 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,546 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:50,547 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:50,550 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:50,553 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,553 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,554 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,554 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:50,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:50,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:50,567 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:50,571 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,572 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,572 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:50,573 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:50,577 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:50,580 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,580 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,580 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,580 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,580 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:50,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:50,590 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:50,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:50,599 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,599 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,599 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:50,601 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:50,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:50,608 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,609 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,609 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,609 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:50,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:50,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:50,623 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:50,628 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,628 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,629 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:50,630 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:50,634 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:50,637 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,637 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,637 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,637 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:50,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:50,646 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:50,651 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:50,656 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,656 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,657 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:50,658 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:50,661 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:50,665 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,665 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,665 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,666 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:50,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:50,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:50,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:50,684 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,684 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,684 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:50,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:50,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:50,692 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,692 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,693 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,693 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:50,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:50,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:50,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:50,711 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,711 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,711 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:50,713 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:50,716 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:50,719 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,720 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,720 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,720 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:50,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:50,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:50,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:50,737 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,738 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,738 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:50,740 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:50,743 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:50,747 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,747 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,747 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,747 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:50,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:50,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:50,762 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:50,766 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,767 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,767 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:50,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:50,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:50,776 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,776 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,776 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,776 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:50,782 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:50,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:50,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:50,795 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,795 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,796 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:50,797 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:50,800 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:50,803 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,804 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,804 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,804 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,804 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:50,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:50,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:50,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:50,823 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,823 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,824 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:50,825 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:50,828 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:50,831 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,832 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,832 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,832 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:50,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:50,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:50,846 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:50,850 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,850 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,851 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:50,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:50,856 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:50,856 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,857 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 16, 64]), torch.Size([8, 16, 16, 64])), 'attention_mask': torch.Size([8, 1, 1, 17]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,857 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,857 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 16, 64]), torch.Size([2, 16, 16, 64])), 'attention_mask': torch.Size([2, 1, 1, 17]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:50,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:50,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:50,870 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:50,874 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])))
2023-10-08 04:36:50,875 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])))
2023-10-08 04:36:50,875 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:50,877 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:50,878 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:50,878 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:50,879 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:50,879 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:50,879 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:50,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:50,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:50,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:50,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:50,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:50,880 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:50,880 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:50,880 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:50,881 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:50,882 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:50,882 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:50,882 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:50,882 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:50,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:50,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:50,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:50,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:50,919 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:50,920 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:50,920 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:50,926 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:50,927 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:50,927 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:50,927 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:50,927 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:50,927 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:50,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:50,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:50,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:50,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:50,930 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:50,930 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:50,930 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:50,930 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:50,931 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:50,934 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:50,934 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:50,934 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:50,935 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:50,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:50,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:50,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:50,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:50,935 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:50,935 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:50,935 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:50,936 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:50,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:50,941 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,942 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,942 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,942 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:50,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:50,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:50,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:50,960 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:50,960 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:50,961 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:50,962 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:50,966 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:50,969 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,969 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,969 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,969 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:50,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:50,978 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:50,982 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:50,986 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:50,987 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:50,987 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:50,988 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:50,991 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:50,995 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:50,995 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,995 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:50,995 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:50,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:51,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:51,005 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:51,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:51,013 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,013 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,013 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:51,015 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:51,018 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:51,022 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,022 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,022 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,022 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,022 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:51,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:51,031 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:51,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:51,039 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,040 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,040 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:51,042 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:51,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:51,048 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,049 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,049 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,049 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:51,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:51,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:51,063 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:51,067 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,068 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,068 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:51,070 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:51,073 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:51,077 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,077 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,077 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,077 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,077 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:51,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:51,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:51,091 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:51,096 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,096 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,096 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:51,098 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:51,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:51,105 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,105 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,105 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,105 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,106 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:51,110 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:51,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:51,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:51,122 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,123 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,123 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:51,125 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:51,128 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:51,131 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,131 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,131 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,131 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:51,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:51,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:51,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:51,149 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,149 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,149 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:51,151 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:51,154 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:51,157 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,158 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,158 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,158 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:51,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:51,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:51,171 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:51,175 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,176 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,176 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:51,178 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:51,181 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:51,184 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,184 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,184 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,184 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:51,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:51,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:51,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:51,205 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,206 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,206 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:51,208 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:51,211 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:51,215 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,215 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,216 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,216 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,216 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:51,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:51,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:51,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:51,234 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,235 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,235 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:51,237 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:51,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:51,244 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,244 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,244 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,244 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,244 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:51,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:51,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:51,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:51,263 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,263 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,264 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:51,265 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:51,269 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:51,272 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,272 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,273 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,273 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,273 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:51,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:51,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:51,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:51,411 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,420 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,420 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:51,422 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:51,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:51,430 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,430 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,430 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,431 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:51,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:51,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:51,449 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:51,456 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,457 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,457 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:51,460 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:51,463 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:51,467 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,467 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,468 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,468 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,468 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:51,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:51,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:51,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:51,490 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,491 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,491 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:51,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:51,497 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:51,500 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,500 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,501 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,501 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:51,506 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:51,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:51,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:51,519 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,520 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,520 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:51,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:51,525 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:51,529 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,529 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,529 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,529 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,530 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:51,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:51,585 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:51,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:51,600 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,604 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,604 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:51,606 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:51,609 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:51,613 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,613 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,614 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,614 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:51,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:51,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:51,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:51,650 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,652 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,653 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:51,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:51,659 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:51,662 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,662 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,663 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,663 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:51,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:51,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:51,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:51,684 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,685 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,686 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:51,688 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:51,692 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:51,694 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,694 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 17, 64]), torch.Size([8, 16, 17, 64])), 'attention_mask': torch.Size([8, 1, 1, 18]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,694 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,694 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 17, 64]), torch.Size([2, 16, 17, 64])), 'attention_mask': torch.Size([2, 1, 1, 18]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,695 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:51,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:51,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:51,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:51,715 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])))
2023-10-08 04:36:51,716 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])))
2023-10-08 04:36:51,716 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:51,718 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:51,719 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:51,720 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:51,721 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:51,721 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:51,721 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:51,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:51,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:51,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:51,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:51,723 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:51,723 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:51,723 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:51,724 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:51,725 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:51,726 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:51,726 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:51,726 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:51,726 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:51,727 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:51,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:51,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:51,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:51,778 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:51,779 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:51,779 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:51,786 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:51,787 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:51,787 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:51,787 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:51,787 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:51,787 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:51,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:51,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:51,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:51,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:51,788 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:51,788 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:51,789 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:51,789 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:51,789 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:51,793 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:51,793 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:51,793 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:51,794 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:51,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:51,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:51,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:51,794 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:51,794 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:51,794 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:51,795 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:51,795 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:51,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:51,801 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,802 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,802 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,802 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:51,808 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:51,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:51,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:51,825 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:51,826 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:51,826 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:51,828 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:51,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:51,834 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,835 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,835 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,835 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:51,840 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:51,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:51,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:51,854 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:51,855 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:51,855 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:51,857 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:51,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:51,864 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,864 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,864 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,864 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:51,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:51,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:51,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:51,882 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:51,883 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:51,883 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:51,885 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:51,888 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:51,891 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,892 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,892 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,892 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:51,897 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:51,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:51,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:51,909 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:51,910 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:51,910 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:51,912 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:51,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:51,918 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,918 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,918 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,919 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,919 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:51,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:51,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:51,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:51,937 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:51,938 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:51,938 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:51,940 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:51,943 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:51,946 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,946 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,947 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,947 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:51,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:51,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:51,960 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:51,964 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:51,965 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:51,965 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:51,967 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:51,970 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:51,973 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:51,974 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,974 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:51,974 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:51,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:51,978 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:51,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:51,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:51,991 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:51,992 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:51,992 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:51,993 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:51,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:52,000 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,000 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,000 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,000 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,001 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:52,005 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:52,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:52,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:52,017 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,018 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,018 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:52,020 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:52,023 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:52,027 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,027 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,027 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,027 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:52,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:52,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:52,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:52,044 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,045 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,045 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:52,046 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:52,049 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:52,053 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,053 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,053 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,053 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:52,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:52,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:52,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:52,072 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,072 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,073 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:52,074 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:52,077 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:52,081 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,081 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,081 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,081 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:52,086 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:52,090 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:52,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:52,099 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,099 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,099 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:52,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:52,104 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:52,108 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,108 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,108 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,108 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,109 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:52,114 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:52,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:52,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:52,129 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,130 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,130 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:52,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:52,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:52,139 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,139 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,139 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:52,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:52,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:52,156 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:52,162 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,162 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,163 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:52,164 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:52,168 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:52,171 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,172 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,172 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,172 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:52,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:52,183 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:52,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:52,194 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,195 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,195 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:52,197 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:52,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:52,204 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,204 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,204 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,205 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:52,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:52,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:52,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:52,225 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,226 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,227 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:52,228 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:52,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:52,236 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,236 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,236 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,236 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:52,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:52,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:52,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:52,255 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,256 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,256 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:52,258 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:52,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:52,265 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,266 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,266 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,266 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:52,271 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:52,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:52,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:52,285 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,285 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,285 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:52,287 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:52,290 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:52,294 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,294 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,294 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,294 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:52,300 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:52,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:52,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:52,314 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,315 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,315 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:52,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:52,320 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:52,324 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,324 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,324 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,325 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:52,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:52,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:52,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:52,343 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,344 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,344 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:52,346 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:52,349 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:52,350 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,350 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 18, 64]), torch.Size([8, 16, 18, 64])), 'attention_mask': torch.Size([8, 1, 1, 19]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,350 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,351 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 18, 64]), torch.Size([2, 16, 18, 64])), 'attention_mask': torch.Size([2, 1, 1, 19]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:52,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:52,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:52,364 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:52,368 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])))
2023-10-08 04:36:52,369 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])))
2023-10-08 04:36:52,369 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:52,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:52,372 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:52,373 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:52,373 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:52,373 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:52,373 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:52,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:52,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:52,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:52,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:52,374 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:52,374 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:52,374 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:52,374 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:52,375 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:52,376 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:52,376 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:52,376 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:52,376 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:52,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:52,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:52,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:52,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:52,417 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:52,417 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:52,418 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:52,423 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:52,424 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:52,424 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:52,424 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:52,424 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:52,425 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:52,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:52,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:52,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:52,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:52,425 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:52,426 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:52,426 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:52,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:52,426 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:52,430 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:52,430 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:52,430 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:52,430 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:52,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:52,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:52,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:52,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:52,431 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:52,431 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:52,431 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:52,431 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:52,434 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:52,437 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,438 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,438 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,438 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:52,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:52,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:52,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:52,456 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,457 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,457 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:52,458 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:52,462 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:52,465 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,465 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,465 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,466 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:52,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:52,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:52,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:52,483 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,484 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,484 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:52,486 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:52,489 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:52,493 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,493 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,493 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,494 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:52,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:52,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:52,507 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:52,512 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,512 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,512 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:52,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:52,517 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:52,521 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,521 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,521 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,521 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,521 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:52,526 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:52,530 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:52,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:52,539 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,540 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,540 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:52,542 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:52,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:52,548 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,549 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,549 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,549 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:52,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:52,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:52,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:52,568 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,568 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,568 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:52,570 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:52,573 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:52,577 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,577 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,577 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,577 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:52,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:52,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:52,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:52,595 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,595 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,596 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:52,597 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:52,600 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:52,604 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,604 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,604 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,604 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:52,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:52,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:52,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:52,623 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,623 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,623 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:52,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:52,628 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:52,631 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,631 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,631 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,632 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:52,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:52,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:52,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:52,651 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,652 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,652 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:52,654 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:52,657 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:52,661 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,661 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,661 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,661 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:52,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:52,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:52,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:52,679 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,680 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,680 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:52,682 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:52,685 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:52,688 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,688 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,689 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,689 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,689 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:52,694 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:52,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:52,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:52,708 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,708 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,709 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:52,710 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:52,714 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:52,717 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,717 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,718 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,718 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:52,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:52,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:52,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:52,737 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,737 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,737 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:52,739 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:52,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:52,746 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,746 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,746 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:52,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:52,758 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:52,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:52,769 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,770 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,770 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:52,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:52,775 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:52,778 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,779 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,779 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,779 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,779 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:52,785 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:52,790 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:52,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:52,801 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,802 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,802 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:52,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:52,806 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:52,810 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,810 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,810 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,810 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,810 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:52,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:52,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:52,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:52,834 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,834 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,834 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:52,836 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:52,839 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:52,843 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,843 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,843 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,843 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,843 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:52,849 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:52,854 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:52,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:52,862 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,863 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,863 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:52,864 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:52,867 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:52,871 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,871 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,871 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,871 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:52,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:52,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:52,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:52,888 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,889 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,889 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:52,891 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:52,894 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:52,897 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,898 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,898 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,898 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,898 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:52,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:52,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:52,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:52,916 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,917 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,917 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:52,919 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:52,923 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:52,927 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,927 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,927 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,927 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,927 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:52,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:52,936 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:52,941 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:52,945 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,945 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,945 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:52,947 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:52,950 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:52,954 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,954 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,954 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,954 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,954 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:52,959 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:52,963 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:52,968 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:52,972 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,973 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,973 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:52,975 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:52,978 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:52,979 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:52,979 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 19, 64]), torch.Size([8, 16, 19, 64])), 'attention_mask': torch.Size([8, 1, 1, 20]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,979 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:52,979 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 19, 64]), torch.Size([2, 16, 19, 64])), 'attention_mask': torch.Size([2, 1, 1, 20]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:52,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:52,984 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:52,988 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:52,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:52,997 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])))
2023-10-08 04:36:52,998 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])))
2023-10-08 04:36:52,998 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:53,000 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:53,001 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:53,002 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:53,002 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:53,002 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:53,002 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:53,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:53,003 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:53,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:53,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:53,004 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:53,004 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:53,004 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:53,005 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:53,006 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:53,006 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:53,006 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:53,006 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:53,007 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:53,007 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:53,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:53,026 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:53,036 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:53,046 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:53,046 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:53,046 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:53,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:53,053 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:53,053 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:53,053 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:53,054 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:53,054 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:53,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:53,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:53,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:53,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:53,055 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:53,055 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:53,055 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:53,055 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:53,055 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:53,059 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:53,059 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:53,059 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:53,059 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:53,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:53,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:53,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:53,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:53,060 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:53,060 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:53,060 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:53,060 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:53,064 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:53,067 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,067 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,067 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,068 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,068 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:53,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:53,077 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:53,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:53,085 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,086 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,086 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:53,088 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:53,091 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:53,094 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,095 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,095 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,095 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,095 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:53,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:53,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:53,109 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:53,113 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,114 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,114 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:53,117 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:53,121 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:53,125 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,125 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,125 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,125 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:53,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:53,135 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:53,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:53,143 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,144 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,144 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:53,146 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:53,149 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:53,152 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,152 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,153 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,153 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:53,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:53,166 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:53,171 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:53,175 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,176 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,176 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:53,178 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:53,181 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:53,185 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,185 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,185 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,185 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:53,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:53,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:53,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:53,203 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,204 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,204 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:53,205 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:53,209 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:53,212 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,212 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,212 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,213 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:53,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:53,222 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:53,226 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:53,230 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,231 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,231 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:53,233 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:53,236 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:53,239 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,240 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,240 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,240 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,240 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:53,245 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:53,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:53,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:53,258 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,258 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,259 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:53,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:53,264 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:53,267 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,267 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,267 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,267 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,268 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:53,272 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:53,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:53,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:53,285 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,286 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,286 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:53,288 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:53,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:53,294 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,294 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,295 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,295 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:53,300 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:53,305 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:53,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:53,314 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,314 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,315 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:53,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:53,319 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:53,323 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,323 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,323 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,324 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:53,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:53,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:53,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:53,342 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,343 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,343 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:53,345 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:53,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:53,351 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,352 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,352 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,352 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:53,357 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:53,361 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:53,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:53,370 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,371 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,372 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:53,373 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:53,377 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:53,380 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,380 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,380 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,380 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:53,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:53,392 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:53,402 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:53,406 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,407 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,407 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:53,408 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:53,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:53,415 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,415 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,416 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,416 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:53,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:53,425 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:53,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:53,446 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,447 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,448 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:53,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:53,454 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:53,457 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,457 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,457 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,458 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:53,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:53,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:53,471 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:53,476 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,476 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,477 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:53,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:53,482 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:53,486 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,487 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,487 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,487 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:53,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:53,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:53,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:53,507 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,507 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,508 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:53,510 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:53,513 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:53,517 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,517 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,517 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,517 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:53,522 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:53,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:53,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:53,535 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,535 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,536 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:53,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:53,540 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:53,544 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,544 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,544 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,544 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:53,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:53,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:53,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:53,561 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,562 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,562 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:53,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:53,567 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:53,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,570 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,570 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,570 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:53,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:53,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:53,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:53,587 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,588 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,588 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:53,589 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:53,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:53,596 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,596 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,596 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,597 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:53,601 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:53,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:53,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:53,614 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,614 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,614 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:53,616 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:53,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:53,620 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,620 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 20, 64]), torch.Size([8, 16, 20, 64])), 'attention_mask': torch.Size([8, 1, 1, 21]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,620 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,620 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 20, 64]), torch.Size([2, 16, 20, 64])), 'attention_mask': torch.Size([2, 1, 1, 21]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,621 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:53,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:53,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:53,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:53,637 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])))
2023-10-08 04:36:53,638 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])))
2023-10-08 04:36:53,638 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:53,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:53,641 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:53,641 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:53,642 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:53,642 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:53,642 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:53,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:53,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:53,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:53,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:53,643 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:53,643 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:53,643 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:53,643 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:53,644 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:53,645 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:53,645 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:53,645 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:53,645 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:53,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:53,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:53,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:53,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:53,683 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:53,684 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:53,684 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:53,691 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:53,691 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:53,691 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:53,692 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:53,692 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:53,692 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:53,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:53,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:53,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:53,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:53,693 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:53,693 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:53,693 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:53,693 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:53,694 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:53,697 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:53,697 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:53,698 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:53,698 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:53,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:53,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:53,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:53,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:53,698 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:53,698 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:53,699 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:53,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:53,701 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:53,705 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,705 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,705 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,705 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,706 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:53,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:53,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:53,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:53,723 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,723 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,724 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:53,725 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:53,729 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:53,732 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,732 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,733 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,733 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:53,738 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:53,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:53,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:53,750 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,751 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,751 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:53,752 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:53,756 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:53,759 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,759 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,760 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,760 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:53,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:53,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:53,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:53,777 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,778 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,778 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:53,779 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:53,782 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:53,786 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,786 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,786 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,787 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:53,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:53,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:53,800 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:53,804 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,805 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,805 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:53,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:53,810 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:53,813 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,814 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,814 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,814 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:53,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:53,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:53,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:53,832 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,832 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,833 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:53,834 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:53,838 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:53,841 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,841 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,842 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,842 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:53,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:53,851 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:53,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:53,860 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,860 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,860 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:53,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:53,865 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:53,869 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,869 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,869 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,869 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:53,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:53,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:53,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:53,887 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,888 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,888 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:53,890 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:53,893 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:53,897 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,897 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,897 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,897 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,897 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:53,902 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:53,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:53,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:53,915 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,915 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,916 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:53,917 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:53,920 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:53,924 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,925 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,925 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,925 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:53,930 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:53,934 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:53,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:53,943 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,944 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,944 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:53,945 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:53,948 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:53,951 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,952 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,952 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,952 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:53,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:53,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:53,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:53,970 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,971 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,971 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:53,973 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:53,976 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:53,980 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:53,980 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,980 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:53,980 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:53,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:53,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:53,989 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:53,993 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:53,998 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:53,998 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:53,998 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:54,000 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:54,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:54,007 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,007 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,007 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,007 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,007 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:54,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:54,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:54,038 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:54,042 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,043 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,043 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:54,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:54,048 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:54,052 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,052 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,052 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,052 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:54,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:54,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:54,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:54,070 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,070 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,070 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:54,072 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:54,076 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:54,079 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,079 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,079 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,079 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,080 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:54,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:54,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:54,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:54,097 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,098 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,098 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:54,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:54,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:54,107 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,107 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,107 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,107 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:54,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:54,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:54,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:54,125 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,125 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,126 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:54,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:54,130 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:54,134 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,134 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,134 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,134 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:54,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:54,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:54,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:54,153 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,154 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:54,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:54,158 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:54,162 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,162 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,162 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,162 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:54,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:54,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:54,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:54,181 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,181 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,181 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:54,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:54,186 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:54,189 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,190 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,190 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,190 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:54,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:54,200 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:54,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:54,209 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,209 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,209 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:54,211 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:54,214 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:54,218 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,218 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,218 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,218 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:54,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:54,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:54,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:54,236 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,236 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,237 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:54,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:54,241 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:54,242 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,243 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 21, 64]), torch.Size([8, 16, 21, 64])), 'attention_mask': torch.Size([8, 1, 1, 22]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,243 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,243 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 21, 64]), torch.Size([2, 16, 21, 64])), 'attention_mask': torch.Size([2, 1, 1, 22]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:54,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:54,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:54,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:54,260 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])))
2023-10-08 04:36:54,261 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])))
2023-10-08 04:36:54,261 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:54,263 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:54,264 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:54,265 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:54,265 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:54,265 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:54,265 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:54,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:54,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:54,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:54,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:54,266 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:54,266 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:54,267 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:54,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:54,268 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:54,268 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:54,268 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:54,269 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:54,269 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:54,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:54,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:54,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:54,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:54,308 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:54,309 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:54,309 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:54,315 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:54,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:54,316 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:54,316 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:54,316 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:54,316 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:54,316 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:54,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:54,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:54,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:54,317 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:54,317 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:54,318 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:54,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:54,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:54,321 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:54,322 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:54,322 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:54,322 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:54,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:54,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:54,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:54,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:54,323 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:54,323 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:54,323 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:54,323 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:54,326 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:54,330 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,330 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,330 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,330 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:54,336 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:54,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:54,345 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:54,349 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,350 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,350 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:54,352 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:54,355 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:54,359 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,359 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,359 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,359 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:54,364 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:54,369 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:54,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:54,378 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,379 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,379 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:54,381 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:54,384 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:54,388 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,388 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,388 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,388 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,388 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:54,393 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:54,397 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:54,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:54,405 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,406 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,406 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:54,408 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:54,411 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:54,414 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,415 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,415 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,415 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,415 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:54,420 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:54,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:54,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:54,434 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,434 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,434 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:54,436 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:54,439 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:54,443 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,443 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,443 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,443 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:54,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:54,478 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:54,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:54,489 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,490 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,490 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:54,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:54,496 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:54,500 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,501 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,501 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,502 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,502 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:54,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:54,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:54,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:54,528 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,529 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,530 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:54,532 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:54,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:54,542 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,542 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,543 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,543 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:54,549 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:54,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:54,558 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:54,562 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,563 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,563 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:54,565 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:54,569 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:54,572 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,572 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,573 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,573 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:54,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:54,582 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:54,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:54,590 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,591 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,591 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:54,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:54,596 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:54,599 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,600 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,600 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,600 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:54,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:54,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:54,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:54,619 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,619 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,620 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:54,621 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:54,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:54,629 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,629 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,629 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,629 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,629 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:54,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:54,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:54,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:54,647 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,647 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,648 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:54,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:54,653 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:54,656 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,656 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,657 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,657 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:54,670 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:54,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:54,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:54,683 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,684 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,684 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:54,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:54,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:54,692 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,692 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,693 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,693 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:54,698 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:54,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:54,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:54,711 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,712 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,712 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:54,713 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:54,717 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:54,720 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,720 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,720 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,720 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:54,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:54,729 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:54,734 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:54,737 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,738 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,738 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:54,740 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:54,743 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:54,746 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,746 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,746 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,746 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,747 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:54,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:54,755 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:54,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:54,764 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,764 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,764 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:54,766 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:54,769 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:54,772 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,772 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,772 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,772 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:54,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:54,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:54,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:54,790 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,791 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,791 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:54,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:54,796 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:54,799 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,799 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,800 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,800 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,800 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:54,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:54,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:54,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:54,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:54,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:54,822 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:54,825 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,826 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,826 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,826 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:54,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:54,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:54,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:54,843 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,844 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,844 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:54,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:54,849 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:54,852 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,853 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,853 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,853 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:54,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:54,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:54,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:54,870 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,871 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,871 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:54,872 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:54,875 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:54,878 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,879 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,879 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,879 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:54,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:54,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:54,893 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:54,897 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,898 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,898 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:54,900 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:54,903 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:54,904 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,904 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 22, 64]), torch.Size([8, 16, 22, 64])), 'attention_mask': torch.Size([8, 1, 1, 23]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,904 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,904 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 22, 64]), torch.Size([2, 16, 22, 64])), 'attention_mask': torch.Size([2, 1, 1, 23]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:54,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:54,913 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:54,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:54,924 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])))
2023-10-08 04:36:54,925 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])))
2023-10-08 04:36:54,925 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:54,926 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:54,927 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:54,928 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:54,928 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:54,928 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:54,929 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:54,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:54,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:54,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:54,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:54,930 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:54,930 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:54,930 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:54,930 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:54,931 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:54,932 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:54,932 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:54,932 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:54,932 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:54,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:54,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:54,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:54,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:54,972 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:54,973 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:54,973 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:54,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:54,979 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:54,979 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:54,980 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:54,980 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:54,980 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:54,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:54,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:54,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:54,981 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:54,981 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:54,981 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:54,981 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:54,981 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:54,981 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:54,985 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:54,985 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:54,985 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:54,985 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:54,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:54,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:54,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:54,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:54,986 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:54,986 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:54,986 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:54,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:54,989 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:54,992 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:54,993 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,993 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:54,993 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:54,993 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:54,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:55,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:55,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:55,010 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,011 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,011 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:55,013 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:55,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:55,020 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,020 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,020 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,020 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:55,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:55,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:55,034 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:55,037 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,038 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,038 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:55,040 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:55,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:55,046 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,046 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,046 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,046 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:55,051 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:55,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:55,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:55,063 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,063 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,064 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:55,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:55,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:55,071 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,072 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,072 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,072 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:55,077 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:55,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:55,085 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:55,089 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,090 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,090 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:55,091 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:55,094 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:55,098 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,098 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,098 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,098 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,098 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:55,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:55,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:55,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:55,116 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,117 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,117 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:55,118 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:55,121 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:55,125 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,125 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,125 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,125 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:55,130 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:55,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:55,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:55,143 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,143 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,144 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:55,145 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:55,148 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:55,151 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,152 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,152 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,152 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:55,157 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:55,161 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:55,165 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:55,170 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,170 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,171 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:55,172 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:55,175 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:55,179 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,179 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,180 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,180 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,180 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:55,185 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:55,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:55,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:55,198 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,200 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,200 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:55,202 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:55,205 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:55,209 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,209 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,209 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,209 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:55,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:55,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:55,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:55,227 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,228 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,228 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:55,229 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:55,233 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:55,236 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,236 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,236 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,237 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:55,241 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:55,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:55,250 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:55,255 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,255 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,255 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:55,257 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:55,260 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:55,264 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,264 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,264 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,264 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,264 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:55,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:55,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:55,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:55,283 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,284 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,284 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:55,286 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:55,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:55,292 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,293 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,293 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,293 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:55,298 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:55,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:55,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:55,311 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,311 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:55,313 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:55,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:55,319 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,319 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,320 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,320 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,320 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:55,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:55,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:55,333 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:55,337 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,338 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,338 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:55,339 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:55,342 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:55,346 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,346 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,346 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,346 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:55,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:55,356 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:55,360 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:55,364 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,365 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,365 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:55,367 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:55,370 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:55,374 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,374 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,374 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,374 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:55,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:55,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:55,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:55,393 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,394 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,394 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:55,396 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:55,399 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:55,402 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,403 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,403 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,403 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:55,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:55,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:55,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:55,422 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,422 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,422 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:55,424 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:55,428 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:55,431 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,432 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,432 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,432 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,432 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:55,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:55,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:55,445 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:55,451 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,451 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,451 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:55,453 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:55,456 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:55,459 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,459 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,459 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,460 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,460 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:55,464 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:55,469 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:55,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:55,477 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,477 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,478 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:55,479 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:55,482 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:55,485 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,486 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,486 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,486 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:55,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:55,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:55,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:55,511 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,512 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,512 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:55,514 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:55,517 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:55,517 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,518 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 23, 64]), torch.Size([8, 16, 23, 64])), 'attention_mask': torch.Size([8, 1, 1, 24]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,518 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,518 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 23, 64]), torch.Size([2, 16, 23, 64])), 'attention_mask': torch.Size([2, 1, 1, 24]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:55,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:55,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:55,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:55,535 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])))
2023-10-08 04:36:55,536 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])))
2023-10-08 04:36:55,536 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:55,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:55,538 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:55,539 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:55,539 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:55,539 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:55,539 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:55,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:55,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:55,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:55,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:55,540 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:55,540 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:55,540 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:55,541 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:55,542 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:55,542 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:55,542 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:55,542 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:55,542 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:55,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:55,553 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:55,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:55,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:55,580 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:55,580 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:55,580 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:55,586 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:55,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:55,587 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:55,587 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:55,588 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:55,588 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:55,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:55,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:55,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:55,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:55,589 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:55,589 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:55,589 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:55,589 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:55,589 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:55,592 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:55,592 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:55,593 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:55,593 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:55,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:55,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:55,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:55,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:55,593 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:55,593 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:55,593 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:55,594 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:55,596 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:55,600 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,600 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,600 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,600 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:55,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:55,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:55,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:55,618 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,618 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,619 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:55,620 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:55,623 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:55,627 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,627 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,627 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,627 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:55,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:55,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:55,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:55,644 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,644 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,644 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:55,646 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:55,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:55,652 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,652 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,652 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,652 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:55,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:55,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:55,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:55,669 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,670 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,670 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:55,672 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:55,675 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:55,678 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,678 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,679 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,679 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:55,684 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:55,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:55,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:55,697 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,698 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,698 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:55,700 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:55,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:55,707 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,707 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,707 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,707 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:55,714 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:55,719 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:55,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:55,727 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,728 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,728 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:55,729 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:55,732 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:55,736 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,736 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,736 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,736 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,736 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:55,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:55,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:55,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:55,754 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,755 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,755 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:55,757 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:55,760 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:55,763 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,763 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,763 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,764 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:55,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:55,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:55,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:55,781 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,782 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,782 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:55,784 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:55,787 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:55,791 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,791 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,792 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,792 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:55,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:55,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:55,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:55,811 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,812 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,813 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:55,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:55,817 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:55,820 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,821 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,821 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,821 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,821 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:55,826 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:55,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:55,836 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:55,840 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,841 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,841 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:55,843 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:55,846 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:55,849 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,849 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,850 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,850 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,850 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:55,855 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:55,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:55,864 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:55,869 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,871 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,871 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:55,873 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:55,876 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:55,880 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,880 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,880 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,880 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:55,886 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:55,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:55,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:55,899 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,900 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,900 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:55,902 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:55,905 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:55,908 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,908 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,909 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,909 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:55,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:55,918 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:55,922 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:55,927 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,928 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,928 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:55,930 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:55,933 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:55,936 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,936 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,936 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,937 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:55,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:55,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:55,951 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:55,955 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,955 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,956 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:55,957 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:55,960 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:55,963 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,964 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,964 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,964 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:55,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:55,973 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:55,977 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:55,982 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:55,982 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:55,983 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:55,984 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:55,987 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:55,991 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:55,991 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,991 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:55,991 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:55,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:55,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:56,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:56,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:56,008 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:56,009 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:56,009 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:56,011 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:56,014 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:56,017 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,017 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,018 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,018 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,018 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:56,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:56,027 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:56,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:56,036 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:56,036 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:56,036 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:56,038 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:56,041 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:56,044 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,045 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,045 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,045 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:56,049 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:56,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:56,058 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:56,062 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:56,063 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:56,063 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:56,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:56,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:56,071 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,071 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,071 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,072 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,072 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:56,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:56,081 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:56,086 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:56,090 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:56,090 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:56,090 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:56,092 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:56,095 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:56,099 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,099 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,099 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:56,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:56,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:56,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:56,117 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:56,118 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:56,118 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:56,120 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:56,123 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:56,124 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,124 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 24, 64]), torch.Size([8, 16, 24, 64])), 'attention_mask': torch.Size([8, 1, 1, 25]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,124 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,124 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 24, 64]), torch.Size([2, 16, 24, 64])), 'attention_mask': torch.Size([2, 1, 1, 25]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,124 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:56,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:56,135 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:56,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:56,144 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])))
2023-10-08 04:36:56,145 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])))
2023-10-08 04:36:56,145 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:56,146 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:56,147 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:56,148 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:56,149 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:56,149 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:56,149 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:56,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:56,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:56,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:56,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:56,150 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:56,150 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:56,150 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:56,150 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:56,151 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:56,152 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:56,152 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:56,152 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:56,152 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:56,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:56,163 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:56,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:56,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:56,216 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:56,217 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:56,217 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:56,224 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:56,225 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:56,225 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:56,226 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:56,226 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:56,226 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:56,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:56,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:56,227 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:56,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:56,228 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:56,228 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:56,229 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:56,229 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:56,230 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:56,234 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:56,234 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:56,234 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:56,234 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:56,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:56,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:56,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:56,235 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:56,235 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:56,235 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:56,235 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:56,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:56,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:56,241 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,241 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,242 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,242 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:56,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:56,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:56,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:56,260 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,261 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,261 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:56,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:56,265 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:56,269 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,269 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,269 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,269 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,269 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:56,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:56,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:56,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:56,287 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,288 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,288 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:56,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:56,293 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:56,296 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,296 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,297 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,297 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:56,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:56,306 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:56,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:56,315 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,316 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,316 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:56,318 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:56,321 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:56,324 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,325 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,325 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,325 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,325 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:56,330 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:56,335 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:56,339 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:56,343 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,344 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,344 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:56,345 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:56,349 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:56,352 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,352 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,353 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,353 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:56,358 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:56,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:56,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:56,371 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,372 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,372 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:56,374 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:56,377 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:56,381 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,381 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,381 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,381 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,381 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:56,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:56,390 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:56,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:56,399 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,399 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,399 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:56,401 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:56,405 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:56,408 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,409 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,409 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,409 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:56,414 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:56,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:56,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:56,427 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,428 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,428 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:56,430 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:56,433 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:56,436 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,437 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,437 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,437 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:56,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:56,447 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:56,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:56,456 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,457 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,457 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:56,459 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:56,462 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:56,465 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,465 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,466 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,466 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:56,470 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:56,475 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:56,479 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:56,484 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,484 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,485 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:56,486 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:56,489 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:56,493 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,493 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,493 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,493 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:56,499 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:56,503 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:56,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:56,514 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,515 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,515 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:56,517 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:56,520 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:56,524 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,524 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,524 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,524 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:56,529 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:56,533 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:56,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:56,542 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,543 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,543 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:56,545 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:56,548 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:56,551 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,551 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,552 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,552 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,552 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:56,557 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:56,561 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:56,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:56,571 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,571 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,572 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:56,573 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:56,576 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:56,580 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,580 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,580 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,581 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,581 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:56,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:56,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:56,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:56,599 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,600 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,600 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:56,602 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:56,605 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:56,608 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,609 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,609 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,609 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:56,614 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:56,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:56,623 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:56,627 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,628 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,628 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:56,630 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:56,634 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:56,637 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,637 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,637 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,637 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:56,642 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:56,647 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:56,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:56,657 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,658 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,659 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:56,660 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:56,664 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:56,668 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,669 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,669 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,669 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:56,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:56,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:56,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:56,687 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,688 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,688 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:56,689 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:56,693 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:56,696 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,697 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,697 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,697 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:56,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:56,708 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:56,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:56,718 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,718 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,718 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:56,720 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:56,724 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:56,728 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,728 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,728 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,728 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:56,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:56,737 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:56,742 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:56,746 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,747 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,747 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:56,749 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:56,752 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:56,756 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,756 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,756 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,757 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:56,762 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:56,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:56,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:56,775 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,776 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,777 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:56,778 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:56,782 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:56,783 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,783 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 25, 64]), torch.Size([8, 16, 25, 64])), 'attention_mask': torch.Size([8, 1, 1, 26]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,783 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,783 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 25, 64]), torch.Size([2, 16, 25, 64])), 'attention_mask': torch.Size([2, 1, 1, 26]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:56,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:56,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:56,798 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:56,803 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])))
2023-10-08 04:36:56,804 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])))
2023-10-08 04:36:56,804 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:56,806 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:56,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:56,808 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:56,808 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:56,808 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:56,808 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:56,808 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:56,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:56,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:56,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:56,812 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:56,813 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:56,813 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:56,813 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:56,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:56,815 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:56,815 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:56,815 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:56,815 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:56,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:56,825 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:56,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:56,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:56,853 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:56,854 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:56,854 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:56,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:56,860 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:56,861 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:56,861 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:56,861 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:56,861 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:56,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:56,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:56,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:56,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:56,862 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:56,862 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:56,862 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:56,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:56,863 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:56,866 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:56,866 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:56,867 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:56,867 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:56,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:56,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:56,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:56,867 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:56,867 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:56,867 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:56,868 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:56,868 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:56,871 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:56,874 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,875 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,875 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,875 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,875 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:56,880 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:56,885 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:56,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:56,893 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:56,894 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:56,894 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:56,896 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:56,899 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:56,902 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,903 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,903 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,903 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,903 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:56,908 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:56,912 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:56,917 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:56,921 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:56,922 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:56,922 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:56,923 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:56,927 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:56,930 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,930 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,930 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,930 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,931 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:56,935 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:56,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:56,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:56,948 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:56,949 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:56,949 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:56,950 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:56,954 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:56,957 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,957 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,958 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,958 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,958 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:56,962 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:56,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:56,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:56,976 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:56,976 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:56,976 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:56,978 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:56,981 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:56,985 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:56,985 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,985 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:56,985 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:56,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:56,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:56,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:56,999 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:57,003 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,004 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,004 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:57,005 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:57,009 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:57,012 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,012 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,013 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,013 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,013 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:57,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:57,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:57,030 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:57,035 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,036 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,036 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:57,038 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:57,041 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:57,045 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,045 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,045 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,045 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,046 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:57,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:57,055 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:57,059 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:57,064 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,064 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,064 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:57,066 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:57,069 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:57,073 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,073 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,074 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,074 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:57,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:57,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:57,088 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:57,092 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,093 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,093 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:57,095 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:57,098 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:57,102 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,102 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,102 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,102 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:57,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:57,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:57,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:57,122 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,122 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,122 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:57,124 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:57,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:57,131 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,131 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,131 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,131 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:57,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:57,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:57,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:57,149 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,150 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,150 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:57,152 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:57,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:57,159 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,159 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,159 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,159 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:57,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:57,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:57,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:57,177 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,178 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,178 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:57,180 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:57,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:57,187 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,187 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,187 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,188 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,188 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:57,193 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:57,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:57,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:57,205 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,205 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,206 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:57,207 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:57,210 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:57,214 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,214 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,214 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,215 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,215 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:57,220 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:57,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:57,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:57,233 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,233 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,233 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:57,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:57,238 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:57,242 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,242 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,242 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,242 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,242 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:57,247 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:57,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:57,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:57,260 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,261 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,261 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:57,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:57,266 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:57,269 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,269 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,270 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,270 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:57,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:57,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:57,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:57,288 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,289 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,289 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:57,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:57,294 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:57,298 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,298 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,298 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,299 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:57,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:57,308 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:57,313 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:57,317 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,318 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,318 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:57,320 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:57,323 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:57,327 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,327 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,327 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,327 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,327 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:57,332 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:57,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:57,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:57,346 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,347 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,347 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:57,349 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:57,352 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:57,356 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,356 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,356 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,356 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,356 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:57,362 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:57,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:57,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:57,376 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,377 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,377 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:57,379 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:57,382 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:57,386 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,386 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,386 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,386 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,386 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:57,391 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:57,396 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:57,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:57,404 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,406 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,406 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:57,408 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:57,411 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:57,412 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,412 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 26, 64]), torch.Size([8, 16, 26, 64])), 'attention_mask': torch.Size([8, 1, 1, 27]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,412 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,412 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 26, 64]), torch.Size([2, 16, 26, 64])), 'attention_mask': torch.Size([2, 1, 1, 27]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,413 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:57,419 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:57,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:57,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:57,435 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])))
2023-10-08 04:36:57,437 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])))
2023-10-08 04:36:57,437 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:57,438 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:57,440 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:57,441 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:57,441 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:57,441 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:57,441 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:57,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:57,441 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:57,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:57,442 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:57,442 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:57,442 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:57,442 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:57,443 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:57,443 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:57,444 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:57,444 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:57,444 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:57,444 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:57,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:57,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:57,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:57,476 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:57,485 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:57,486 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:57,486 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:57,492 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:57,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:57,493 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:57,493 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:57,494 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:57,494 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:57,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:57,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:57,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:57,495 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:57,495 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:57,495 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:57,496 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:57,496 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:57,496 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:57,500 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:57,500 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:57,500 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:57,501 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:57,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:57,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:57,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:57,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:57,502 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:57,502 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:57,502 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:57,502 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:57,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:57,509 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,509 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,509 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,510 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:57,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:57,520 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:57,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:57,528 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,529 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,529 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:57,531 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:57,534 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:57,538 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,538 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,538 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,538 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,538 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:57,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:57,547 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:57,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:57,555 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,556 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,556 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:57,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:57,562 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:57,565 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,565 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,566 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,566 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:57,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:57,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:57,580 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:57,584 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,585 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,585 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:57,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:57,590 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:57,594 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,594 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,594 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,594 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:57,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:57,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:57,607 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:57,611 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,612 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,612 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:57,614 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:57,617 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:57,620 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,620 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,621 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,621 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,621 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:57,625 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:57,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:57,634 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:57,638 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,639 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,639 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:57,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:57,644 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:57,647 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,647 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,648 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,648 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,648 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:57,653 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:57,657 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:57,662 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:57,666 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,666 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,666 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:57,668 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:57,672 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:57,675 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,675 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,676 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,676 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,676 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:57,681 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:57,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:57,700 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:57,705 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,705 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,706 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:57,707 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:57,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:57,714 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,715 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,715 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,715 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:57,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:57,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:57,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:57,734 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,735 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,735 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:57,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:57,740 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:57,743 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,743 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,744 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,744 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,744 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:57,749 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:57,753 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:57,757 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:57,762 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,762 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,762 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:57,764 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:57,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:57,771 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,771 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,772 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,772 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,772 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:57,777 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:57,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:57,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:57,790 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,790 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,791 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:57,792 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:57,796 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:57,799 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,799 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,799 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,800 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,800 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:57,805 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:57,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:57,813 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:57,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,818 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:57,820 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:57,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:57,827 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,827 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,827 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,827 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:57,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:57,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:57,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:57,845 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,846 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,846 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:57,848 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:57,851 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:57,855 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,855 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,856 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,856 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:57,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:57,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:57,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:57,874 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,875 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,875 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:57,876 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:57,879 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:57,883 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,883 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,883 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,883 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:57,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:57,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:57,897 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:57,901 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,902 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,902 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:57,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:57,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:57,910 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,911 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,911 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,911 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:57,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:57,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:57,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:57,929 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,929 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,929 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:57,931 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:57,934 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:57,938 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,938 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,938 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,938 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:57,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:57,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:57,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:57,956 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,958 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,958 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:57,959 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:57,963 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:57,966 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,966 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,966 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,967 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:57,972 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:57,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:57,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:57,985 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:57,986 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:57,986 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:57,987 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:57,991 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:57,994 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:57,994 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,995 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:57,995 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:57,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:58,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:58,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:58,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:58,014 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:58,014 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:58,015 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:58,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:58,020 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:58,023 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,023 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,023 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,024 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,024 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:58,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:58,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:58,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:58,041 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:58,041 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:58,042 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:58,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:58,046 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:58,047 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,047 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 27, 64]), torch.Size([8, 16, 27, 64])), 'attention_mask': torch.Size([8, 1, 1, 28]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,048 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,048 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 27, 64]), torch.Size([2, 16, 27, 64])), 'attention_mask': torch.Size([2, 1, 1, 28]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:58,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:58,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:58,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:58,066 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])))
2023-10-08 04:36:58,066 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])))
2023-10-08 04:36:58,067 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:58,068 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:58,069 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:58,070 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:58,070 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:58,070 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:58,070 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:58,070 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:58,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:58,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:58,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:58,071 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:58,072 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:58,072 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:58,072 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:58,073 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:58,073 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:58,074 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:58,074 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:58,074 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:58,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:58,083 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:58,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:58,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:58,111 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:58,112 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:58,112 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:58,118 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:58,119 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:58,119 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:58,119 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:58,119 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:58,119 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:58,119 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:58,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:58,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:58,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:58,120 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:58,120 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:58,120 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:58,121 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:58,121 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:58,125 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:58,125 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:58,125 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:58,125 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:58,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:58,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:58,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:58,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:58,126 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:58,126 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:58,126 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:58,126 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:58,129 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:58,132 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,133 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,133 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,133 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,133 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:58,138 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:58,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:58,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:58,161 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,162 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,162 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:58,165 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:58,170 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:58,176 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,176 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,176 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,177 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:58,184 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:58,189 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:58,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:58,201 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,202 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,202 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:58,205 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:58,208 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:58,212 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,212 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,212 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,212 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,213 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:58,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:58,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:58,230 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:58,236 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,237 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,237 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:58,240 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:58,243 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:58,247 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,247 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,247 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,247 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:58,254 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:58,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:58,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:58,271 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,272 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,272 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:58,274 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:58,278 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:58,281 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,281 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,281 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,282 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,282 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:58,288 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:58,292 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:58,296 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:58,300 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,301 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,301 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:58,303 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:58,306 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:58,309 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,309 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,310 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,310 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,310 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:58,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:58,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:58,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:58,328 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,329 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,329 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:58,330 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:58,334 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:58,337 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,337 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,337 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,338 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:58,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:58,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:58,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:58,356 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,357 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:58,358 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:58,361 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:58,365 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,365 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,365 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,365 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,365 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:58,370 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:58,374 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:58,379 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:58,386 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,408 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,408 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:58,410 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:58,413 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:58,417 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,417 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,418 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,418 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:58,424 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:58,429 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:58,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:58,439 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,439 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,440 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:58,441 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:58,446 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:58,450 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,450 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,451 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,451 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,451 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:58,456 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:58,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:58,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:58,472 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,473 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,473 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:58,475 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:58,478 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:58,482 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,482 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,483 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,483 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,483 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:58,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:58,493 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:58,498 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:58,502 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,503 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,504 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:58,506 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:58,509 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:58,513 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,513 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,513 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,513 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,513 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:58,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:58,523 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:58,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:58,531 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,532 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,532 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:58,534 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:58,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:58,540 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,540 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,540 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,541 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:58,545 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:58,550 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:58,554 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:58,561 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,561 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,561 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:58,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:58,566 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:58,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,570 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,570 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,570 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:58,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:58,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:58,584 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:58,588 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,588 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,588 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:58,590 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:58,593 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:58,597 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,597 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,597 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,597 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:58,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:58,607 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:58,612 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:58,616 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,617 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,617 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:58,619 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:58,622 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:58,626 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,626 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,626 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,626 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:58,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:58,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:58,643 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:58,647 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,648 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,648 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:58,649 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:58,655 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:58,659 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,659 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,659 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,659 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:58,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:58,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:58,673 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:58,678 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,678 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,679 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:58,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:58,684 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:58,687 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,687 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,687 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,688 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:58,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:58,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:58,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:58,706 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,706 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,707 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:58,708 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:58,712 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:58,715 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,715 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,716 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,716 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,716 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:58,721 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:58,725 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:58,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:58,734 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,735 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,735 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:58,736 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:58,740 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:58,741 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,741 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 28, 64]), torch.Size([8, 16, 28, 64])), 'attention_mask': torch.Size([8, 1, 1, 29]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,741 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,741 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 28, 64]), torch.Size([2, 16, 28, 64])), 'attention_mask': torch.Size([2, 1, 1, 29]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:58,746 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:58,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:58,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:58,760 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])))
2023-10-08 04:36:58,761 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])))
2023-10-08 04:36:58,761 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:58,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:58,764 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:58,765 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:58,765 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:58,765 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:58,765 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:58,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:58,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:58,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:58,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:58,766 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:58,766 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:58,767 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:58,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:58,768 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:58,768 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:58,769 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:58,769 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:58,769 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:58,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:58,780 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:58,789 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:58,798 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:58,808 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:58,808 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:58,809 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:58,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:58,815 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:58,815 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:58,816 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:58,816 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:58,816 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:58,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:58,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:58,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:58,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:58,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:58,817 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:58,817 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:58,817 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:58,818 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:58,821 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:58,821 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:58,821 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:58,822 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:58,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:58,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:58,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:58,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:58,822 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:58,823 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:58,823 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:58,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:58,826 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:58,830 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,830 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,830 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,831 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,831 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:58,836 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:58,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:58,845 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:58,850 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:58,850 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:58,851 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:58,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:58,856 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:58,860 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,860 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,860 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,860 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,860 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:58,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:58,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:58,876 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:58,880 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:58,881 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:58,881 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:58,883 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:58,886 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:58,890 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,890 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,890 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,890 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,890 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:58,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:58,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:58,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:58,909 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:58,910 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:58,910 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:58,911 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:58,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:58,918 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,918 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,918 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,918 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,919 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:58,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:58,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:58,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:58,936 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:58,937 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:58,937 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:58,939 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:58,942 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:58,946 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,946 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,946 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,946 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:58,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:58,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:58,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:58,964 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:58,966 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:58,966 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:58,968 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:58,971 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:58,975 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:58,975 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,975 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:58,975 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:58,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:58,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:58,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:58,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:58,993 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:58,994 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:58,995 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:58,996 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:58,999 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:59,003 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,003 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,003 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,003 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:59,008 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:59,012 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:59,017 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:59,022 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,023 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,023 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:59,024 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:59,028 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:59,032 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,032 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,032 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,032 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,032 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:59,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:59,041 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:59,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:59,050 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,050 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,051 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:59,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:59,056 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:59,059 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,059 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,060 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,060 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:59,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:59,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:59,073 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:59,077 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,078 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,078 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:59,080 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:59,083 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:59,086 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,087 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,087 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,087 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:59,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:59,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:59,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:59,105 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,106 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,106 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:59,108 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:59,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:59,115 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,115 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,115 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,115 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,115 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:59,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:59,125 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:59,129 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:59,133 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,134 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,134 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:59,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:59,139 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:59,142 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,143 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,143 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,143 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:59,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:59,153 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:59,158 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:59,163 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,163 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,164 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:59,165 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:59,168 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:59,172 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,173 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,173 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,173 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:59,178 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:59,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:59,186 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:59,191 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,191 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,192 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:59,193 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:59,196 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:59,200 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,200 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,200 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,200 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:59,212 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:59,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:59,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:59,227 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,228 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,229 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:59,231 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:59,234 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:59,238 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,238 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,238 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,238 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,238 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:59,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:59,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:59,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:59,256 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,257 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,257 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:59,259 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:59,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:59,266 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,266 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,266 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,267 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:59,272 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:59,276 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:59,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:59,285 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,285 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,286 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:59,287 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:59,290 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:59,294 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,294 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,295 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,295 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:59,300 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:59,304 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:59,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:59,314 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,315 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,315 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:59,317 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:59,320 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:59,324 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,324 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,324 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,324 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:59,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:59,334 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:59,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:59,342 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,343 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,343 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:59,345 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:59,348 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:59,352 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,352 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,352 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,352 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,353 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:36:59,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:36:59,363 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:36:59,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:36:59,372 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,373 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,373 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:36:59,375 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:59,378 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:59,379 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,379 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 29, 64]), torch.Size([8, 16, 29, 64])), 'attention_mask': torch.Size([8, 1, 1, 30]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,380 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,380 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 29, 64]), torch.Size([2, 16, 29, 64])), 'attention_mask': torch.Size([2, 1, 1, 30]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:36:59,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:36:59,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:36:59,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:36:59,398 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])))
2023-10-08 04:36:59,399 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])))
2023-10-08 04:36:59,399 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:36:59,400 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:36:59,402 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:59,402 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:59,403 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:59,403 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:59,403 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:59,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:36:59,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:36:59,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:36:59,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:36:59,404 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:59,404 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:59,404 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:36:59,405 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:36:59,405 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:59,406 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:59,406 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:59,406 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:59,406 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:59,406 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:36:59,416 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:36:59,427 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:36:59,436 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:36:59,445 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:36:59,446 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:36:59,446 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:36:59,456 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:36:59,457 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:59,457 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:36:59,457 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:59,457 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:36:59,457 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:59,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:36:59,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:36:59,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:36:59,458 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:36:59,458 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:59,458 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:59,459 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:36:59,459 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:36:59,459 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:59,463 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:36:59,463 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:36:59,463 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:36:59,463 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:36:59,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:36:59,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:36:59,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:36:59,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:36:59,464 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:36:59,464 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:36:59,464 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:36:59,464 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:36:59,467 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:59,471 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,471 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,472 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,472 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,472 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:36:59,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:36:59,485 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:36:59,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:36:59,494 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,495 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,495 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:36:59,496 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:36:59,500 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:59,503 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,504 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,504 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,504 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,504 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:36:59,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:36:59,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:36:59,518 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:36:59,523 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,524 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,525 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:36:59,526 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:36:59,530 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:59,533 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,534 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,534 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,534 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:36:59,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:36:59,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:36:59,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:36:59,552 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,552 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,553 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:36:59,555 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:36:59,558 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:59,561 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,561 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,561 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,562 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,562 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:36:59,566 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:36:59,571 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:36:59,575 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:36:59,579 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,581 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,581 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:36:59,582 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:36:59,586 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:59,589 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,589 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,589 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,589 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,589 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:36:59,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:36:59,598 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:36:59,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:36:59,607 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,608 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,608 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:36:59,609 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:36:59,613 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:59,616 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,616 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,617 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,617 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,617 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:36:59,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:36:59,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:36:59,630 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:36:59,634 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,635 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,635 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:36:59,636 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:36:59,640 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:59,643 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,644 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,644 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,644 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,644 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:36:59,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:36:59,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:36:59,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:36:59,663 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,664 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,664 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:36:59,666 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:36:59,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:59,674 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,674 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,675 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,675 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,675 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:36:59,680 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:36:59,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:36:59,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:36:59,694 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,695 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,696 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:36:59,697 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:36:59,701 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:59,704 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,705 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,705 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,705 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:36:59,710 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:36:59,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:36:59,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:36:59,724 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,725 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,726 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:36:59,728 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:36:59,731 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:59,735 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,735 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,735 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,736 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,736 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:36:59,741 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:36:59,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:36:59,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:36:59,755 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,756 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,756 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:36:59,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:36:59,761 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:59,765 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,765 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,766 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,766 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,766 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:36:59,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:36:59,776 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:36:59,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:36:59,786 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,787 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,787 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:36:59,789 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:36:59,793 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:59,797 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,797 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,797 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,797 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,797 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:36:59,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:36:59,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:36:59,812 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:36:59,817 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,817 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,818 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:36:59,819 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:36:59,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:59,827 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,827 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,827 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,827 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:36:59,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:36:59,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:36:59,841 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:36:59,846 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,846 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,846 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:36:59,848 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:36:59,852 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:59,855 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,855 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,856 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,856 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,856 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:36:59,861 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:36:59,865 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:36:59,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:36:59,873 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,874 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,874 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:36:59,876 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:36:59,879 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:59,883 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,883 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,883 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,883 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,883 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:36:59,888 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:36:59,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:36:59,897 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:36:59,901 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,902 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,902 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:36:59,904 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:36:59,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:59,910 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,910 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,911 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,911 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:36:59,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:36:59,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:36:59,925 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:36:59,929 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,930 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,930 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:36:59,932 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:36:59,935 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:59,938 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,939 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,939 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,939 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,939 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:36:59,944 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:36:59,948 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:36:59,953 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:36:59,957 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,957 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,958 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:36:59,959 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:36:59,962 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:59,966 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,966 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,966 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,966 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,967 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:36:59,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:36:59,976 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:36:59,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:36:59,984 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:36:59,985 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:36:59,985 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:36:59,986 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:36:59,989 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:36:59,993 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:36:59,993 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,994 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:36:59,994 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:36:59,994 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:00,000 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:00,004 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:00,009 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:00,012 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:37:00,013 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:37:00,013 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:00,015 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:00,018 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:00,019 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,019 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 30, 64]), torch.Size([8, 16, 30, 64])), 'attention_mask': torch.Size([8, 1, 1, 31]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,020 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,020 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 30, 64]), torch.Size([2, 16, 30, 64])), 'attention_mask': torch.Size([2, 1, 1, 31]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:00,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:00,030 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:00,034 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:00,038 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])))
2023-10-08 04:37:00,039 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])))
2023-10-08 04:37:00,039 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:00,041 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:00,042 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:00,043 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:00,043 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:00,043 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:00,043 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:00,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:00,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:00,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:00,044 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:00,044 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:00,044 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:00,044 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:00,045 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:00,046 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:00,046 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:00,046 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:00,047 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:00,047 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:00,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:00,057 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:00,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:00,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:00,085 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:00,086 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:00,086 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:00,092 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:00,093 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:00,093 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:37:00,093 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:00,093 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:37:00,094 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:00,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:37:00,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:37:00,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:37:00,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:37:00,095 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:00,095 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:00,095 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:37:00,095 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:00,095 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:00,099 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:00,099 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:00,099 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:00,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:00,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:37:00,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:37:00,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:37:00,100 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:37:00,100 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:00,100 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:00,100 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:37:00,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:00,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:00,107 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,107 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,107 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,107 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:37:00,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:37:00,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:37:00,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:37:00,126 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,127 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,127 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:37:00,129 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:00,132 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:00,136 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,136 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,136 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,137 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,137 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:37:00,143 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:37:00,147 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:37:00,152 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:37:00,157 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,157 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,158 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:37:00,159 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:00,162 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:00,166 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,166 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,166 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,166 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:37:00,172 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:37:00,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:37:00,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:37:00,185 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,186 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,186 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:37:00,188 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:00,191 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:00,195 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,195 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,195 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,195 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:37:00,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:37:00,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:37:00,209 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:37:00,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,214 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:37:00,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:00,219 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:00,223 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,223 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,223 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,223 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:37:00,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:37:00,233 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:37:00,237 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:37:00,242 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,242 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,243 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:37:00,244 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:00,247 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:00,251 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,251 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,251 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,251 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:37:00,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:37:00,261 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:37:00,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:37:00,269 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,270 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,270 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:37:00,272 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:00,275 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:00,279 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,279 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,279 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,279 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:37:00,284 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:37:00,289 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:37:00,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:37:00,299 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,299 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,300 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:37:00,301 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:00,305 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:00,309 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,309 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,309 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,309 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:37:00,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:37:00,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:37:00,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:37:00,328 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,328 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,328 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:37:00,330 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:00,333 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:00,337 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,337 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,337 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,337 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:37:00,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:37:00,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:37:00,351 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:37:00,356 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,356 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,357 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:37:00,358 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:00,362 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:00,365 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,366 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,366 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,366 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,366 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:37:00,371 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:37:00,375 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:37:00,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:37:00,384 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,385 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,385 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:37:00,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:00,390 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:00,394 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,394 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,394 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,394 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,395 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:37:00,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:37:00,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:37:00,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:37:00,413 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,413 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,414 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:37:00,415 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:00,419 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:00,422 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,422 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,423 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,423 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:37:00,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:37:00,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:37:00,438 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:37:00,442 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,443 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,443 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:37:00,445 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:00,448 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:00,452 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,452 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,452 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,452 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:37:00,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:37:00,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:37:00,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:37:00,471 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,472 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,472 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:37:00,474 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:00,477 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:00,480 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,480 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,481 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,481 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,481 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:37:00,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:37:00,492 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:37:00,497 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:37:00,501 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,502 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,502 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:37:00,504 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:00,508 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:00,511 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,511 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,511 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,512 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,512 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:37:00,517 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:37:00,521 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:37:00,527 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:37:00,531 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,532 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,532 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:37:00,534 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:00,537 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:00,541 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,541 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,541 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,541 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,541 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:37:00,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:37:00,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:37:00,555 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:37:00,560 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,560 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,561 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:37:00,562 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:00,565 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:00,569 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,569 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,569 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,569 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:37:00,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:37:00,578 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:37:00,583 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:37:00,588 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,589 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,589 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:37:00,590 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:00,594 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:00,598 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,598 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,598 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,598 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,598 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:37:00,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:37:00,609 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:37:00,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:37:00,620 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,621 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,621 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:37:00,625 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:00,631 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:00,637 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,637 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,637 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,638 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,638 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:00,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:00,665 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:00,671 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:00,676 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,678 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,678 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:00,680 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:00,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:00,684 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,684 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 31, 64]), torch.Size([8, 16, 31, 64])), 'attention_mask': torch.Size([8, 1, 1, 32]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,684 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,684 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 31, 64]), torch.Size([2, 16, 31, 64])), 'attention_mask': torch.Size([2, 1, 1, 32]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:00,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:00,696 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:00,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:00,707 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])))
2023-10-08 04:37:00,709 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])))
2023-10-08 04:37:00,709 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:00,711 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:00,712 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:00,713 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:00,713 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:00,713 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:00,713 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:00,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:00,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:00,720 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:00,724 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:00,727 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:00,727 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:00,727 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:00,728 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:00,729 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:00,729 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:00,729 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:00,729 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:00,730 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:00,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:00,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:00,758 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:00,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:00,782 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:00,783 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:00,783 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:00,789 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:00,790 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:00,791 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:37:00,791 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:00,791 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:37:00,792 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:00,792 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:37:00,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:37:00,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:37:00,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:37:00,794 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:00,794 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:00,795 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:37:00,795 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:00,796 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:00,799 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:00,799 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:00,800 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:00,800 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:00,800 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:37:00,800 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:37:00,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:37:00,801 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:37:00,801 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:00,801 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:00,801 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:37:00,802 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:00,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:00,808 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,808 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,808 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,808 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:37:00,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:37:00,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:37:00,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:37:00,827 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:00,828 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:00,828 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:37:00,830 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:00,833 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:00,837 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,837 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,837 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,837 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,837 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:37:00,842 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:37:00,847 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:37:00,852 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:37:00,856 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:00,857 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:00,857 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:37:00,858 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:00,862 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:00,867 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,868 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,868 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,868 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,868 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:37:00,873 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:37:00,877 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:37:00,881 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:37:00,886 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:00,886 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:00,887 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:37:00,888 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:00,891 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:00,895 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,895 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,895 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,895 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,895 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:37:00,900 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:37:00,904 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:37:00,909 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:37:00,913 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:00,913 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:00,914 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:37:00,915 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:00,919 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:00,922 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,922 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,922 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,923 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:37:00,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:37:00,932 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:37:00,936 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:37:00,940 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:00,941 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:00,941 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:37:00,943 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:00,946 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:00,949 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,949 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,950 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,950 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,950 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:37:00,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:37:00,960 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:37:00,964 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:37:00,968 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:00,969 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:00,969 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:37:00,971 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:00,974 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:00,977 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:00,978 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,978 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:00,978 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:00,978 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:37:00,983 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:37:00,987 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:37:00,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:37:00,996 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:00,996 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:00,997 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:37:00,998 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:01,001 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:01,005 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,005 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,005 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,005 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,005 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:37:01,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:37:01,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:37:01,019 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:37:01,023 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,025 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,025 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:37:01,027 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:01,030 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:01,033 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,034 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,034 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,034 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,034 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:37:01,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:37:01,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:37:01,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:37:01,052 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,053 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,053 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:37:01,054 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:01,057 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:01,061 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,061 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,061 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,062 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,062 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:37:01,067 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:37:01,071 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:37:01,076 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:37:01,080 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,080 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,080 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:37:01,082 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:01,085 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:01,088 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,089 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,089 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,089 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:37:01,094 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:37:01,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:37:01,103 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:37:01,108 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,108 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,109 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:37:01,110 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:01,114 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:01,117 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,118 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,118 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,118 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,118 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:37:01,123 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:37:01,128 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:37:01,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:37:01,136 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,137 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,137 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:37:01,138 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:01,141 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:01,145 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,145 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,145 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,145 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:37:01,150 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:37:01,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:37:01,159 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:37:01,163 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,164 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,164 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:37:01,166 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:01,169 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:01,173 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,173 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,173 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,174 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,174 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:37:01,194 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:37:01,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:37:01,203 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:37:01,207 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,208 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,208 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:37:01,210 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:01,213 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:01,217 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,217 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,217 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,217 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:37:01,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:37:01,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:37:01,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:37:01,236 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,237 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,237 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:37:01,239 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:01,242 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:01,245 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,246 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,246 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,246 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,246 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:37:01,251 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:37:01,256 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:37:01,260 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:37:01,264 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,265 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,265 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:37:01,267 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:01,270 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:01,273 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,273 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,274 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,274 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,274 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:37:01,278 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:37:01,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:37:01,287 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:37:01,292 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,292 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,293 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:37:01,294 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:01,298 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:01,301 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,301 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,301 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,301 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,301 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:37:01,307 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:37:01,311 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:37:01,317 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:37:01,321 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,322 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,322 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:37:01,323 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:01,327 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:01,331 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,331 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,331 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,331 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:01,337 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:01,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:01,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:01,351 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,353 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,353 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:01,354 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:01,357 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:01,358 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,359 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 32, 64]), torch.Size([8, 16, 32, 64])), 'attention_mask': torch.Size([8, 1, 1, 33]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,359 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,359 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 32, 64]), torch.Size([2, 16, 32, 64])), 'attention_mask': torch.Size([2, 1, 1, 33]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,359 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:01,364 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:01,368 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:01,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:01,377 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])))
2023-10-08 04:37:01,379 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])))
2023-10-08 04:37:01,379 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:01,381 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:01,382 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:01,383 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:01,383 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:01,383 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:01,383 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:01,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:01,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:01,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:01,385 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:01,385 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:01,386 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:01,386 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:01,386 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:01,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:01,388 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:01,388 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:01,389 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:01,389 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:01,389 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:01,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:01,409 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:01,418 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:01,427 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:01,428 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:01,428 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:01,435 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:01,436 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:01,436 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:37:01,436 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:01,436 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:37:01,436 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:01,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:37:01,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:37:01,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:37:01,437 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:37:01,438 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:01,438 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:01,438 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:37:01,438 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:01,439 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:01,443 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:01,443 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:01,443 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:01,443 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:01,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:37:01,443 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:37:01,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:37:01,444 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:37:01,444 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:01,444 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:01,444 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:37:01,444 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:01,447 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:01,451 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,451 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,451 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,451 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,452 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:37:01,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:37:01,461 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:37:01,466 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:37:01,470 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,471 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,471 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:37:01,473 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:01,476 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:01,480 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,480 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,480 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,480 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,480 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:37:01,485 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:37:01,489 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:37:01,494 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:37:01,498 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,499 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,499 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:37:01,501 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:01,505 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:01,508 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,508 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,508 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,509 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,509 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:37:01,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:37:01,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:37:01,524 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:37:01,530 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,531 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,531 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:37:01,533 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:01,536 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:01,539 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,540 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,540 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,540 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:37:01,546 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:37:01,551 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:37:01,556 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:37:01,560 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,561 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,562 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:37:01,563 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:01,566 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:01,570 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,570 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,570 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,570 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:37:01,576 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:37:01,581 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:37:01,586 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:37:01,591 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,592 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,592 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:37:01,594 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:01,598 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:01,603 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,603 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,604 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,604 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,604 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:37:01,608 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:37:01,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:37:01,617 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:37:01,621 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,622 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,622 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:37:01,624 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:01,627 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:01,631 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,631 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,631 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,632 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,632 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:37:01,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:37:01,641 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:37:01,645 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:37:01,649 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,650 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,650 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:37:01,652 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:01,655 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:01,659 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,659 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,659 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,659 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,660 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:37:01,664 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:37:01,669 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:37:01,674 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:37:01,678 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,679 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,679 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:37:01,681 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:01,685 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:01,688 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,688 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,688 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,688 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:37:01,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:37:01,697 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:37:01,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:37:01,706 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,707 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,707 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:37:01,709 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:01,713 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:01,716 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,716 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,717 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,717 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,717 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:37:01,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:37:01,728 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:37:01,733 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:37:01,739 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,740 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,740 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:37:01,742 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:01,747 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:01,750 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,750 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,751 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,751 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,751 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:37:01,756 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:37:01,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:37:01,764 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:37:01,769 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,771 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,771 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:37:01,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:01,776 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:01,780 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,780 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,781 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,781 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:37:01,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:37:01,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:37:01,796 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:37:01,800 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,801 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,802 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:37:01,803 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:01,807 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:01,811 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,811 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,811 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,811 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:37:01,817 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:37:01,822 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:37:01,827 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:37:01,831 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,832 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,833 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:37:01,835 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:01,838 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:01,842 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,842 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,843 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,843 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,843 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:37:01,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:37:01,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:37:01,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:37:01,863 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,864 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,864 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:37:01,866 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:01,870 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:01,873 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,873 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,874 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,874 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:37:01,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:37:01,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:37:01,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:37:01,894 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,895 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,895 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:37:01,897 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:01,901 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:01,904 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,905 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,905 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,905 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,906 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:37:01,911 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:37:01,916 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:37:01,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:37:01,925 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,926 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,926 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:37:01,928 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:01,932 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:01,936 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,936 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,936 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,937 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:37:01,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:37:01,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:37:01,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:37:01,957 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,958 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,959 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:37:01,961 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:01,965 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:01,968 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:01,968 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,969 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:01,969 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:01,969 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:37:01,974 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:37:01,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:37:01,985 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:37:01,990 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:01,991 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:01,991 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:37:01,993 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:01,997 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:02,000 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,000 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,001 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,001 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,001 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:02,006 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:02,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:02,015 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:02,020 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:02,021 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:02,021 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:02,023 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:02,027 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:02,028 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,028 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 33, 64]), torch.Size([8, 16, 33, 64])), 'attention_mask': torch.Size([8, 1, 1, 34]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,028 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,029 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 33, 64]), torch.Size([2, 16, 33, 64])), 'attention_mask': torch.Size([2, 1, 1, 34]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,029 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:02,034 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:02,039 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:02,043 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:02,047 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])))
2023-10-08 04:37:02,048 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])))
2023-10-08 04:37:02,048 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:02,050 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:02,051 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:02,052 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:02,052 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:02,052 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:02,052 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:02,052 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:02,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:02,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:02,053 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:02,053 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:02,053 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:02,053 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:02,054 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:02,055 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:02,056 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:02,056 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:02,056 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:02,056 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:02,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:02,066 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:02,075 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:02,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:02,093 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:02,094 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:02,094 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:02,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:02,101 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:02,101 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:37:02,101 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:02,101 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:37:02,101 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:02,101 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:37:02,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:37:02,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:37:02,102 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:37:02,102 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:02,102 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:02,103 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:37:02,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:02,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:02,106 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:02,107 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:02,107 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:02,107 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:02,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:37:02,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:37:02,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:37:02,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:37:02,108 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:02,108 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:02,108 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:37:02,108 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:02,111 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:02,115 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,115 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,115 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,115 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:37:02,121 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:37:02,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:37:02,131 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:37:02,135 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,137 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,137 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:37:02,138 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:02,142 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:02,145 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,145 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,146 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,146 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,146 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:37:02,151 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:37:02,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:37:02,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:37:02,165 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,166 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,166 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:37:02,168 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:02,171 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:02,175 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,175 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,175 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,175 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,175 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:37:02,181 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:37:02,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:37:02,192 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:37:02,197 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,198 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,199 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:37:02,201 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:02,204 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:02,208 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,208 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,208 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,208 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,208 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:37:02,214 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:37:02,219 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:37:02,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:37:02,239 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,240 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,240 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:37:02,242 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:02,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:02,249 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,249 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,249 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,249 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,249 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:37:02,255 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:37:02,259 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:37:02,265 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:37:02,269 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,271 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,271 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:37:02,272 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:02,276 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:02,280 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,280 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,280 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,280 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,280 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:37:02,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:37:02,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:37:02,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:37:02,299 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,300 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,300 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:37:02,302 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:02,305 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:02,308 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,308 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,309 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,309 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,309 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:37:02,315 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:37:02,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:37:02,323 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:37:02,327 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,328 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,328 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:37:02,330 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:02,334 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:02,338 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,338 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,338 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,338 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,338 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:37:02,343 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:37:02,348 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:37:02,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:37:02,357 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,357 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,358 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:37:02,359 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:02,362 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:02,366 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,366 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,366 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,367 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,367 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:37:02,372 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:37:02,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:37:02,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:37:02,384 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,385 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,385 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:37:02,387 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:02,390 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:02,394 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,394 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,394 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,394 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,394 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:37:02,399 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:37:02,403 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:37:02,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:37:02,412 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,412 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,413 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:37:02,414 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:02,417 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:02,421 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,421 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,421 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,421 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,421 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:37:02,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:37:02,430 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:37:02,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:37:02,439 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,440 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,441 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:37:02,442 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:02,446 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:02,449 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,449 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,450 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,450 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,450 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:37:02,455 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:37:02,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:37:02,463 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:37:02,467 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,468 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,468 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:37:02,470 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:02,473 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:02,477 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,477 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,477 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,477 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:37:02,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:37:02,487 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:37:02,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:37:02,495 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,496 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,496 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:37:02,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:02,501 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:02,505 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,505 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,505 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,505 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:37:02,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:37:02,514 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:37:02,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:37:02,523 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,524 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,524 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:37:02,526 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:02,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:02,533 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,533 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,533 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,533 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:37:02,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:37:02,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:37:02,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:37:02,553 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,554 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,554 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:37:02,556 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:02,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:02,562 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,562 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,563 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,563 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,563 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:37:02,568 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:37:02,572 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:37:02,577 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:37:02,581 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,582 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,582 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:37:02,583 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:02,587 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:02,590 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,590 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,590 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,591 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,591 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:37:02,595 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:37:02,600 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:37:02,605 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:37:02,609 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,610 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,610 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:37:02,611 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:02,615 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:02,618 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,619 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,619 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,619 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,619 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:37:02,624 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:37:02,628 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:37:02,633 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:37:02,637 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,637 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,638 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:37:02,639 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:02,643 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:02,648 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,649 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,649 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,649 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:02,655 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:02,661 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:02,666 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:02,671 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,672 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,672 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:02,673 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:02,677 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:02,678 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,678 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 34, 64]), torch.Size([8, 16, 34, 64])), 'attention_mask': torch.Size([8, 1, 1, 35]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,679 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,679 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 34, 64]), torch.Size([2, 16, 34, 64])), 'attention_mask': torch.Size([2, 1, 1, 35]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,679 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:02,684 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:02,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:02,693 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:02,697 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])))
2023-10-08 04:37:02,698 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])))
2023-10-08 04:37:02,698 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:02,700 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:02,701 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:02,702 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:02,702 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:02,702 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:02,702 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:02,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:02,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:02,702 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:02,703 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:02,703 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:02,703 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:02,703 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:02,704 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:02,704 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:02,705 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:02,705 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:02,705 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:02,705 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:02,705 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:02,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:02,723 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:02,732 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:02,744 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:02,747 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:02,747 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:02,758 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:02,759 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:02,759 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:37:02,760 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:02,760 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:37:02,760 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:02,760 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:37:02,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:37:02,761 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:37:02,762 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:37:02,762 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:02,762 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:02,763 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:37:02,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:02,763 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:02,769 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:02,770 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:02,770 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:02,770 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:02,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:37:02,770 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:37:02,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:37:02,771 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:37:02,771 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:02,771 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:02,772 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:37:02,772 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:02,777 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:02,780 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,781 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,781 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,781 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,781 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:37:02,786 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:37:02,791 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:37:02,795 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:37:02,800 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:02,800 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:02,800 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:37:02,802 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:02,805 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:02,809 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,809 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,809 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,809 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,809 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:37:02,814 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:37:02,819 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:37:02,824 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:37:02,828 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:02,829 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:02,829 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:37:02,831 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:02,834 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:02,838 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,838 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,838 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,838 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,838 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:37:02,843 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:37:02,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:37:02,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:37:02,858 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:02,859 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:02,859 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:37:02,861 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:02,864 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:02,868 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,868 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,868 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,869 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,869 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:37:02,874 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:37:02,878 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:37:02,882 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:37:02,887 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:02,887 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:02,887 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:37:02,889 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:02,892 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:02,895 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,896 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,896 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,896 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:37:02,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:37:02,905 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:37:02,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:37:02,914 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:02,915 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:02,915 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:37:02,916 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:02,919 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:02,923 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,923 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,923 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,923 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:37:02,929 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:37:02,933 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:37:02,937 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:37:02,941 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:02,942 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:02,942 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:37:02,944 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:02,947 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:02,950 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,951 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,951 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,951 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,951 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:37:02,956 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:37:02,961 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:37:02,965 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:37:02,970 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:02,971 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:02,971 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:37:02,973 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:02,977 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:02,980 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:02,981 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,981 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:02,981 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:02,981 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:37:02,986 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:37:02,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:37:02,996 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:37:03,000 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,001 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,001 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:37:03,003 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:03,006 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:03,010 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,010 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,010 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,011 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,011 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:37:03,016 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:37:03,020 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:37:03,025 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:37:03,029 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,030 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,030 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:37:03,032 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:03,035 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:03,039 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,039 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,040 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,040 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,040 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:37:03,045 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:37:03,050 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:37:03,054 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:37:03,058 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,059 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,060 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:37:03,061 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:03,065 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:03,068 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,069 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,069 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,069 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:37:03,074 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:37:03,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:37:03,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:37:03,089 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,089 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,090 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:37:03,092 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:03,095 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:03,098 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,098 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,099 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,099 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,099 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:37:03,104 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:37:03,108 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:37:03,113 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:37:03,117 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,118 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,118 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:37:03,120 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:03,123 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:03,126 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,127 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,127 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,127 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,127 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:37:03,132 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:37:03,136 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:37:03,141 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:37:03,145 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,146 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,146 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:37:03,148 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:03,151 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:03,154 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,154 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,154 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,155 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,155 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:37:03,160 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:37:03,164 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:37:03,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:37:03,172 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,173 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,173 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:37:03,175 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:03,178 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:03,182 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,182 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,182 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,182 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:37:03,187 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:37:03,192 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:37:03,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:37:03,200 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,201 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,201 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:37:03,204 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:03,207 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:03,211 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,211 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,211 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,212 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,212 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:37:03,216 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:37:03,221 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:37:03,225 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:37:03,229 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,230 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,230 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:37:03,232 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:03,235 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:03,238 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,238 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,238 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,238 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:37:03,243 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:37:03,248 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:37:03,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:37:03,256 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,257 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,257 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:37:03,259 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:03,262 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:03,265 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,265 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,265 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,266 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,266 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:37:03,270 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:37:03,275 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:37:03,279 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:37:03,283 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,284 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,284 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:37:03,286 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:03,289 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:03,292 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,292 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,293 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,293 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,293 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:03,297 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:03,302 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:03,306 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:03,310 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,311 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,311 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:03,313 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:03,316 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:03,317 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,317 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 35, 64]), torch.Size([8, 16, 35, 64])), 'attention_mask': torch.Size([8, 1, 1, 36]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,318 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,318 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 35, 64]), torch.Size([2, 16, 35, 64])), 'attention_mask': torch.Size([2, 1, 1, 36]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,318 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:03,322 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:03,326 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:03,331 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:03,336 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])))
2023-10-08 04:37:03,337 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])))
2023-10-08 04:37:03,337 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:03,338 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:03,340 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:03,341 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:03,341 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:03,341 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:03,341 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:03,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:03,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:03,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:03,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:03,342 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:03,343 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:03,343 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:03,343 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:03,344 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:03,345 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:03,345 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:03,345 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:03,345 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:03,346 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:03,355 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:03,364 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:03,373 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:03,382 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:03,391 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:03,391 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:03,398 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:03,399 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:03,400 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:37:03,400 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:03,400 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:37:03,400 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:03,400 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:37:03,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:37:03,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:37:03,401 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:37:03,402 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:03,402 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:03,402 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:37:03,402 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:03,403 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:03,406 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:03,407 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:03,407 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:03,407 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:03,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:37:03,407 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:37:03,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:37:03,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:37:03,408 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:03,408 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:03,409 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:37:03,409 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:03,412 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:03,416 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,416 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,416 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,417 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,417 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:37:03,423 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:37:03,428 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:37:03,433 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:37:03,437 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,438 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,439 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:37:03,440 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:03,443 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:03,447 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,447 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,447 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,448 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,448 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:37:03,453 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:37:03,457 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:37:03,462 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:37:03,466 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,468 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,468 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:37:03,470 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:03,473 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:03,476 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,477 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,477 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,477 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,477 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:37:03,482 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:37:03,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:37:03,491 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:37:03,495 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,496 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,496 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:37:03,498 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:03,501 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:03,504 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,505 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,505 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,505 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,505 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:37:03,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:37:03,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:37:03,519 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:37:03,523 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,524 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,524 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:37:03,526 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:03,529 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:03,533 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,533 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,534 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,534 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,534 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:37:03,539 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:37:03,543 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:37:03,548 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:37:03,552 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,553 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,553 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:37:03,555 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:03,559 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:03,563 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,563 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,563 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,564 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,564 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:37:03,570 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:37:03,574 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:37:03,579 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:37:03,583 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,584 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,584 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:37:03,586 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:03,589 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:03,593 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,593 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,593 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,594 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,594 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:37:03,599 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:37:03,603 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:37:03,607 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:37:03,611 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,612 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,612 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:37:03,614 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:03,617 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:03,621 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,621 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,621 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,621 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:37:03,626 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:37:03,631 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:37:03,635 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:37:03,639 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,640 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,640 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:37:03,642 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:03,645 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:03,648 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,649 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,649 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,649 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:37:03,654 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:37:03,658 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:37:03,663 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:37:03,667 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,668 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,668 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:37:03,670 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:03,673 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:03,677 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,677 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,677 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,678 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,678 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:37:03,683 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:37:03,688 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:37:03,692 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:37:03,696 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,697 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,698 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:37:03,699 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:03,703 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:03,706 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,706 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,707 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,707 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,707 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:37:03,713 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:37:03,718 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:37:03,722 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:37:03,727 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,729 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,730 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:37:03,731 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:03,735 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:03,738 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,738 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,739 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,739 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,739 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:37:03,750 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:37:03,754 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:37:03,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:37:03,764 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,765 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,765 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:37:03,767 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:03,770 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:03,774 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,774 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,774 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,774 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,774 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:37:03,779 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:37:03,783 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:37:03,788 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:37:03,792 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,793 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,793 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:37:03,795 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:03,798 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:03,802 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,802 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,802 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,802 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,802 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:37:03,807 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:37:03,811 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:37:03,816 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:37:03,820 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,821 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,821 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:37:03,823 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:03,826 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:03,830 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,830 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,830 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,830 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,830 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:37:03,835 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:37:03,839 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:37:03,844 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:37:03,848 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,848 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,849 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:37:03,850 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:03,853 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:03,857 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,857 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,857 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,857 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,857 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:37:03,862 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:37:03,866 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:37:03,871 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:37:03,875 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,877 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,877 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:37:03,879 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:03,882 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:03,886 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,886 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,886 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,887 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,887 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:37:03,892 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:37:03,896 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:37:03,901 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:37:03,905 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,905 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,906 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:37:03,907 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:03,910 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:03,914 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,914 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,914 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,914 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,914 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:37:03,919 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:37:03,923 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:37:03,928 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:37:03,932 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,933 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,933 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:37:03,935 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:03,938 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:03,941 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,941 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,942 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,942 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,942 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:03,946 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:03,951 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:03,955 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:03,959 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,960 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,960 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:03,962 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:03,965 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:03,966 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:03,966 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 36, 64]), torch.Size([8, 16, 36, 64])), 'attention_mask': torch.Size([8, 1, 1, 37]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,966 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:03,966 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 36, 64]), torch.Size([2, 16, 36, 64])), 'attention_mask': torch.Size([2, 1, 1, 37]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:03,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:03,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:03,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:03,980 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:03,984 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])))
2023-10-08 04:37:03,985 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])))
2023-10-08 04:37:03,986 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:03,988 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:03,989 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:03,990 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:03,990 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:03,990 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:03,990 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:03,990 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:03,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:03,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:03,991 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:03,992 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:03,992 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:03,992 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:03,993 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:03,994 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:03,995 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:03,995 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:03,995 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:03,995 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:03,995 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:04,005 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:04,014 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:04,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:04,033 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:04,034 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:04,034 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:04,040 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:04,041 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:04,041 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:37:04,041 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:04,041 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:37:04,042 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:04,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:37:04,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:37:04,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:37:04,042 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:37:04,043 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:04,043 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:04,043 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:37:04,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:04,043 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:04,047 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:04,047 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:04,047 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:04,047 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:04,047 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:37:04,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:37:04,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:37:04,048 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:37:04,048 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:04,048 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:04,048 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:37:04,049 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:04,052 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:04,055 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,055 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,055 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,056 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:37:04,060 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:37:04,064 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:37:04,069 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:37:04,072 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,073 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,073 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:37:04,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:04,078 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:04,082 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,082 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,082 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,082 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,082 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:37:04,087 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:37:04,092 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:37:04,096 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:37:04,100 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,101 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,101 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:37:04,102 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:04,106 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:04,111 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,112 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,112 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,112 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:37:04,117 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:37:04,122 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:37:04,126 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:37:04,130 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,131 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,131 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:37:04,133 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:04,136 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:04,139 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,140 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,140 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,140 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,140 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:37:04,145 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:37:04,149 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:37:04,154 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:37:04,158 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,158 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,159 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:37:04,160 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:04,164 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:04,167 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,167 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,168 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,168 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,168 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:37:04,173 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:37:04,177 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:37:04,182 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:37:04,186 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,187 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,187 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:37:04,189 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:04,192 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:04,196 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,196 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,196 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,196 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,196 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:37:04,201 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:37:04,205 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:37:04,210 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:37:04,214 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,214 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,215 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:37:04,216 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:04,219 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:04,223 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,223 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,224 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,224 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,224 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:37:04,229 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:37:04,234 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:37:04,239 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:37:04,243 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,244 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,244 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:37:04,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:04,249 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:04,252 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,252 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,253 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,253 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,253 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:37:04,258 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:37:04,262 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:37:04,267 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:37:04,271 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,272 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,272 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:37:04,273 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:04,277 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:04,280 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,280 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,280 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,280 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,281 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:37:04,286 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:37:04,290 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:37:04,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:37:04,305 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,305 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,306 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:37:04,307 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:04,310 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:04,314 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,314 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,314 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,314 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,314 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:37:04,319 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:37:04,324 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:37:04,328 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:37:04,332 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,333 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,333 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:37:04,335 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:04,338 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:04,341 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,342 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,342 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,342 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,342 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:37:04,347 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:37:04,352 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:37:04,356 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:37:04,360 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,361 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,361 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:37:04,363 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:04,366 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:04,370 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,370 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,370 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,370 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,370 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:37:04,376 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:37:04,380 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:37:04,384 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:37:04,388 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,389 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,389 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:37:04,391 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:04,394 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:04,398 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,398 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,398 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,398 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,398 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:37:04,404 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:37:04,408 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:37:04,412 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:37:04,417 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,417 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,418 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:37:04,419 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:04,422 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:04,426 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,426 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,426 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,426 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,426 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:37:04,431 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:37:04,435 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:37:04,440 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:37:04,444 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,445 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,445 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:37:04,446 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:04,450 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:04,453 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,453 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,454 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,454 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,454 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:37:04,459 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:37:04,467 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:37:04,486 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:37:04,490 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,491 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,492 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:37:04,493 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:04,497 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:04,500 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,500 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,501 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,501 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,501 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:37:04,506 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:37:04,510 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:37:04,515 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:37:04,519 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,520 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,520 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:37:04,522 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:04,526 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:04,530 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,530 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,530 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,530 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,531 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:37:04,535 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:37:04,540 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:37:04,544 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:37:04,549 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,550 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,550 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:37:04,551 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:04,555 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:04,559 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,559 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,559 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,559 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,560 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:37:04,565 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:37:04,569 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:37:04,573 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:37:04,577 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,578 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,578 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:37:04,580 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:04,583 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:04,587 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,587 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,588 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,588 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,588 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:04,593 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:04,597 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:04,602 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:04,606 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,607 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,607 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:04,609 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:04,612 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:04,613 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,613 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 37, 64]), torch.Size([8, 16, 37, 64])), 'attention_mask': torch.Size([8, 1, 1, 38]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,613 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,613 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 37, 64]), torch.Size([2, 16, 37, 64])), 'attention_mask': torch.Size([2, 1, 1, 38]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,613 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:04,618 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:04,622 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:04,627 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:04,631 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])))
2023-10-08 04:37:04,631 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])))
2023-10-08 04:37:04,632 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:04,633 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:04,634 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:04,635 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:04,636 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:04,636 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:04,636 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:04,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:04,636 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:04,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:04,637 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:04,637 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:04,637 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:04,637 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:04,638 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:04,639 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:04,639 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:04,639 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:04,639 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:04,639 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:04,640 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:04,649 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:04,659 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:04,668 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:04,677 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:04,677 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:04,677 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:04,683 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:04,684 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:04,684 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-08 04:37:04,684 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:04,685 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-08 04:37:04,685 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:04,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 0
2023-10-08 04:37:04,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 1
2023-10-08 04:37:04,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 2
2023-10-08 04:37:04,685 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.wte, batch: 3
2023-10-08 04:37:04,686 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:04,686 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:04,686 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.wte


2023-10-08 04:37:04,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.drop to cpu
2023-10-08 04:37:04,686 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:04,690 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:04,690 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:04,690 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:04,690 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:04,690 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 0
2023-10-08 04:37:04,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 1
2023-10-08 04:37:04,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 2
2023-10-08 04:37:04,691 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.drop, batch: 3
2023-10-08 04:37:04,691 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:04,691 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:04,691 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.drop


2023-10-08 04:37:04,692 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-08 04:37:04,695 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:04,698 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,698 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,698 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,698 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,699 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 0
2023-10-08 04:37:04,704 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 1
2023-10-08 04:37:04,709 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 2
2023-10-08 04:37:04,715 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.0, batch: 3
2023-10-08 04:37:04,719 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,720 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,721 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-08 04:37:04,722 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-08 04:37:04,726 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:04,729 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,729 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,730 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,730 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,730 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 0
2023-10-08 04:37:04,735 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 1
2023-10-08 04:37:04,740 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 2
2023-10-08 04:37:04,745 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.1, batch: 3
2023-10-08 04:37:04,749 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,750 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,750 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-08 04:37:04,751 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-08 04:37:04,755 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:04,758 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,758 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,759 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,759 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,759 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 0
2023-10-08 04:37:04,765 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 1
2023-10-08 04:37:04,769 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 2
2023-10-08 04:37:04,773 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.2, batch: 3
2023-10-08 04:37:04,777 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,778 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,778 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-08 04:37:04,780 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-08 04:37:04,783 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:04,786 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,787 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,787 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,787 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,787 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 0
2023-10-08 04:37:04,793 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 1
2023-10-08 04:37:04,799 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 2
2023-10-08 04:37:04,803 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.3, batch: 3
2023-10-08 04:37:04,808 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,809 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,809 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-08 04:37:04,810 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-08 04:37:04,814 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:04,817 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,817 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,817 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,817 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,818 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 0
2023-10-08 04:37:04,823 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 1
2023-10-08 04:37:04,828 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 2
2023-10-08 04:37:04,832 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.4, batch: 3
2023-10-08 04:37:04,837 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,838 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,838 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-08 04:37:04,840 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-08 04:37:04,843 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:04,847 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,847 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,847 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,848 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,848 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 0
2023-10-08 04:37:04,853 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 1
2023-10-08 04:37:04,858 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 2
2023-10-08 04:37:04,863 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.5, batch: 3
2023-10-08 04:37:04,868 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,868 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,868 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-08 04:37:04,870 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-08 04:37:04,874 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:04,877 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,878 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,878 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,879 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,879 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 0
2023-10-08 04:37:04,884 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 1
2023-10-08 04:37:04,889 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 2
2023-10-08 04:37:04,894 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.6, batch: 3
2023-10-08 04:37:04,899 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,900 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,901 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-08 04:37:04,902 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-08 04:37:04,906 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:04,910 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,910 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,910 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,910 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,910 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 0
2023-10-08 04:37:04,915 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 1
2023-10-08 04:37:04,920 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 2
2023-10-08 04:37:04,924 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.7, batch: 3
2023-10-08 04:37:04,928 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,929 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,929 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-08 04:37:04,931 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-08 04:37:04,934 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:04,938 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,938 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,938 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,938 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,938 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 0
2023-10-08 04:37:04,943 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 1
2023-10-08 04:37:04,947 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 2
2023-10-08 04:37:04,952 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.8, batch: 3
2023-10-08 04:37:04,956 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,957 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,957 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-08 04:37:04,958 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-08 04:37:04,962 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:04,965 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,965 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,965 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,966 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,966 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 0
2023-10-08 04:37:04,971 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 1
2023-10-08 04:37:04,975 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 2
2023-10-08 04:37:04,979 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.9, batch: 3
2023-10-08 04:37:04,983 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:04,983 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:04,984 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-08 04:37:04,985 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-08 04:37:04,988 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:04,992 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:04,992 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,992 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:04,992 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:04,992 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 0
2023-10-08 04:37:04,997 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 1
2023-10-08 04:37:05,002 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 2
2023-10-08 04:37:05,010 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.10, batch: 3
2023-10-08 04:37:05,014 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,014 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,015 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-08 04:37:05,016 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-08 04:37:05,019 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:05,022 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,023 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,023 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,023 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,023 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 0
2023-10-08 04:37:05,028 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 1
2023-10-08 04:37:05,033 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 2
2023-10-08 04:37:05,037 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.11, batch: 3
2023-10-08 04:37:05,042 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,042 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,043 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-08 04:37:05,044 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-08 04:37:05,047 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:05,051 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,051 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,051 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,051 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,051 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 0
2023-10-08 04:37:05,056 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 1
2023-10-08 04:37:05,061 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 2
2023-10-08 04:37:05,065 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.12, batch: 3
2023-10-08 04:37:05,069 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,070 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,070 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-08 04:37:05,072 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-08 04:37:05,075 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:05,079 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,079 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,079 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,079 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,079 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 0
2023-10-08 04:37:05,084 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 1
2023-10-08 04:37:05,089 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 2
2023-10-08 04:37:05,093 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.13, batch: 3
2023-10-08 04:37:05,097 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,098 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,098 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-08 04:37:05,100 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-08 04:37:05,103 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:05,106 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,107 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,107 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,107 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,107 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 0
2023-10-08 04:37:05,112 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 1
2023-10-08 04:37:05,116 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 2
2023-10-08 04:37:05,120 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.14, batch: 3
2023-10-08 04:37:05,125 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,125 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,126 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-08 04:37:05,127 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-08 04:37:05,131 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:05,134 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,134 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,134 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,134 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,134 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 0
2023-10-08 04:37:05,139 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 1
2023-10-08 04:37:05,144 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 2
2023-10-08 04:37:05,148 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.15, batch: 3
2023-10-08 04:37:05,152 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,153 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,153 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-08 04:37:05,155 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-08 04:37:05,158 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:05,161 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,162 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,162 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,162 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,162 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 0
2023-10-08 04:37:05,167 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 1
2023-10-08 04:37:05,171 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 2
2023-10-08 04:37:05,176 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.16, batch: 3
2023-10-08 04:37:05,181 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,181 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,181 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-08 04:37:05,183 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-08 04:37:05,187 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:05,190 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,190 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,190 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,190 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,190 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 0
2023-10-08 04:37:05,195 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 1
2023-10-08 04:37:05,199 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 2
2023-10-08 04:37:05,204 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.17, batch: 3
2023-10-08 04:37:05,208 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,209 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,209 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-08 04:37:05,211 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-08 04:37:05,214 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:05,218 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,218 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,218 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,218 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,218 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 0
2023-10-08 04:37:05,223 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 1
2023-10-08 04:37:05,228 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 2
2023-10-08 04:37:05,232 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.18, batch: 3
2023-10-08 04:37:05,241 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,242 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,242 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-08 04:37:05,245 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-08 04:37:05,250 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:05,251 [flexgen_forward.py:106 in new_forward] DEBUG - args: ()
2023-10-08 04:37:05,252 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {'hidden_states': torch.Size([8, 1, 1024]), 'layer_past': (torch.Size([8, 16, 38, 64]), torch.Size([8, 16, 38, 64])), 'attention_mask': torch.Size([8, 1, 1, 39]), 'position_ids': torch.Size([8, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,252 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: ()
2023-10-08 04:37:05,252 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {'hidden_states': torch.Size([2, 1, 1024]), 'layer_past': (torch.Size([2, 16, 38, 64]), torch.Size([2, 16, 38, 64])), 'attention_mask': torch.Size([2, 1, 1, 39]), 'position_ids': torch.Size([2, 1]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}
2023-10-08 04:37:05,252 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 0
2023-10-08 04:37:05,272 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 1
2023-10-08 04:37:05,277 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 2
2023-10-08 04:37:05,283 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.h.19, batch: 3
2023-10-08 04:37:05,288 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 1024]), (torch.Size([2, 16, 39, 64]), torch.Size([2, 16, 39, 64])))
2023-10-08 04:37:05,290 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 1024]), (torch.Size([8, 16, 39, 64]), torch.Size([8, 16, 39, 64])))
2023-10-08 04:37:05,290 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-08 04:37:05,291 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-08 04:37:05,293 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:05,294 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:05,294 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:05,294 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:05,294 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:05,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0
2023-10-08 04:37:05,294 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1
2023-10-08 04:37:05,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2
2023-10-08 04:37:05,295 [flexgen_forward.py:120 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3
2023-10-08 04:37:05,295 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 1024])
2023-10-08 04:37:05,296 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 1024])
2023-10-08 04:37:05,296 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-08 04:37:05,296 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-08 04:37:05,297 [offload.py:56 in load_layer_weights] DEBUG - load_layer_weights: transformer.wte to cpu
2023-10-08 04:37:05,298 [flexgen_forward.py:106 in new_forward] DEBUG - args: (torch.Size([8, 1, 1024]),)
2023-10-08 04:37:05,298 [flexgen_forward.py:107 in new_forward] DEBUG - kwargs: {}
2023-10-08 04:37:05,298 [flexgen_forward.py:113 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 1024]),)
2023-10-08 04:37:05,298 [flexgen_forward.py:114 in new_forward] DEBUG - kwargs_0: {}
2023-10-08 04:37:05,299 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-08 04:37:05,313 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-08 04:37:05,329 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-08 04:37:05,341 [flexgen_forward.py:120 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-08 04:37:05,351 [flexgen_forward.py:132 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 51200])
2023-10-08 04:37:05,352 [flexgen_forward.py:134 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 51200])
2023-10-08 04:37:05,352 [offload.py:67 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-08 04:37:05,361 [flexgen_test.py:33 in test_hf_gen] INFO - for i in range(10): 
        print(i)
    print("\n")
    print("\n")
    print("\n")
    print("\
2023-10-08 04:37:05,361 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:37:05,361 [flexgen_test.py:33 in test_hf_gen] INFO - Who are you? Are you conscious? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man? Are you
2023-10-08 04:37:05,361 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:37:05,361 [flexgen_test.py:33 in test_hf_gen] INFO - Where is Deutschland?#!/usr/bin/env python

# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed under the
2023-10-08 04:37:05,362 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:37:05,362 [flexgen_test.py:33 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
    Examples for the NUR
2023-10-08 04:37:05,362 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:37:05,362 [flexgen_test.py:33 in test_hf_gen] INFO - for i in range(10): 
        print(i)
    print("\n")
    print("\n")
    print("\n")
    print("\
2023-10-08 04:37:05,362 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:37:05,362 [flexgen_test.py:33 in test_hf_gen] INFO - Who are you? Are you conscious? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man? Are you a man? Are you
2023-10-08 04:37:05,362 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:37:05,362 [flexgen_test.py:33 in test_hf_gen] INFO - Where is Deutschland?#!/usr/bin/env python

# Copyright (c) Facebook, Inc. and its affiliates.
# This source code is licensed under the
2023-10-08 04:37:05,362 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:37:05,362 [flexgen_test.py:33 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
    Examples for the NUR
2023-10-08 04:37:05,362 [flexgen_test.py:34 in test_hf_gen] INFO - ----------
2023-10-08 04:37:05,372 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.wte from flexgen to old.
2023-10-08 04:37:05,372 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.drop from flexgen to old.
2023-10-08 04:37:05,372 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.0 from flexgen to old.
2023-10-08 04:37:05,372 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.1 from flexgen to old.
2023-10-08 04:37:05,372 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.2 from flexgen to old.
2023-10-08 04:37:05,372 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.3 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.4 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.5 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.6 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.7 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.8 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.9 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.10 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.11 from flexgen to old.
2023-10-08 04:37:05,373 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.12 from flexgen to old.
2023-10-08 04:37:05,374 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.13 from flexgen to old.
2023-10-08 04:37:05,374 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.14 from flexgen to old.
2023-10-08 04:37:05,374 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.15 from flexgen to old.
2023-10-08 04:37:05,374 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.16 from flexgen to old.
2023-10-08 04:37:05,374 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.17 from flexgen to old.
2023-10-08 04:37:05,374 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.18 from flexgen to old.
2023-10-08 04:37:05,374 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.h.19 from flexgen to old.
2023-10-08 04:37:05,374 [flexgen_forward.py:22 in to_old_forward] DEBUG - transformer.ln_f from flexgen to old.
2023-10-08 04:37:05,375 [flexgen_forward.py:22 in to_old_forward] DEBUG - lm_head from flexgen to old.
