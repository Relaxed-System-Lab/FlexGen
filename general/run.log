2023-10-30 08:06:26,883 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpiutn6hvo
2023-10-30 08:06:26,884 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpiutn6hvo/_remote_module_non_scriptable.py
2023-10-30 08:06:27,333 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-30 08:06:27,434 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 08:06:28,984 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-30 08:06:29,266 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 08:06:29,267 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 08:06:29,267 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-30 08:06:29,267 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-30 08:06:30,054 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 08:06:30,137 [model.py:111 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-30 08:06:30,137 [model.py:60 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-30 08:06:30,175 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-30 08:06:30,257 [model.py:68 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-30 08:06:30,260 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 1. 0.], size_todo: 86630400
2023-10-30 08:06:30,261 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 1. 0.], size_todo: 85056000
2023-10-30 08:06:30,261 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0. 1. 0.], size_todo: 85054464
2023-10-30 08:06:30,262 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0. 1. 0.], size_todo: 77966592
2023-10-30 08:06:30,263 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0. 1. 0.], size_todo: 70878720
2023-10-30 08:06:30,264 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0. 1. 0.], size_todo: 63790848
2023-10-30 08:06:30,265 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0. 1. 0.], size_todo: 56702976
2023-10-30 08:06:30,266 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0. 1. 0.], size_todo: 49615104
2023-10-30 08:06:30,267 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0. 1. 0.], size_todo: 42527232
2023-10-30 08:06:30,268 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0. 1. 0.], size_todo: 35439360
2023-10-30 08:06:30,269 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0. 1. 0.], size_todo: 28351488
2023-10-30 08:06:30,270 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0. 1. 0.], size_todo: 21263616
2023-10-30 08:06:30,270 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0. 1. 0.], size_todo: 14175744
2023-10-30 08:06:30,271 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0. 1. 0.], size_todo: 7087872
2023-10-30 08:06:30,272 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0. 1. 0.], size_todo: 0
2023-10-30 08:06:30,273 [model.py:211 in get_policy_weight_map] DEBUG - lm_head, [0. 1. 0.], size_todo: 0
2023-10-30 08:06:30,273 [model.py:215 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-30 08:06:30,275 [model.py:221 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.23 GiB (100.00%), Disk Mem 0.00 Gib (0.00%)
2023-10-30 08:06:30,317 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 08:06:30,469 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-30 08:06:30,470 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-30 08:06:30,470 [model.py:298 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-30 08:06:30,470 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-30 08:06:30,470 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-30 08:06:30,470 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-30 08:06:30,471 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-30 08:06:30,471 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-30 08:06:30,471 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-30 08:06:30,471 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-30 08:06:30,471 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-30 08:06:30,471 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-30 08:06:30,471 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-30 08:06:30,472 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-30 08:06:30,472 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-30 08:06:30,472 [model.py:298 in to_test_forward] DEBUG - lm_head to test forward
2023-10-30 08:06:30,475 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:30,477 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-30 08:06:30,477 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:30,478 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-30 08:06:30,479 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:30,489 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-30 08:06:30,492 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:30,498 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-30 08:06:30,501 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:30,507 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-30 08:06:30,510 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:30,516 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-30 08:06:30,519 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:30,525 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-30 08:06:30,528 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:30,534 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-30 08:06:30,537 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:30,543 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-30 08:06:30,546 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:30,552 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-30 08:06:30,555 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:30,561 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-30 08:06:30,563 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:30,569 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-30 08:06:30,571 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:30,578 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-30 08:06:30,580 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:30,586 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-30 08:06:30,589 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:30,590 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-30 08:06:30,590 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:30,600 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-30 08:06:30,605 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-30 08:06:30,605 [model.py:306 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-30 08:06:30,605 [model.py:306 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-30 08:06:30,605 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-30 08:06:30,605 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-30 08:06:30,605 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-30 08:06:30,606 [model.py:306 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-30 08:06:30,607 [model.py:306 in reset_forward] DEBUG - lm_head from test to old.
2023-10-30 08:06:30,615 [model.py:408 in init_all_weights] DEBUG - init all weights...
2023-10-30 08:06:30,651 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-30 08:06:30,651 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-30 08:06:30,651 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-30 08:06:30,652 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-30 08:06:30,652 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-30 08:06:30,652 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-30 08:06:30,652 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-30 08:06:30,652 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-30 08:06:30,652 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-30 08:06:30,652 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-30 08:06:30,653 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-30 08:06:30,653 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-30 08:06:30,653 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-30 08:06:30,653 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-30 08:06:30,653 [flexgen.py:159 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-30 08:06:30,653 [flexgen.py:159 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-30 08:06:30,692 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-30 08:06:30,836 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])",)
2023-10-30 08:06:30,836 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:30,836 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:30,837 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:30,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:30,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:30,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:30,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:30,839 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 08:06:30,839 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:30,839 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])", "<class 'int'>: 0")
2023-10-30 08:06:30,840 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:30,840 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:30,840 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:30,843 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:30,844 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:30,844 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:30,845 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:30,845 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 08:06:30,845 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:30,846 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:30,846 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:30,846 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:30,849 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:30,856 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,859 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,861 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,863 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,863 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:30,863 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:30,864 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:30,864 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:30,864 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:30,867 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:30,874 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,877 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,879 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,881 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,881 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:30,881 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:30,882 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:30,882 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:30,882 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:30,885 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:30,896 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,899 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,901 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,905 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,905 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:30,905 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:30,905 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:30,906 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:30,906 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:30,909 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:30,916 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,919 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,921 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,923 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,923 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:30,924 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:30,924 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:30,924 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:30,924 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:30,928 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:30,935 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,937 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,940 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,942 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,942 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:30,943 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:30,943 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:30,943 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:30,943 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:30,947 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:30,954 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,957 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,959 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,962 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,962 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:30,962 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:30,963 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:30,963 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:30,963 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:30,967 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:30,973 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,976 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,978 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,981 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,981 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:30,981 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:30,981 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:30,982 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:30,982 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:30,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:30,993 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,995 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:30,997 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,000 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,000 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:31,000 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:31,001 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:31,001 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,001 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:31,005 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:31,012 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,015 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,017 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,020 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,020 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:31,020 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:31,021 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:31,021 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,021 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:31,025 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:31,032 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,034 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,037 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,039 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,039 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:31,039 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:31,040 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:31,040 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,041 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:31,045 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:31,052 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,054 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,056 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,059 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,059 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:31,060 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:31,060 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:31,060 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,061 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:31,065 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:31,069 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,072 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,075 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,077 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-30 08:06:31,077 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-30 08:06:31,077 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:31,078 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:31,078 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,078 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:31,079 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:31,080 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:31,080 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:31,081 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:31,081 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-30 08:06:31,082 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-30 08:06:31,082 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:31,083 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-30 08:06:31,083 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,083 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:31,084 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:31,095 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-30 08:06:31,105 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-30 08:06:31,114 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-30 08:06:31,123 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-30 08:06:31,125 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 9, 50272])
2023-10-30 08:06:31,125 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:31,127 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:31,127 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,127 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:31,128 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:31,129 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,130 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,130 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,131 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,131 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,131 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:31,132 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 10])", "<class 'int'>: 9")
2023-10-30 08:06:31,132 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,132 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:31,133 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:31,138 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,139 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,139 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,140 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,140 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,140 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:31,141 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,141 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,142 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:31,146 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:31,153 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,155 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,157 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,158 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,159 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,159 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:31,159 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,160 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,160 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:31,164 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:31,171 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,173 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,175 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,177 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,177 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,177 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:31,178 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,178 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,178 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:31,182 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:31,190 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,192 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,193 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,195 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,195 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,195 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:31,196 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,196 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,196 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:31,200 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:31,208 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,209 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,211 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,213 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,213 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,213 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:31,213 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,214 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,214 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:31,218 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:31,225 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,226 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,228 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,229 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,230 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,230 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:31,230 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,230 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,231 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:31,235 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:31,242 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,243 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,245 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,247 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,247 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,247 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:31,247 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,248 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,248 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:31,251 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:31,259 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,261 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,262 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,264 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,264 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,264 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:31,265 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,265 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,265 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:31,269 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:31,276 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,277 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,279 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,281 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,281 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,281 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:31,282 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,282 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,282 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:31,286 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:31,297 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,300 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,302 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,304 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,305 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,305 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:31,306 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,306 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,306 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:31,310 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:31,319 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,322 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,324 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,326 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,326 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,326 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:31,327 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,327 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,327 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:31,332 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:31,340 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,343 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,345 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,347 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,347 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,347 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:31,348 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,348 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,349 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:31,353 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:31,358 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,360 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,362 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,364 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-30 08:06:31,364 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-30 08:06:31,364 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:31,365 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,365 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,365 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:31,366 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:31,370 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,370 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,371 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,371 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,371 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,372 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:31,372 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,372 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,372 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:31,373 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:31,380 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,386 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,392 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,397 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,403 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:31,403 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:31,405 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:31,405 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,406 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:31,406 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:31,407 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,407 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,408 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,408 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,409 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,409 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:31,409 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 11])", "<class 'int'>: 10")
2023-10-30 08:06:31,409 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,409 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:31,410 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:31,415 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,415 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,415 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,416 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,416 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,416 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:31,417 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,417 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,417 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:31,421 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:31,428 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,430 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,432 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,434 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,435 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,435 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:31,435 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,436 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,436 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:31,440 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:31,448 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,452 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,454 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,456 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,456 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,457 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:31,457 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,457 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,458 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:31,461 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:31,468 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,469 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,471 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,473 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,473 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,473 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:31,473 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,474 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,474 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:31,477 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:31,483 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,485 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,487 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,488 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,488 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,489 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:31,489 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,489 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,489 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:31,493 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:31,499 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,527 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,541 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,544 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,544 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,545 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:31,545 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,546 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,546 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:31,549 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:31,572 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,574 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,576 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,577 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,578 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,578 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:31,579 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,579 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,579 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:31,583 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:31,590 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,592 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,594 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,595 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,596 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,596 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:31,596 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,597 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,597 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:31,601 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:31,608 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,610 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,611 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,613 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,613 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,613 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:31,614 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,614 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,614 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:31,618 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:31,625 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,627 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,628 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,630 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,630 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,630 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:31,631 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,631 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,632 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:31,635 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:31,642 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,644 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,646 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,647 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,647 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,648 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:31,648 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,648 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,649 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:31,652 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:31,660 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,661 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,663 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,665 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,665 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,665 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:31,665 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,666 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,666 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:31,669 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:31,673 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,675 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,677 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,678 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-30 08:06:31,678 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-30 08:06:31,679 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:31,679 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,679 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,679 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:31,680 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:31,684 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,685 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,685 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,685 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,685 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,685 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:31,686 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,686 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,686 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:31,686 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:31,694 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,700 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,706 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,711 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,712 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:31,712 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:31,713 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:31,714 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,714 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:31,714 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:31,715 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,715 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,716 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,716 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,716 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,716 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:31,717 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 12])", "<class 'int'>: 11")
2023-10-30 08:06:31,717 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,717 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:31,717 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:31,722 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,723 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,723 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,723 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,723 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,724 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:31,724 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,724 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,725 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:31,728 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:31,737 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,741 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,743 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,745 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,746 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,746 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:31,747 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,748 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,748 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:31,752 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:31,760 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,761 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,763 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,765 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,765 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,765 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:31,766 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,766 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,766 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:31,769 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:31,777 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,778 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,780 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,782 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,782 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,782 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:31,783 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,783 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,783 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:31,787 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:31,794 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,796 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,799 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,802 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,803 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,803 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:31,803 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,804 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,804 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:31,808 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:31,815 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,816 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,818 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,820 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,821 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,821 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:31,821 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,822 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,822 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:31,826 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:31,833 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,835 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,836 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,838 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,838 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:31,839 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,839 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,839 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:31,843 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:31,850 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,851 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,853 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,855 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,855 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,855 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:31,856 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,856 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,856 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:31,860 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:31,868 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,869 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,871 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,873 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,873 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,873 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:31,874 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,874 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,874 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:31,877 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:31,885 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,886 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,888 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,890 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,890 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,890 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:31,891 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,891 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,891 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:31,894 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:31,901 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,903 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,905 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,907 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,907 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,907 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:31,908 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,908 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,908 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:31,912 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:31,919 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,921 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,922 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,924 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,924 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,924 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:31,925 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,925 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,925 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:31,929 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:31,933 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,935 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,936 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,938 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-30 08:06:31,938 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-30 08:06:31,938 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:31,939 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,939 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,939 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:31,940 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:31,944 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,944 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,944 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,945 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,945 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,945 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:31,945 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,945 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,945 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:31,946 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:31,953 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,959 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,965 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,973 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:31,974 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:31,974 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:31,975 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:31,975 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,976 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:31,976 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:31,977 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,977 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,978 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,978 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,978 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,978 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:31,979 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 13])", "<class 'int'>: 12")
2023-10-30 08:06:31,979 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:31,979 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:31,979 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:31,984 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,985 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,985 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,986 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:31,986 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:31,986 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:31,987 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:31,987 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:31,987 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:31,991 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:31,998 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:31,999 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,001 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,003 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,003 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,003 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:32,003 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,004 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,004 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:32,008 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:32,015 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,017 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,018 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,020 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,020 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,020 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:32,021 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,021 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,021 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:32,025 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:32,032 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,034 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,036 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,037 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,038 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,038 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:32,038 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,038 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,039 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:32,042 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:32,050 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,052 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,053 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,055 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,055 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,055 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:32,056 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,056 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,056 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:32,060 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:32,067 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,069 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,070 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,072 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,072 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,072 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:32,073 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,073 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,073 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:32,077 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:32,084 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,086 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,087 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,089 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,089 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,089 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:32,090 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,090 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,090 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:32,094 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:32,101 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,103 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,104 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,106 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,106 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,106 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:32,107 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,107 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,107 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:32,111 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:32,118 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,120 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,121 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,123 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,123 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,123 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:32,124 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,124 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,124 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:32,128 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:32,135 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,137 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,139 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,140 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,140 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,141 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:32,141 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,141 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,141 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:32,145 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:32,152 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,154 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,156 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,158 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,158 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,158 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:32,158 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,159 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,159 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:32,162 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:32,170 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,172 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,173 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,175 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,175 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,175 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:32,176 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,176 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,176 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:32,180 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:32,183 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,185 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,187 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,188 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-30 08:06:32,189 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-30 08:06:32,189 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:32,189 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,189 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,189 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:32,190 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:32,194 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,194 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,194 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,195 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,195 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,195 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:32,195 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,196 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,196 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:32,196 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:32,203 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,209 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,215 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,221 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,222 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:32,222 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:32,223 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:32,223 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,223 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:32,224 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:32,224 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,225 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,225 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,225 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,226 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,226 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:32,226 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 14])", "<class 'int'>: 13")
2023-10-30 08:06:32,227 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,227 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:32,227 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:32,232 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,232 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,233 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,233 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,234 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,234 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:32,234 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,235 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,235 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:32,238 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:32,246 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,247 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,249 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,250 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,251 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,251 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:32,251 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,251 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,252 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:32,255 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:32,266 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,268 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,270 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,272 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,272 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,273 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:32,273 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,273 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,274 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:32,277 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:32,284 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,288 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,290 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,291 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,291 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,292 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:32,292 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,293 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,293 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:32,297 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:32,309 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,313 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,316 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,317 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,318 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,318 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:32,318 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,319 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,319 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:32,322 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:32,330 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,332 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,333 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,335 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,335 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,335 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:32,336 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,336 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,336 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:32,339 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:32,347 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,349 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,352 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,353 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,353 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,354 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:32,354 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,354 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,354 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:32,358 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:32,365 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,368 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,370 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,372 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,372 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,372 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:32,373 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,373 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,373 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:32,376 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:32,384 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,386 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,387 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,389 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,389 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,389 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:32,390 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,390 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,390 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:32,394 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:32,401 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,501 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,503 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,507 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,507 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,507 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:32,508 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,508 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,508 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:32,512 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:32,518 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,523 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,525 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,527 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,528 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,529 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:32,529 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,529 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,530 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:32,533 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:32,541 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,543 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,545 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,546 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,546 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,546 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:32,547 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,547 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,547 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:32,551 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:32,555 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,557 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,558 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,560 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-30 08:06:32,560 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-30 08:06:32,560 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:32,561 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,561 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,561 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:32,561 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:32,565 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,565 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,565 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,566 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,566 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,566 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:32,566 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,567 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,567 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:32,567 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:32,575 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,581 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,586 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,593 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,594 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:32,594 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:32,596 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:32,596 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,596 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:32,597 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:32,597 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,598 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,598 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,598 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,598 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,598 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:32,599 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 15])", "<class 'int'>: 14")
2023-10-30 08:06:32,599 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,599 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:32,599 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:32,604 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,604 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,605 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,605 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,605 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,605 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:32,606 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,606 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,606 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:32,610 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:32,616 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,618 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,619 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,621 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,621 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,621 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:32,622 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,622 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,622 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:32,625 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:32,632 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,634 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,635 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,637 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,637 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,637 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:32,638 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,638 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,638 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:32,642 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:32,648 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,650 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,652 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,653 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,653 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,654 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:32,654 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,654 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,654 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:32,658 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:32,665 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,666 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,668 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,670 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,670 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,670 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:32,670 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,671 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,671 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:32,674 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:32,681 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,683 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,684 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,686 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,686 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,686 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:32,687 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,687 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,687 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:32,691 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:32,697 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,699 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,701 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,702 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,702 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,703 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:32,703 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,703 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,703 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:32,707 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:32,714 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,716 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,717 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,719 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,719 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,719 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:32,720 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,720 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,720 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:32,723 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:32,730 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,732 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,734 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,735 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,735 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,735 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:32,736 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,736 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,736 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:32,740 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:32,747 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,748 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,750 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,751 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,752 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,752 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:32,752 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,752 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,753 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:32,756 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:32,763 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,765 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,766 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,772 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,772 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,773 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:32,773 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,773 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,773 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:32,777 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:32,783 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,785 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,787 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,788 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,788 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,788 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:32,789 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,789 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,789 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:32,793 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:32,796 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,798 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,799 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,801 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-30 08:06:32,801 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-30 08:06:32,802 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:32,802 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,802 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,802 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:32,803 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:32,806 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,806 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,807 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,807 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,807 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,807 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:32,808 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,808 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,808 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:32,808 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:32,820 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,825 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,830 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,835 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:32,836 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:32,836 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:32,837 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:32,837 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,837 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:32,838 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:32,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,839 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,839 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,839 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,839 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:32,839 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 16])", "<class 'int'>: 15")
2023-10-30 08:06:32,840 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:32,840 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:32,840 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:32,844 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,845 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,845 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,845 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:32,845 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:32,846 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:32,846 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,846 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,847 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:32,850 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:32,856 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,858 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,859 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,861 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,861 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:32,861 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:32,862 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,862 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,862 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:32,865 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:32,872 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,873 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,875 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,876 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,877 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:32,877 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:32,877 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,877 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,877 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:32,881 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:32,887 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,889 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,890 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,892 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,892 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:32,892 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:32,893 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,893 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,893 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:32,896 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:32,903 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,904 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,906 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,907 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,908 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:32,908 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:32,908 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,908 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,909 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:32,911 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:32,918 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,920 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,921 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,923 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,923 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:32,923 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:32,924 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:32,924 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:32,924 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:32,927 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:32,934 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,935 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:32,937 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,330 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,331 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:33,331 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:33,332 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,332 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,332 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:33,335 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:33,343 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,345 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,346 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,348 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,348 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:33,348 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:33,349 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,349 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,349 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:33,353 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:33,361 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,364 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,365 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,367 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,367 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:33,367 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:33,368 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,368 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,368 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:33,373 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:33,381 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,384 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,385 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,387 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,387 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:33,387 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:33,388 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,388 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,388 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:33,391 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:33,399 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,401 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,402 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,404 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,404 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:33,404 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:33,405 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,405 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,405 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:33,409 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:33,416 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,418 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,420 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,421 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,421 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:33,422 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:33,422 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,422 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,422 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:33,426 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:33,430 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,431 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,433 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,434 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-30 08:06:33,435 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-30 08:06:33,435 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:33,435 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,435 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:33,435 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:33,436 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:33,440 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,441 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,441 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,441 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,441 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:33,441 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:33,442 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,442 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:33,442 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:33,443 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:33,450 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:33,456 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:33,462 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:33,468 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:33,469 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:33,469 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:33,470 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:33,471 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:33,471 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:33,471 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:33,472 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,472 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,473 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,473 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,473 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:33,473 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:33,474 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 17])", "<class 'int'>: 16")
2023-10-30 08:06:33,474 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:33,474 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:33,474 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:33,480 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,480 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,481 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,481 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,481 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:33,481 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:33,482 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,482 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,482 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:33,485 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:33,493 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,496 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,498 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,500 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,500 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,500 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:33,501 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,501 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,501 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:33,505 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:33,512 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,515 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,516 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,518 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,518 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,518 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:33,519 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,519 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,519 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:33,523 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:33,530 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,532 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,534 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,535 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,535 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,536 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:33,536 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,536 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,537 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:33,540 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:33,548 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,549 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,551 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,553 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,553 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,553 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:33,553 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,554 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,554 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:33,558 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:33,570 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,572 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,575 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,576 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,576 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,577 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:33,577 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,577 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,577 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:33,581 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:33,588 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,590 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,592 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,593 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,593 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,594 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:33,594 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,594 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,594 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:33,598 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:33,605 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,606 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,608 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,610 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,610 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,610 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:33,610 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,611 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,611 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:33,615 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:33,632 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,636 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,649 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,654 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,656 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,656 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:33,657 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,657 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,657 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:33,661 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:33,670 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,672 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,674 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,676 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,676 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,676 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:33,677 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,677 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,677 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:33,680 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:33,687 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,689 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,690 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,692 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,693 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,693 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:33,693 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,694 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,694 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:33,697 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:33,704 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,706 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,708 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,710 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,711 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,711 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:33,712 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,712 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,712 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:33,715 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:33,719 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,722 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,724 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,726 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-30 08:06:33,726 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-30 08:06:33,726 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:33,727 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,727 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:33,727 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:33,728 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:33,731 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,732 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,732 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,732 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,733 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:33,733 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:33,733 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,733 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:33,733 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:33,734 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:33,743 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:33,751 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:33,759 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:33,768 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:33,769 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:33,769 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:33,770 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:33,770 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:33,771 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:33,771 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:33,772 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,772 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,773 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,773 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,773 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:33,774 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:33,774 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 18])", "<class 'int'>: 17")
2023-10-30 08:06:33,774 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:33,775 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:33,775 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:33,779 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,780 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,780 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,781 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:33,781 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:33,781 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:33,782 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,782 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,782 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:33,785 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:33,793 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,795 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,797 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,799 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,799 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,799 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:33,800 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,800 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,800 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:33,804 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:33,811 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,814 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,816 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,818 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,818 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,818 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:33,819 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,819 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,819 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:33,823 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:33,830 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,832 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,836 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,844 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,844 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,844 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:33,845 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,845 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,845 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:33,848 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:33,856 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,858 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,860 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,862 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,862 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,863 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:33,863 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,863 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,864 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:33,866 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:33,874 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,876 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,878 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,880 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,880 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,880 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:33,881 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,881 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,882 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:33,886 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:33,893 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,895 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,898 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,900 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,900 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,900 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:33,901 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,901 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,901 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:33,905 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:33,912 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,914 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,916 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,918 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,918 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,919 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:33,919 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,920 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,920 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:33,923 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:33,933 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,935 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,937 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,940 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,940 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,941 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:33,941 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,942 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,942 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:33,945 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:33,954 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,956 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,958 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,960 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,961 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,961 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:33,962 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,962 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,962 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:33,965 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:33,972 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,975 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,977 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,979 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,979 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,979 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:33,980 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,980 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,981 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:33,984 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:33,991 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,993 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,995 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,997 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:33,997 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:33,997 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:33,998 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:33,998 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:33,999 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:34,002 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:34,006 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:34,008 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:34,010 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:34,012 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-30 08:06:34,013 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-30 08:06:34,013 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:34,014 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,014 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,014 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:34,015 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:34,018 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,019 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,019 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,019 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,020 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,020 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:34,020 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,020 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,020 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:34,021 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:34,031 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,039 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,046 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,054 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,055 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:34,055 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:34,056 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:34,057 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,057 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:34,057 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:34,058 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,058 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,059 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,059 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,059 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,060 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:34,060 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 19])", "<class 'int'>: 18")
2023-10-30 08:06:34,060 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,061 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:34,061 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:34,065 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,066 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,066 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,067 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,067 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,067 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:34,067 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,068 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,068 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:34,071 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:34,078 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,080 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,082 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,084 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,084 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,084 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:34,085 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,085 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,085 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:34,088 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:34,095 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,098 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,100 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,102 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,102 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,102 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:34,103 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,103 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,104 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:34,107 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:34,115 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,117 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,119 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,121 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,122 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,122 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:34,123 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,123 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,123 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:34,126 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:34,134 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,136 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,138 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,140 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,140 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,140 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:34,141 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,141 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,142 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:34,145 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:34,152 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,154 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,156 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,158 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,158 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,158 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:34,159 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,159 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,159 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:34,162 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:34,169 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,171 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,173 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,175 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,175 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,175 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:34,175 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,175 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,176 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:34,179 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:34,185 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,187 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,188 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,190 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,190 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,190 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:34,190 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,191 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,191 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:34,194 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:34,201 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,202 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,204 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,206 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,206 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,206 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:34,206 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,207 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,207 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:34,210 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:34,216 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,218 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,220 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,221 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,221 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,221 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:34,222 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,222 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,222 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:34,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:34,232 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,234 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,235 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,237 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,237 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,237 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:34,238 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,238 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,238 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:34,241 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:34,247 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,249 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,251 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,252 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,252 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,252 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:34,253 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,253 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,253 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:34,256 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:34,260 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,262 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,263 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,265 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-30 08:06:34,265 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-30 08:06:34,265 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:34,265 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,266 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,266 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:34,266 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:34,269 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,270 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,270 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,270 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,270 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,270 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:34,271 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,271 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,271 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:34,271 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:34,280 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,286 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,292 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,298 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,299 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:34,299 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:34,300 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:34,300 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,300 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:34,301 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:34,301 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,302 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,302 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,302 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,302 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,303 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:34,303 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 20])", "<class 'int'>: 19")
2023-10-30 08:06:34,303 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:34,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:34,308 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,308 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,309 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,309 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,309 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,309 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:34,310 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,310 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,310 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:34,313 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:34,320 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,321 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,323 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,324 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,325 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,325 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:34,325 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,325 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,326 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:34,329 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:34,335 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,337 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,338 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,339 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,340 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,340 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:34,340 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,340 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,341 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:34,343 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:34,350 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,352 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,353 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,358 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,358 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,358 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:34,359 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,359 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,359 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:34,362 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:34,369 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,371 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,372 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,374 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,374 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,374 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:34,374 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,375 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,375 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:34,378 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:34,390 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,392 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,393 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,395 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,395 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,395 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:34,396 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,396 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,396 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:34,399 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:34,407 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,409 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,410 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,412 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,412 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,412 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:34,413 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,413 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,413 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:34,416 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:34,423 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,424 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,426 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,427 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,427 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,427 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:34,428 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,428 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,428 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:34,431 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:34,439 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,441 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,442 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,444 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,444 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,444 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:34,445 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,445 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,445 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:34,449 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:34,456 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,457 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,459 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,461 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,461 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,611 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:34,612 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,612 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,612 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:34,616 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:34,630 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,632 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,633 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,635 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,635 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,635 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:34,637 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,638 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,638 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:34,642 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:34,655 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,657 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,666 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,669 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,669 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,670 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:34,671 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,671 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,671 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:34,677 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:34,683 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,687 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,690 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,693 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-30 08:06:34,693 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-30 08:06:34,694 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:34,694 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,694 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,694 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:34,695 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:34,700 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,700 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,701 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,701 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,702 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,702 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:34,702 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,703 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,703 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:34,703 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:34,711 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,717 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,724 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,730 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,730 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:34,731 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:34,732 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:34,732 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,732 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:34,733 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:34,734 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,734 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,734 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,735 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,735 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,735 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:34,736 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 21])", "<class 'int'>: 20")
2023-10-30 08:06:34,736 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,736 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:34,737 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:34,742 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,743 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,743 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,744 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,744 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,744 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:34,744 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,745 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,745 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:34,748 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:34,756 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,758 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,759 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,761 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,762 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,762 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:34,763 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,763 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,763 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:34,767 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:34,774 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,776 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,778 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,780 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,780 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,780 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:34,781 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,781 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,781 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:34,784 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:34,792 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,794 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,796 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,797 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,797 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,798 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:34,798 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,798 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,798 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:34,803 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:34,811 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,812 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,815 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,817 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,817 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,817 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:34,818 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,818 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,818 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:34,823 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:34,832 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,834 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,836 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,838 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,838 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:34,839 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,839 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,839 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:34,842 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:34,850 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,852 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,854 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,856 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,856 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,856 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:34,857 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,857 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,857 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:34,861 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:34,868 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,870 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,872 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,874 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,874 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,874 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:34,875 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,875 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,875 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:34,879 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:34,887 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,889 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,890 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,893 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,893 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,893 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:34,894 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,894 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,894 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:34,898 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:34,905 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,907 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,909 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,911 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,911 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,911 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:34,912 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,912 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,912 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:34,915 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:34,923 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,925 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,927 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,928 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,929 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,929 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:34,929 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,930 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,930 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:34,933 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:34,941 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,944 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,946 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,948 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,949 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,949 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:34,950 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,950 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:34,950 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:34,955 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:34,958 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,961 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,962 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,964 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-30 08:06:34,964 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-30 08:06:34,964 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:34,965 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,965 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,965 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:34,966 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:34,970 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,970 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,971 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,971 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:34,971 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:34,971 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:34,971 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:34,972 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:34,972 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:34,972 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:34,980 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,986 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,992 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,999 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:34,999 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:35,000 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:35,001 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:35,002 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,002 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:35,003 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:35,004 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,004 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,005 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,005 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,006 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,006 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:35,007 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 22])", "<class 'int'>: 21")
2023-10-30 08:06:35,007 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,007 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:35,007 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:35,012 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,013 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,013 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,014 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,014 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,014 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:35,014 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,015 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,015 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:35,020 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:35,027 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,029 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,031 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,033 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,034 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,034 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:35,034 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,034 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,035 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:35,039 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:35,046 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,049 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,051 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,053 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,053 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,053 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:35,054 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,054 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,054 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:35,058 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:35,067 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,069 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,070 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,072 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,072 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,072 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:35,073 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,073 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,073 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:35,077 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:35,084 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,086 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,088 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,090 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,090 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,090 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:35,091 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,091 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,091 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:35,094 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:35,102 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,104 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,106 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,108 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,108 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,108 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:35,108 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,109 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,109 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:35,112 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:35,119 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,121 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,122 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,125 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,125 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,125 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:35,126 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,126 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,126 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:35,130 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:35,137 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,138 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,140 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,142 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,142 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,142 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:35,143 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,143 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,143 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:35,146 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:35,152 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,154 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,156 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,157 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,158 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,158 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:35,158 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,158 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,158 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:35,161 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:35,168 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,170 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,172 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,173 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,173 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,174 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:35,174 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,174 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,174 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:35,178 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:35,184 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,186 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,187 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,189 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,189 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,189 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:35,190 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,190 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,190 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:35,193 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:35,200 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,201 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,203 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,205 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,205 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,205 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:35,206 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,206 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,206 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:35,209 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:35,214 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,215 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,217 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,219 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-30 08:06:35,219 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-30 08:06:35,219 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:35,219 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,220 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,220 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:35,220 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:35,224 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,224 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,225 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,225 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,225 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,225 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:35,226 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,226 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:35,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:35,234 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,240 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,246 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,261 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,303 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:35,306 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:35,312 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:35,312 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,313 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:35,313 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:35,314 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,314 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,314 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,315 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,315 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,315 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:35,315 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 23])", "<class 'int'>: 22")
2023-10-30 08:06:35,315 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,316 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:35,316 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:35,320 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,321 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,321 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,322 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,322 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,322 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:35,322 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,323 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,323 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:35,326 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:35,333 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,335 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,336 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,338 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,338 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,338 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:35,339 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,339 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,339 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:35,342 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:35,349 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,351 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,352 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,354 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,354 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,354 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:35,355 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,355 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,355 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:35,359 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:35,366 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,368 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,369 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,371 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,371 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,371 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:35,372 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,372 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,372 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:35,376 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:35,382 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,384 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,386 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,387 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,387 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,388 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:35,388 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,389 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,389 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:35,392 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:35,400 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,404 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,405 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,407 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,408 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,408 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:35,409 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,409 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,409 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:35,412 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:35,424 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,426 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,429 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,430 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,431 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,431 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:35,432 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,432 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,432 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:35,435 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:35,442 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,444 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,447 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,449 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,449 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,449 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:35,450 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,450 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,450 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:35,454 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:35,461 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,462 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,464 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,466 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,466 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,466 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:35,467 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,467 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,467 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:35,470 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:35,477 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,479 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,480 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,482 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,482 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,482 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:35,483 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,483 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,483 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:35,486 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:35,493 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,495 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,496 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,498 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,498 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,498 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:35,499 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,499 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,499 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:35,503 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:35,509 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,514 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,516 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,518 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,518 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,518 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:35,519 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,519 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,519 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:35,522 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:35,525 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,527 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,529 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,531 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-30 08:06:35,531 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-30 08:06:35,531 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:35,532 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,532 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,532 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:35,533 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:35,537 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,537 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,538 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,538 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,538 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,538 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:35,539 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,539 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,539 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:35,539 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:35,546 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,552 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,558 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,564 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,564 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:35,565 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:35,566 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:35,566 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,566 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:35,567 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:35,567 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,568 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,568 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,568 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,568 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,569 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:35,569 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 24])", "<class 'int'>: 23")
2023-10-30 08:06:35,569 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,569 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:35,570 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:35,574 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,575 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,575 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,575 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,653 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,653 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:35,654 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,654 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,654 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:35,658 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:35,665 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,667 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,668 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,670 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,670 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,670 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:35,670 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,671 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,671 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:35,674 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:35,681 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,683 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,685 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,686 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,686 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,686 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:35,687 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,687 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,687 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:35,691 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:35,698 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,700 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,701 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,703 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,703 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,703 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:35,704 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,704 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,704 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:35,707 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:35,719 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,720 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,722 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,725 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,726 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,726 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:35,727 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,727 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,728 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:35,732 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:35,744 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,746 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,748 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,749 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,750 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,750 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:35,751 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,751 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,751 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:35,755 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:35,762 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,764 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,766 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,767 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,768 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,768 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:35,769 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,769 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,769 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:35,773 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:35,780 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,782 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,783 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,785 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,785 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,785 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:35,786 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,786 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,787 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:35,790 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:35,797 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,799 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,800 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,802 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,802 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,802 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:35,803 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,803 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,803 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:35,807 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:35,813 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,815 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,817 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,819 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,819 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,819 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:35,820 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,820 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,820 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:35,823 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:35,830 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,832 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,834 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,835 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,835 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,835 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:35,836 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,836 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,836 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:35,839 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:35,846 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,848 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,850 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,851 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,852 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,852 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:35,852 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,852 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,853 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:35,856 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:35,860 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,861 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,863 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,865 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-30 08:06:35,865 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-30 08:06:35,865 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:35,866 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,866 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,867 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:35,867 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:35,871 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,871 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,871 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,872 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,872 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,872 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:35,872 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,872 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,873 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:35,873 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:35,880 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,886 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,892 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,900 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:35,927 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:35,927 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:35,929 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:35,929 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,929 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:35,930 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:35,932 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,932 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,933 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,933 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,933 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,934 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:35,934 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 25])", "<class 'int'>: 24")
2023-10-30 08:06:35,934 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:35,935 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:35,935 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:35,941 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,942 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,942 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,943 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:35,943 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:35,943 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:35,944 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,944 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,944 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:35,948 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:35,957 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:35,959 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:35,966 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:35,968 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:35,968 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:35,969 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:35,969 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,970 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,970 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:35,974 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:35,982 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:35,984 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:35,986 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:35,987 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:35,988 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:35,988 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:35,988 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:35,989 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:35,989 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:35,993 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:36,001 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,003 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,005 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,007 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,007 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,007 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:36,008 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,008 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,008 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:36,012 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:36,020 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,022 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,024 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,026 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,026 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,027 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:36,027 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,028 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,028 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:36,032 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:36,040 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,042 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,044 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,046 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,046 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,046 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:36,047 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,047 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,047 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:36,051 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:36,059 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,061 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,063 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,065 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,065 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,065 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:36,066 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,066 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,066 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:36,070 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:36,078 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,080 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,082 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,085 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,085 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,085 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:36,086 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,086 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,086 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:36,090 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:36,098 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,100 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,102 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,104 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,104 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,104 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:36,105 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,105 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,105 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:36,109 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:36,117 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,120 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,122 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,124 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,124 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,124 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:36,125 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,125 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,125 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:36,129 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:36,137 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,139 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,141 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,143 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,144 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,144 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:36,144 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,145 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,145 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:36,148 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:36,156 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,158 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,160 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,162 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,162 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,162 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:36,163 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,163 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,163 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:36,167 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:36,171 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,174 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,176 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,178 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-30 08:06:36,179 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-30 08:06:36,179 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:36,179 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,179 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,179 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:36,180 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:36,184 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,184 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,185 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,185 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,185 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,185 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:36,186 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,186 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,186 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:36,186 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:36,196 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,204 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,211 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,222 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,223 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:36,224 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:36,225 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:36,225 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:36,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:36,227 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,227 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,228 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,228 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,229 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,229 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:36,229 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 26])", "<class 'int'>: 25")
2023-10-30 08:06:36,230 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,230 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:36,230 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:36,235 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,236 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,236 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,237 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,237 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,237 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:36,238 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,238 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,239 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:36,242 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:36,250 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,251 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,253 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,255 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,256 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,256 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:36,256 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,257 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,257 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:36,261 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:36,268 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,270 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,271 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,273 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,273 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,274 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:36,274 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,274 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,274 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:36,278 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:36,285 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,287 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,289 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,290 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,291 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,291 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:36,291 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,292 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,292 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:36,295 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:36,302 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,304 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,305 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,307 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,307 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,307 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:36,308 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,308 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,308 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:36,312 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:36,319 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,321 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,322 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,324 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,324 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,324 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:36,325 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,325 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,325 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:36,329 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:36,336 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,338 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,340 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,341 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,341 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,342 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:36,342 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,342 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,342 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:36,346 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:36,353 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,355 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,356 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,358 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,358 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,359 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:36,359 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,359 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,359 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:36,363 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:36,370 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,372 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,373 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,375 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,375 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,376 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:36,376 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,376 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,377 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:36,380 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:36,387 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,389 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,391 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,392 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,393 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,393 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:36,393 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,393 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,394 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:36,397 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:36,404 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,405 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,407 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,409 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,409 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,409 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:36,409 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,410 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,410 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:36,413 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:36,421 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,422 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,424 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,426 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,426 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,426 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:36,426 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,427 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,427 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:36,430 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:36,434 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,435 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,437 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,439 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-30 08:06:36,439 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-30 08:06:36,439 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:36,439 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,440 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,440 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:36,440 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:36,444 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,444 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,444 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,445 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,445 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,445 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:36,445 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,445 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,446 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:36,446 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:36,453 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,459 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,465 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,471 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,471 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:36,471 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:36,473 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:36,473 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,473 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:36,473 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:36,474 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,475 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,475 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,475 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,475 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,476 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:36,476 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 27])", "<class 'int'>: 26")
2023-10-30 08:06:36,476 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,476 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:36,477 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:36,481 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,482 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,482 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,483 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,483 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,483 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:36,483 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,484 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,484 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:36,487 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:36,494 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,496 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,497 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,499 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,499 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,499 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:36,500 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,500 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,500 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:36,503 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:36,511 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,513 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,514 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,516 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,516 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,516 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:36,517 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,517 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,517 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:36,521 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:36,544 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,546 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,553 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,557 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,557 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,557 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:36,558 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,558 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,559 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:36,562 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:36,570 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,574 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,577 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,580 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,580 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,580 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:36,581 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,582 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,582 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:36,586 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:36,595 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,599 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,611 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,613 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,614 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,614 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:36,615 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,615 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,615 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:36,619 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:36,626 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,628 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,630 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,632 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,632 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,632 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:36,633 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,633 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,634 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:36,637 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:36,653 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,655 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,657 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,659 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,659 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,688 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:36,689 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,689 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,689 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:36,692 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:36,720 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,722 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,724 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,726 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,726 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,726 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:36,727 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,727 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,727 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:36,731 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:36,738 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,740 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,742 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,743 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,743 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,743 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:36,744 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,744 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,744 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:36,748 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:36,754 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,756 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,758 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,760 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,760 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,760 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:36,761 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,761 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,761 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:36,764 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:36,772 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,774 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,776 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,777 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,778 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,778 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:36,779 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,779 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,779 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:36,782 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:36,789 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,791 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,793 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,795 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-30 08:06:36,796 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-30 08:06:36,796 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:36,796 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,797 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,797 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:36,797 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:36,801 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,801 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,801 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,802 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,802 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,802 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:36,802 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,803 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,803 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:36,803 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:36,810 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,816 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,821 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,827 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:36,828 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:36,828 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:36,829 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:36,829 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,829 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:36,830 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:36,831 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,831 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,831 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,832 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,832 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,832 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:36,832 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 28])", "<class 'int'>: 27")
2023-10-30 08:06:36,833 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:36,833 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:36,833 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:36,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,839 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,839 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:36,839 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:36,839 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:36,840 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,840 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,840 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:36,844 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:36,851 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,853 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,855 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,856 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,856 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,857 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:36,857 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,857 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,857 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:36,861 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:36,868 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,870 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,871 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,873 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,873 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,873 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:36,874 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,874 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,874 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:36,878 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:36,886 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,887 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,890 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,892 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,892 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,893 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:36,893 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,893 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,893 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:36,897 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:36,904 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,906 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,907 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,909 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,909 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,909 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:36,910 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,910 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,910 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:36,914 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:36,921 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,923 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,924 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,926 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,926 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,926 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:36,927 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,927 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,927 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:36,931 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:36,938 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,940 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,942 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,944 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,944 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,944 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:36,945 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,945 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,945 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:36,949 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:36,956 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,958 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,959 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,961 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,961 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,961 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:36,962 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,962 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,962 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:36,966 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:36,973 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,975 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,976 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,978 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,978 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,978 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:36,979 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,979 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,979 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:36,983 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:36,990 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,992 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,993 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,995 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:36,995 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:36,995 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:36,996 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:36,996 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:36,996 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:37,000 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:37,009 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,011 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,013 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,014 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,014 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:37,014 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:37,015 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,015 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,015 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:37,019 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:37,026 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,028 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,030 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,031 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,032 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:37,032 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:37,032 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,033 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,033 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:37,036 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:37,040 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,042 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,043 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,045 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-30 08:06:37,045 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-30 08:06:37,045 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:37,046 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,046 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,046 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:37,047 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:37,050 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,051 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,051 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,051 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,052 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,052 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:37,052 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,052 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,052 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:37,053 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:37,061 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,067 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,073 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,078 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,079 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:37,080 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:37,081 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:37,081 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,081 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:37,082 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:37,083 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,083 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,084 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,084 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,085 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,085 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:37,085 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 29])", "<class 'int'>: 28")
2023-10-30 08:06:37,086 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,086 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:37,087 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:37,091 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,092 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,092 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,093 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,093 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,094 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:37,094 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,095 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,095 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:37,098 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:37,105 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,107 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,108 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,110 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,110 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,111 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:37,111 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,111 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,111 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:37,115 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:37,122 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,124 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,125 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,127 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,127 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,127 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:37,128 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,128 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,128 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:37,132 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:37,139 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,141 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,142 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,144 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,144 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,145 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:37,145 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,146 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,146 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:37,149 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:37,157 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,159 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,161 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,163 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,163 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,163 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:37,164 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,164 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,164 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:37,168 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:37,175 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,177 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,178 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,180 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,180 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,180 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:37,181 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,181 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,181 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:37,185 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:37,192 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,194 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,196 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,197 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,197 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,197 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:37,198 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,198 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,198 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:37,202 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:37,209 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,211 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,213 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,214 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,215 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,215 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:37,215 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,215 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,215 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:37,219 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:37,226 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,228 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,229 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,231 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,231 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,231 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:37,232 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,232 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,232 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:37,236 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:37,243 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,245 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,247 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,248 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,249 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,249 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:37,249 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,249 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,250 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:37,253 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:37,260 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,262 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,264 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,265 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,266 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,266 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:37,266 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,267 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,267 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:37,270 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:37,277 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,279 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,280 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,282 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,282 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,282 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:37,283 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,283 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,283 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:37,287 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:37,291 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,293 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,294 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,296 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-30 08:06:37,296 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-30 08:06:37,296 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:37,297 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,297 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,297 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:37,297 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:37,301 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,301 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,302 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,302 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,302 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,302 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:37,303 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,303 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:37,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:37,311 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,317 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,324 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,330 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,331 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:37,332 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:37,333 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:37,333 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,334 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:37,334 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:37,335 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,336 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,336 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,337 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,337 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,337 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:37,337 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 30])", "<class 'int'>: 29")
2023-10-30 08:06:37,337 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,338 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:37,338 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:37,343 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,344 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,344 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,344 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,345 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,345 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:37,345 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,345 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,346 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:37,349 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:37,356 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,358 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,360 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,361 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,362 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,362 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:37,362 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,362 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,363 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:37,366 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:37,374 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,376 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,377 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,379 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,379 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,379 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:37,380 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,380 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,380 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:37,384 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:37,391 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,393 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,395 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,396 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,397 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,397 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:37,397 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,397 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,398 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:37,401 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:37,408 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,410 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,411 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,413 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,413 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,413 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:37,414 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,414 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,414 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:37,418 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:37,425 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,427 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,428 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,430 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,430 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,430 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:37,431 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,431 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,431 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:37,435 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:37,442 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,444 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,445 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,447 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,447 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,447 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:37,448 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,448 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,448 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:37,452 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:37,460 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,461 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,463 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,464 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,465 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,465 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:37,466 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,466 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,466 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:37,470 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:37,477 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,479 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,481 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,482 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,482 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,482 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:37,483 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,483 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,483 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:37,487 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:37,494 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,496 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,498 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,500 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,501 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,501 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:37,501 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,501 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,502 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:37,505 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:37,527 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,529 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,530 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,532 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,532 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,532 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:37,533 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,533 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,533 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:37,537 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:37,544 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,545 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,547 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,550 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,550 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,551 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:37,551 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,551 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,551 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:37,555 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:37,559 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,561 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,563 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,566 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-30 08:06:37,566 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-30 08:06:37,566 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:37,567 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,567 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,567 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:37,568 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:37,572 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,572 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,573 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,573 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,573 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,574 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:37,574 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,574 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,574 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:37,575 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:37,582 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,588 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,594 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,600 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,601 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:37,601 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:37,602 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:37,603 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,603 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:37,604 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:37,605 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,605 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,606 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,606 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,606 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,606 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:37,607 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 31])", "<class 'int'>: 30")
2023-10-30 08:06:37,607 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,607 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:37,607 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:37,612 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,613 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,613 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,613 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,614 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,614 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:37,614 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,615 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,615 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:37,618 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:37,626 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,628 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,629 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,631 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,632 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,632 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:37,633 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,633 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,633 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:37,637 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:37,643 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,645 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,647 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,649 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,649 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,649 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:37,650 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,650 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,650 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:37,653 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:37,660 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,662 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,664 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,666 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,666 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,666 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:37,666 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,667 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,667 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:37,670 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:37,677 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,679 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,681 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,682 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,683 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,683 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:37,683 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,684 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,684 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:37,687 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:37,694 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,696 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,697 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,699 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,699 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,699 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:37,700 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,700 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,700 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:37,704 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:37,711 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,713 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,715 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,716 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,717 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,717 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:37,717 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,718 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,718 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:37,721 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:37,728 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,731 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,732 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,734 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,734 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,734 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:37,735 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,735 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,735 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:37,739 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:37,746 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,748 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,749 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,751 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,751 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,751 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:37,752 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,752 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,752 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:37,756 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:37,764 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,766 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,767 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,769 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,770 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,770 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:37,770 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,771 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,771 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:37,774 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:37,781 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,783 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,785 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,787 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,787 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,787 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:37,788 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,788 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,788 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:37,791 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:37,798 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,800 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,802 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,803 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,804 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,804 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:37,804 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,804 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,805 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:37,808 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:37,812 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,814 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,815 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,817 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-30 08:06:37,817 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-30 08:06:37,817 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:37,818 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,818 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,818 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:37,818 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:37,822 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,822 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,823 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,823 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,823 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,823 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:37,823 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,824 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,824 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:37,824 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:37,832 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,838 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,845 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,852 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:37,853 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:37,853 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:37,854 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:37,854 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,854 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:37,855 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:37,856 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,856 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,856 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,857 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,857 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,857 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:37,857 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 32])", "<class 'int'>: 31")
2023-10-30 08:06:37,857 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:37,857 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:37,858 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:37,863 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,863 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,863 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,864 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:37,864 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:37,864 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:37,865 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,865 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,865 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:37,868 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:37,876 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,878 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,879 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,881 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,881 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:37,881 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:37,882 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,882 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,882 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:37,886 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:37,893 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,895 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,896 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,898 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,898 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:37,898 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:37,899 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,899 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,899 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:37,903 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:37,909 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,911 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,913 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,914 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,915 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:37,915 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:37,915 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,916 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,916 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:37,919 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:37,926 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,928 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,930 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,931 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,931 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:37,932 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:37,932 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,932 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,932 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:37,935 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:37,943 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,945 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,946 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,947 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,948 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:37,948 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:37,948 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,949 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,949 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:37,952 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:37,959 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,961 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,963 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,964 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,965 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:37,965 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:37,965 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,965 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,965 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:37,969 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:37,976 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,978 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,979 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,981 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,981 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:37,981 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:37,982 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,982 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,982 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:37,986 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:37,993 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,995 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,996 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,998 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:37,998 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:37,998 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:37,999 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:37,999 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:37,999 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:38,002 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:38,009 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,011 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,013 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,015 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,015 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:38,015 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:38,016 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,016 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,016 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:38,019 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:38,027 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,028 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,030 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,032 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,033 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:38,033 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:38,033 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,034 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,034 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:38,037 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:38,047 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,048 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,050 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,052 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,052 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:38,052 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:38,053 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,053 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,053 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:38,057 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:38,061 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,062 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,064 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,066 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-30 08:06:38,066 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-30 08:06:38,066 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:38,067 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,067 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,067 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:38,068 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:38,071 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,072 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,072 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,072 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,072 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,073 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:38,073 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,073 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,073 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:38,073 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:38,081 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,088 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,094 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,100 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,101 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:38,101 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:38,102 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:38,102 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,102 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:38,103 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:38,103 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,104 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,104 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,104 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,105 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,105 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:38,105 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 33])", "<class 'int'>: 32")
2023-10-30 08:06:38,105 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,105 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:38,106 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:38,110 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,111 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,111 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,112 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,112 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,112 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:38,113 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,113 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,113 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:38,116 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:38,124 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,125 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,127 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,129 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,130 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,130 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:38,130 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,131 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,131 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:38,134 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:38,141 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,143 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,145 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,146 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,147 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,147 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:38,147 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,148 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,148 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:38,151 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:38,158 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,160 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,162 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,163 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,163 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,163 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:38,164 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,164 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,164 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:38,168 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:38,175 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,177 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,179 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,181 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,181 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,181 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:38,181 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,182 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,182 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:38,186 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:38,193 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,195 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,196 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,198 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,198 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,199 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:38,199 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,199 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,199 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:38,203 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:38,210 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,212 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,213 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,235 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,236 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,236 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:38,237 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,237 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,237 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:38,241 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:38,248 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,250 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,252 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,254 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,254 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,254 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:38,255 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,255 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,255 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:38,260 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:38,267 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,268 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,270 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,271 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,272 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,272 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:38,272 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,272 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,273 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:38,276 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:38,283 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,284 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,286 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,287 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,288 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,288 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:38,288 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,289 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,289 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:38,292 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:38,300 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,302 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,304 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,306 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,307 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,307 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:38,307 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,307 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,308 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:38,311 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:38,317 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,319 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,320 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,322 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,322 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,322 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:38,323 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,323 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,323 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:38,326 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:38,329 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,331 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,332 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,334 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-30 08:06:38,334 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-30 08:06:38,334 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:38,335 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,335 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,335 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:38,336 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:38,339 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,339 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,339 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,340 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,340 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,340 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:38,340 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,340 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,341 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:38,341 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:38,348 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,354 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,359 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,365 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,366 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:38,366 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:38,367 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:38,368 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,368 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:38,368 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:38,369 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,369 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,370 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,370 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,370 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,370 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:38,370 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 34])", "<class 'int'>: 33")
2023-10-30 08:06:38,371 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,371 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:38,371 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:38,375 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,375 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,376 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,376 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,376 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,376 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:38,377 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,377 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,377 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:38,380 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:38,387 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,389 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,391 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,393 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,393 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,393 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:38,394 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,394 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,395 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:38,398 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:38,405 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,407 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,409 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,410 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,410 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,411 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:38,411 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,411 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,411 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:38,415 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:38,421 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,423 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,424 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,426 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,426 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,426 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:38,427 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,427 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,427 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:38,430 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:38,436 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,438 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,440 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,441 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,441 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,442 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:38,442 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,442 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,442 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:38,446 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:38,459 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,463 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,468 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,472 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,472 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,472 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:38,473 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,474 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,474 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:38,480 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:38,487 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,489 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,491 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,492 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,492 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,492 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:38,493 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,493 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,493 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:38,497 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:38,504 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,507 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,508 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,510 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,510 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,510 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:38,511 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,511 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,511 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:38,514 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:38,524 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,526 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,527 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,529 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,529 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,529 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:38,530 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,530 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,530 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:38,534 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:38,541 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,543 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,544 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,546 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,546 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,546 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:38,547 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,547 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,547 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:38,551 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:38,559 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,561 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,563 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,565 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,565 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,565 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:38,566 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,566 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,566 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:38,569 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:38,575 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,577 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,579 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,580 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,580 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,580 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:38,581 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,581 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,581 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:38,584 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:38,587 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,589 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,590 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,592 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-30 08:06:38,592 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-30 08:06:38,592 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:38,593 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,593 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,593 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:38,594 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:38,599 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,599 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,599 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,600 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,600 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,600 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:38,600 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,600 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,601 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:38,601 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:38,608 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,614 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,619 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,626 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,626 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:38,627 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:38,628 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:38,628 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,628 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:38,628 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:38,629 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,629 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,630 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,630 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,630 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,630 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:38,630 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 35])", "<class 'int'>: 34")
2023-10-30 08:06:38,630 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,631 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:38,631 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:38,635 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,635 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,636 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,636 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,636 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,636 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:38,637 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,637 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,637 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:38,640 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:38,646 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,648 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,649 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,650 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,651 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,651 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:38,651 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,651 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,651 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:38,654 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:38,660 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,662 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,663 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,664 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,664 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,665 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:38,665 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,665 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,665 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:38,668 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:38,674 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,676 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,678 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,679 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,679 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,679 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:38,680 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,680 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,680 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:38,683 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:38,689 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,691 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,692 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,797 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,797 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,798 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:38,798 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,798 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,798 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:38,803 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:38,811 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,813 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,815 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,816 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,816 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,816 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:38,817 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,817 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,817 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:38,820 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:38,826 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,828 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,829 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,831 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,831 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,831 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:38,832 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,832 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,832 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:38,835 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:38,841 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,843 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,844 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,846 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,846 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,846 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:38,847 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,847 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,847 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:38,850 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:38,858 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,861 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,862 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,864 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,865 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,865 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:38,866 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,866 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,866 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:38,870 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:38,877 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,879 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,880 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,882 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,882 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,882 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:38,883 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,883 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,883 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:38,887 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:38,894 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,895 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,897 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,899 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,899 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,899 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:38,899 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,900 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,900 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:38,903 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:38,910 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,912 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,914 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,915 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,916 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,916 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:38,916 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,917 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,917 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:38,920 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:38,924 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,925 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,927 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,929 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-30 08:06:38,929 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-30 08:06:38,929 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:38,930 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,930 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,930 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:38,931 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:38,934 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,935 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,935 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,935 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,936 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,936 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:38,936 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,936 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,936 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:38,937 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:38,945 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,951 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,957 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,964 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:38,965 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:38,965 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:38,966 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:38,966 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,966 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:38,967 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:38,968 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,968 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,968 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,969 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,969 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,969 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:38,969 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 36])", "<class 'int'>: 35")
2023-10-30 08:06:38,969 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:38,970 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:38,970 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:38,974 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,975 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,975 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,976 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:38,976 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:38,976 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:38,977 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,977 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,977 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:38,981 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:38,988 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:38,990 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:38,992 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:38,994 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:38,994 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:38,994 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:38,995 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:38,995 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:38,995 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:38,999 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:39,030 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,046 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,050 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,051 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,052 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,052 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:39,053 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,053 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,053 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:39,058 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:39,069 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,071 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,072 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,074 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,074 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,075 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:39,075 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,076 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,076 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:39,080 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:39,087 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,090 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,092 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,094 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,094 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,095 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:39,095 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,096 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,096 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:39,099 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:39,106 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,108 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,110 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,111 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,112 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,112 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:39,113 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,113 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,113 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:39,117 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:39,123 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,125 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,126 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,128 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,128 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,129 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:39,129 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,130 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,130 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:39,133 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:39,144 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,146 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,147 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,149 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,149 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,149 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:39,150 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,150 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,150 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:39,153 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:39,160 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,161 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,163 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,165 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,165 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,165 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:39,165 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,166 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,166 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:39,169 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:39,176 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,178 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,180 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,181 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,182 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,182 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:39,182 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,182 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,183 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:39,186 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:39,192 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,194 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,195 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,197 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,197 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,197 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:39,198 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,198 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,198 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:39,201 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:39,208 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,209 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,211 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,213 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,213 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,213 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:39,213 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,213 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,214 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:39,217 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:39,220 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,222 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,224 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,225 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-30 08:06:39,225 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-30 08:06:39,226 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:39,226 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,226 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,226 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:39,227 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:39,230 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,231 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,231 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,231 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,231 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:39,232 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:39,232 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,232 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,232 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:39,233 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:39,240 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,246 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,251 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,256 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,257 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:39,257 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:39,258 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:39,259 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,259 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:39,259 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:39,260 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,260 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,261 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,261 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,261 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:39,261 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:39,262 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 37])", "<class 'int'>: 36")
2023-10-30 08:06:39,262 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,262 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:39,262 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:39,266 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,267 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,267 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,268 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,268 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:39,268 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:39,268 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,269 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,269 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:39,272 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:39,279 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,281 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,282 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,284 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,284 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,284 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:39,284 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,285 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,285 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:39,288 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:39,294 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,296 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,298 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,299 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,299 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,300 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:39,300 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,300 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,300 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:39,303 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:39,310 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,312 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,314 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,315 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,316 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,316 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:39,316 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,316 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,317 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:39,320 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:39,327 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,328 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,330 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,331 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,332 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,332 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:39,332 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,332 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,333 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:39,336 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:39,343 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,344 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,346 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,347 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,348 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,348 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:39,348 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,348 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,349 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:39,352 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:39,359 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,361 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,363 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,364 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,365 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,365 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:39,365 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,366 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,366 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:39,369 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:39,376 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,378 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,380 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,381 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,382 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,382 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:39,382 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,383 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,383 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:39,386 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:39,393 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,395 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,396 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,398 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,398 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,398 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:39,399 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,399 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,399 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:39,403 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:39,410 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,412 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,414 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,415 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,416 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,416 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:39,416 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,416 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,417 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:39,420 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:39,427 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,429 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,430 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,432 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,432 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,432 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:39,433 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,433 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,433 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:39,436 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:39,443 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,445 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,447 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,448 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,448 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,449 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:39,449 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,449 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,449 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:39,453 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:39,456 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,458 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,459 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,461 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-30 08:06:39,461 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-30 08:06:39,461 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:39,462 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,462 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,462 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:39,463 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:39,466 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,467 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,467 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,467 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,468 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:39,468 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:39,468 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,468 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,468 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:39,469 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:39,478 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,489 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,500 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,507 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,508 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:39,508 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:39,510 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-30 08:06:39,510 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,511 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:39,511 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:39,513 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,513 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,513 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,514 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,514 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:39,514 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-30 08:06:39,515 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 38])", "<class 'int'>: 37")
2023-10-30 08:06:39,515 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,515 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-30 08:06:39,516 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:39,521 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,521 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,522 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,522 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,523 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:39,523 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-30 08:06:39,523 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,524 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,524 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-30 08:06:39,527 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:39,535 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,537 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,538 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,540 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,540 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,540 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-30 08:06:39,541 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,541 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,541 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-30 08:06:39,545 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:39,552 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,554 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,557 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,559 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,559 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,559 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-30 08:06:39,559 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,560 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,560 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-30 08:06:39,563 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:39,571 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,573 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,574 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,576 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,577 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,577 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-30 08:06:39,578 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,578 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,578 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-30 08:06:39,581 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:39,589 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,591 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,593 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,595 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,595 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,595 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-30 08:06:39,596 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,596 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,596 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-30 08:06:39,600 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:39,608 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,610 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,612 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,613 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,613 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,614 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-30 08:06:39,614 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,614 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,615 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-30 08:06:39,619 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:39,626 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,628 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,629 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,631 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,632 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,632 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-30 08:06:39,632 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,632 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,633 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-30 08:06:39,637 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:39,644 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,646 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,647 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,649 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,650 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,650 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-30 08:06:39,650 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,650 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,651 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-30 08:06:39,654 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:39,661 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,663 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,665 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,667 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,667 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,668 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-30 08:06:39,668 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,668 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,669 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-30 08:06:39,673 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:39,680 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,682 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,684 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,687 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,687 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,687 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-30 08:06:39,688 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,688 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,688 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-30 08:06:39,691 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:39,699 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,701 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,703 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,705 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,705 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,705 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-30 08:06:39,706 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,706 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,706 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-30 08:06:39,710 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:39,717 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,719 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,721 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,722 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,722 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,723 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-30 08:06:39,723 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,723 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-30 08:06:39,724 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-30 08:06:39,727 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:39,731 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,733 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,735 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,737 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-30 08:06:39,737 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-30 08:06:39,737 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-30 08:06:39,737 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,738 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,738 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-30 08:06:39,738 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:39,743 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,743 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,743 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,744 [flexgen.py:117 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-30 08:06:39,744 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-30 08:06:39,744 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-30 08:06:39,744 [flexgen.py:98 in flexgen_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-30 08:06:39,744 [flexgen.py:99 in flexgen_forward] DEBUG - kwargs: {}
2023-10-30 08:06:39,745 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-30 08:06:39,745 [model.py:390 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-30 08:06:39,757 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,769 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,779 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,788 [flexgen.py:117 in flexgen_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-30 08:06:39,793 [flexgen.py:134 in flexgen_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-30 08:06:39,793 [model.py:400 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-30 08:06:39,796 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-30 08:06:39,797 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 08:06:39,797 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-30 08:06:39,797 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 08:06:39,797 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-30 08:06:39,797 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 08:06:39,797 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-30 08:06:39,797 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-30 08:06:39,807 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-30 08:06:39,807 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-30 08:06:39,807 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-30 08:06:39,807 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-30 08:06:39,808 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-30 08:06:39,808 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-30 08:06:39,808 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-30 08:06:39,808 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-30 08:06:39,808 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-30 08:06:39,808 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-30 08:06:39,808 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-30 08:06:39,808 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-30 08:06:39,809 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-30 08:06:39,809 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-30 08:06:39,809 [flexgen.py:66 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-30 08:06:39,809 [flexgen.py:66 in layer_reset] DEBUG - lm_head from flexgen to old.
